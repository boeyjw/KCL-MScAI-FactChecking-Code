{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402ecfd6-f01c-43ef-aaa4-018f9dc63dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/gen\")\n",
    "sys.path.insert(1, \"../src/rte\")\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from util import read_data, write_jsonl\n",
    "from aggregate import generate_micro_macro_df, generate_doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1929c84-28f4-4949-b880-a85b8f8b8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "LOOKUP = {\n",
    "    \"verifiable\": {\"no\": \"NOT VERIFIABLE\", \"yes\": \"VERIFIABLE\"},\n",
    "    \"label\": {\"nei\": \"NOT ENOUGH INFO\", \"r\": \"REFUTES\", \"s\": \"SUPPORTS\"}\n",
    "}\n",
    "\n",
    "SEED = 123456789\n",
    "\n",
    "LABEL2ID = {\"SUPPORTS\": 0, \"NOT ENOUGH INFO\": 1, \"REFUTES\": 2}\n",
    "ID2LABEL = {0: \"SUPPORTS\", 1: \"NOT ENOUGH INFO\", 2: \"REFUTES\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05b8ab-6775-4eed-98be-10904943ded1",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aacd87-5b38-45b8-8be3-f7046bfa06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    TextClassificationPipeline,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd986a-1e45-4f38-8487-dddf2ee3ef24",
   "metadata": {},
   "source": [
    "# Huggingface Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b42b8e-2eb3-444b-9275-192519540439",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e5281d-7e7d-410e-b1ba-730e334247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2c86dd-cb1b-4b77-836d-1e5619f4bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"xlnet-base-cased\"\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        num_labels=3, \n",
    "        id2label=ID2LABEL, \n",
    "        label2id=LABEL2ID\n",
    "    )\n",
    "\n",
    "model = model_init()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, do_lower_case=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"evidence\"], examples[\"claim\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4834e0-770d-49dc-8041-f4d9d2729053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"fever\", \"climatefeverpure\", \"fever-climatefeverpure\", \"climatefever\", \"fever-climatefever\"]\n",
    "task = [\"hp_tune\", \"output\"]\n",
    "doc_sent = [\"doc\", \"sent\"]\n",
    "\n",
    "di = 2\n",
    "ti = 0\n",
    "ds = 0\n",
    "\n",
    "model_store_path = Path(\"/users/k21190024/study/fact-check-transfer-learning/scratch/thesis/models\").joinpath(model_checkpoint)\n",
    "model_store_path.mkdir(exist_ok=True)\n",
    "model_store_path = model_store_path / f\"{dataset[di]}-{model_checkpoint}-{doc_sent[ds]}-{task[ti]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940210d7-1da5-4e6f-b4f6-95304ceb496b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a618a5-0c63-4a99-b7cb-89deea2416f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/146415 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10277 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datap = Path(f\"/users/k21190024/study/fact-check-transfer-learning/scratch/dumps/bert-data-{doc_sent[ds]}-evidence\")\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.train.n5.jsonl\")),\n",
    "    \"validation\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.dev.n5.jsonl\")),\n",
    "    \"fever_test\": Dataset.from_list(read_data(datap / f\"{dataset[0]}.test.n5.jsonl\")),\n",
    "    \"climatefever_test\": Dataset.from_list(read_data(datap / f\"{dataset[1]}.test.n5.jsonl\"))\n",
    "}).map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d7f9a-4ddc-4a9b-b6cf-d31233fff94b",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06cfa045-016f-44e2-91c9-567ddd56ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# effective batch size of 32\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 8\n",
    "per_device_eval_batch_size = 32\n",
    "\n",
    "\n",
    "learning_rate = 4e-4\n",
    "epoch = 4\n",
    "metric_name = \"f1\"\n",
    "warmup_ratio=0.1\n",
    "save_steps=200\n",
    "eval_steps=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666373d-06fa-476c-9aaf-ffb3daadbf4b",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967aecb0-5bc7-488c-aefa-caf4525216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shard the data if the dataset is large for hyperparameter tuning\n",
    "shard = data[\"train\"].num_rows > 50000\n",
    "hp_tune_train = data[\"train\"].shuffle(seed=SEED).shard(num_shards=5, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e23d248-bb05-43da-a056-4e1fcc2a17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy = \"no\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\",\n",
    "    tf32=True\n",
    ")\n",
    "\n",
    "_ = model.train()\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=hp_tune_train if shard else data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5445d33-24fc-4a6d-8ed2-a89b26aa65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [1e-5, 3e-5, 2e-5]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "    }\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d4edde-28e8-4b30-a700-dc3f4a2a2ce9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 13:33:37,304] A new study created in memory with name: no-name-dcbe787d-f66a-4bd2-af76-b8a98d4d3867\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a XLNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4575' max='4575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4575/4575 46:36, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.378833</td>\n",
       "      <td>0.837307</td>\n",
       "      <td>0.836011</td>\n",
       "      <td>0.855487</td>\n",
       "      <td>0.832568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.399713</td>\n",
       "      <td>0.863482</td>\n",
       "      <td>0.862281</td>\n",
       "      <td>0.885130</td>\n",
       "      <td>0.860991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.452900</td>\n",
       "      <td>0.275815</td>\n",
       "      <td>0.909507</td>\n",
       "      <td>0.909022</td>\n",
       "      <td>0.912236</td>\n",
       "      <td>0.908609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.452900</td>\n",
       "      <td>0.235704</td>\n",
       "      <td>0.916318</td>\n",
       "      <td>0.915778</td>\n",
       "      <td>0.921890</td>\n",
       "      <td>0.916091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.277892</td>\n",
       "      <td>0.921962</td>\n",
       "      <td>0.921714</td>\n",
       "      <td>0.922702</td>\n",
       "      <td>0.921687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.260226</td>\n",
       "      <td>0.919529</td>\n",
       "      <td>0.918989</td>\n",
       "      <td>0.924739</td>\n",
       "      <td>0.918920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.232552</td>\n",
       "      <td>0.926535</td>\n",
       "      <td>0.926139</td>\n",
       "      <td>0.930207</td>\n",
       "      <td>0.926628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.258453</td>\n",
       "      <td>0.924978</td>\n",
       "      <td>0.924559</td>\n",
       "      <td>0.928125</td>\n",
       "      <td>0.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.230073</td>\n",
       "      <td>0.927897</td>\n",
       "      <td>0.927676</td>\n",
       "      <td>0.929492</td>\n",
       "      <td>0.928107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.285133</td>\n",
       "      <td>0.928773</td>\n",
       "      <td>0.928547</td>\n",
       "      <td>0.930368</td>\n",
       "      <td>0.928993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.297969</td>\n",
       "      <td>0.929454</td>\n",
       "      <td>0.929189</td>\n",
       "      <td>0.931361</td>\n",
       "      <td>0.929637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.274180</td>\n",
       "      <td>0.931011</td>\n",
       "      <td>0.930798</td>\n",
       "      <td>0.931722</td>\n",
       "      <td>0.930893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.111700</td>\n",
       "      <td>0.268519</td>\n",
       "      <td>0.932762</td>\n",
       "      <td>0.932478</td>\n",
       "      <td>0.934757</td>\n",
       "      <td>0.932871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.111700</td>\n",
       "      <td>0.321636</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>0.929006</td>\n",
       "      <td>0.932297</td>\n",
       "      <td>0.929477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.314174</td>\n",
       "      <td>0.927119</td>\n",
       "      <td>0.926750</td>\n",
       "      <td>0.930454</td>\n",
       "      <td>0.927316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.304682</td>\n",
       "      <td>0.931789</td>\n",
       "      <td>0.931483</td>\n",
       "      <td>0.933639</td>\n",
       "      <td>0.931751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.335223</td>\n",
       "      <td>0.925367</td>\n",
       "      <td>0.924950</td>\n",
       "      <td>0.929438</td>\n",
       "      <td>0.925564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.073500</td>\n",
       "      <td>0.288678</td>\n",
       "      <td>0.931692</td>\n",
       "      <td>0.931355</td>\n",
       "      <td>0.934151</td>\n",
       "      <td>0.931693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.073500</td>\n",
       "      <td>0.354553</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>0.926759</td>\n",
       "      <td>0.928701</td>\n",
       "      <td>0.927157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.348784</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.929885</td>\n",
       "      <td>0.932942</td>\n",
       "      <td>0.930314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.358115</td>\n",
       "      <td>0.931108</td>\n",
       "      <td>0.930778</td>\n",
       "      <td>0.933433</td>\n",
       "      <td>0.931153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.352286</td>\n",
       "      <td>0.930524</td>\n",
       "      <td>0.930183</td>\n",
       "      <td>0.932958</td>\n",
       "      <td>0.930548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:20:15,654] Trial 0 finished with value: 3.7242138598056367 and parameters: {'learning_rate': 3e-05, 'num_train_epochs': 5}. Best is trial 0 with value: 3.7242138598056367.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3660' max='3660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3660/3660 37:23, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.461152</td>\n",
       "      <td>0.791379</td>\n",
       "      <td>0.789335</td>\n",
       "      <td>0.832482</td>\n",
       "      <td>0.775212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.341258</td>\n",
       "      <td>0.891603</td>\n",
       "      <td>0.891080</td>\n",
       "      <td>0.895289</td>\n",
       "      <td>0.890827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>0.289134</td>\n",
       "      <td>0.897344</td>\n",
       "      <td>0.896626</td>\n",
       "      <td>0.905870</td>\n",
       "      <td>0.896682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>0.286995</td>\n",
       "      <td>0.897538</td>\n",
       "      <td>0.896639</td>\n",
       "      <td>0.911930</td>\n",
       "      <td>0.896698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.279071</td>\n",
       "      <td>0.920016</td>\n",
       "      <td>0.919603</td>\n",
       "      <td>0.923609</td>\n",
       "      <td>0.920059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.222393</td>\n",
       "      <td>0.928384</td>\n",
       "      <td>0.928012</td>\n",
       "      <td>0.930923</td>\n",
       "      <td>0.928179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.232925</td>\n",
       "      <td>0.920989</td>\n",
       "      <td>0.920545</td>\n",
       "      <td>0.925446</td>\n",
       "      <td>0.921088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.245319</td>\n",
       "      <td>0.924297</td>\n",
       "      <td>0.923877</td>\n",
       "      <td>0.928008</td>\n",
       "      <td>0.924271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.217089</td>\n",
       "      <td>0.932471</td>\n",
       "      <td>0.932189</td>\n",
       "      <td>0.933861</td>\n",
       "      <td>0.932297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.293239</td>\n",
       "      <td>0.926146</td>\n",
       "      <td>0.925934</td>\n",
       "      <td>0.927853</td>\n",
       "      <td>0.926438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.359257</td>\n",
       "      <td>0.921281</td>\n",
       "      <td>0.920965</td>\n",
       "      <td>0.924271</td>\n",
       "      <td>0.921649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.301752</td>\n",
       "      <td>0.931789</td>\n",
       "      <td>0.931514</td>\n",
       "      <td>0.933452</td>\n",
       "      <td>0.931797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.302855</td>\n",
       "      <td>0.927119</td>\n",
       "      <td>0.926773</td>\n",
       "      <td>0.929731</td>\n",
       "      <td>0.927175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.310923</td>\n",
       "      <td>0.929260</td>\n",
       "      <td>0.928920</td>\n",
       "      <td>0.932213</td>\n",
       "      <td>0.929436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.334951</td>\n",
       "      <td>0.932665</td>\n",
       "      <td>0.932368</td>\n",
       "      <td>0.934771</td>\n",
       "      <td>0.932756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.357115</td>\n",
       "      <td>0.924686</td>\n",
       "      <td>0.924256</td>\n",
       "      <td>0.929195</td>\n",
       "      <td>0.924911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.345382</td>\n",
       "      <td>0.930622</td>\n",
       "      <td>0.930271</td>\n",
       "      <td>0.933572</td>\n",
       "      <td>0.930759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.343414</td>\n",
       "      <td>0.929843</td>\n",
       "      <td>0.929495</td>\n",
       "      <td>0.932780</td>\n",
       "      <td>0.929985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:57:40,601] Trial 1 finished with value: 3.7221037008785203 and parameters: {'learning_rate': 3e-05, 'num_train_epochs': 4}. Best is trial 0 with value: 3.7242138598056367.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1830' max='1830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1830/1830 18:40, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.366577</td>\n",
       "      <td>0.849372</td>\n",
       "      <td>0.848200</td>\n",
       "      <td>0.864520</td>\n",
       "      <td>0.845575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.298726</td>\n",
       "      <td>0.903376</td>\n",
       "      <td>0.902899</td>\n",
       "      <td>0.907475</td>\n",
       "      <td>0.903262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.242055</td>\n",
       "      <td>0.920405</td>\n",
       "      <td>0.920086</td>\n",
       "      <td>0.921525</td>\n",
       "      <td>0.919872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.230889</td>\n",
       "      <td>0.926340</td>\n",
       "      <td>0.925953</td>\n",
       "      <td>0.929219</td>\n",
       "      <td>0.926231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.261186</td>\n",
       "      <td>0.925075</td>\n",
       "      <td>0.924794</td>\n",
       "      <td>0.926284</td>\n",
       "      <td>0.924887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.231314</td>\n",
       "      <td>0.927703</td>\n",
       "      <td>0.927321</td>\n",
       "      <td>0.930795</td>\n",
       "      <td>0.927720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.235613</td>\n",
       "      <td>0.924881</td>\n",
       "      <td>0.924411</td>\n",
       "      <td>0.929643</td>\n",
       "      <td>0.924939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.229402</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>0.929089</td>\n",
       "      <td>0.931431</td>\n",
       "      <td>0.929560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.228252</td>\n",
       "      <td>0.928092</td>\n",
       "      <td>0.927719</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>0.928217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 15:16:22,343] Trial 2 finished with value: 3.7154045118570203 and parameters: {'learning_rate': 2e-05, 'num_train_epochs': 2}. Best is trial 0 with value: 3.7242138598056367.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [915/915 09:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.287858</td>\n",
       "      <td>0.891895</td>\n",
       "      <td>0.892042</td>\n",
       "      <td>0.893595</td>\n",
       "      <td>0.892326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.263472</td>\n",
       "      <td>0.908144</td>\n",
       "      <td>0.907696</td>\n",
       "      <td>0.911921</td>\n",
       "      <td>0.908032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.246835</td>\n",
       "      <td>0.912718</td>\n",
       "      <td>0.912102</td>\n",
       "      <td>0.919548</td>\n",
       "      <td>0.912368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.224521</td>\n",
       "      <td>0.920794</td>\n",
       "      <td>0.920321</td>\n",
       "      <td>0.925199</td>\n",
       "      <td>0.920693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 15:25:23,697] Trial 3 finished with value: 3.6870075852707855 and parameters: {'learning_rate': 2e-05, 'num_train_epochs': 1}. Best is trial 0 with value: 3.7242138598056367.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [915/915 08:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.364059</td>\n",
       "      <td>0.843145</td>\n",
       "      <td>0.842029</td>\n",
       "      <td>0.856091</td>\n",
       "      <td>0.839488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276462</td>\n",
       "      <td>0.899095</td>\n",
       "      <td>0.898529</td>\n",
       "      <td>0.903813</td>\n",
       "      <td>0.898503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.244128</td>\n",
       "      <td>0.910188</td>\n",
       "      <td>0.909583</td>\n",
       "      <td>0.916224</td>\n",
       "      <td>0.909672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.241427</td>\n",
       "      <td>0.915734</td>\n",
       "      <td>0.915275</td>\n",
       "      <td>0.919557</td>\n",
       "      <td>0.915553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 15:34:24,406] Trial 4 finished with value: 3.666118683211651 and parameters: {'learning_rate': 1e-05, 'num_train_epochs': 1}. Best is trial 0 with value: 3.7242138598056367.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [915/915 09:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.303253</td>\n",
       "      <td>0.885278</td>\n",
       "      <td>0.884799</td>\n",
       "      <td>0.886488</td>\n",
       "      <td>0.884151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.272751</td>\n",
       "      <td>0.910577</td>\n",
       "      <td>0.910428</td>\n",
       "      <td>0.910577</td>\n",
       "      <td>0.910338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.228079</td>\n",
       "      <td>0.916902</td>\n",
       "      <td>0.916491</td>\n",
       "      <td>0.919466</td>\n",
       "      <td>0.916446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.219647</td>\n",
       "      <td>0.925659</td>\n",
       "      <td>0.925345</td>\n",
       "      <td>0.928093</td>\n",
       "      <td>0.925784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 15:43:25,938] Trial 5 finished with value: 3.7048807723537664 and parameters: {'learning_rate': 3e-05, 'num_train_epochs': 1}. Best is trial 0 with value: 3.7242138598056367.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1830' max='1830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1830/1830 18:41, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.366577</td>\n",
       "      <td>0.849372</td>\n",
       "      <td>0.848200</td>\n",
       "      <td>0.864520</td>\n",
       "      <td>0.845575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.298726</td>\n",
       "      <td>0.903376</td>\n",
       "      <td>0.902899</td>\n",
       "      <td>0.907475</td>\n",
       "      <td>0.903262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.242055</td>\n",
       "      <td>0.920405</td>\n",
       "      <td>0.920086</td>\n",
       "      <td>0.921525</td>\n",
       "      <td>0.919872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.230889</td>\n",
       "      <td>0.926340</td>\n",
       "      <td>0.925953</td>\n",
       "      <td>0.929219</td>\n",
       "      <td>0.926231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.261186</td>\n",
       "      <td>0.925075</td>\n",
       "      <td>0.924794</td>\n",
       "      <td>0.926284</td>\n",
       "      <td>0.924887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.231314</td>\n",
       "      <td>0.927703</td>\n",
       "      <td>0.927321</td>\n",
       "      <td>0.930795</td>\n",
       "      <td>0.927720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.235613</td>\n",
       "      <td>0.924881</td>\n",
       "      <td>0.924411</td>\n",
       "      <td>0.929643</td>\n",
       "      <td>0.924939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.229402</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>0.929089</td>\n",
       "      <td>0.931431</td>\n",
       "      <td>0.929560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.228252</td>\n",
       "      <td>0.928092</td>\n",
       "      <td>0.927719</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>0.928217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 16:02:08,697] Trial 6 finished with value: 3.7154045118570203 and parameters: {'learning_rate': 2e-05, 'num_train_epochs': 2}. Best is trial 0 with value: 3.7242138598056367.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='3660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/3660 02:02 < 35:47, 1.61 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.622866</td>\n",
       "      <td>0.652525</td>\n",
       "      <td>0.648553</td>\n",
       "      <td>0.794777</td>\n",
       "      <td>0.545240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 16:04:12,811] Trial 7 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='4575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/4575 02:02 < 45:07, 1.62 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.654347</td>\n",
       "      <td>0.647271</td>\n",
       "      <td>0.643296</td>\n",
       "      <td>0.820709</td>\n",
       "      <td>0.539004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 16:06:16,679] Trial 8 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='1830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/1830 02:02 < 16:48, 1.62 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.468949</td>\n",
       "      <td>0.780383</td>\n",
       "      <td>0.778189</td>\n",
       "      <td>0.830208</td>\n",
       "      <td>0.760902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 16:08:20,431] Trial 9 pruned. \n"
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    n_trials=10, \n",
    "    hp_space=optuna_hp_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a73ebc-2b56-45fd-93e8-c7f8caf940c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='0', objective=3.7242138598056367, hyperparameters={'learning_rate': 3e-05, 'num_train_epochs': 5}, run_summary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94567a32-b8bc-47b6-a670-1235f2784399",
   "metadata": {},
   "source": [
    "## Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b313c5e4-bd9f-41d2-a073-223e41db8f9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22875' max='22875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22875/22875 3:58:02, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.752081</td>\n",
       "      <td>0.632675</td>\n",
       "      <td>0.628747</td>\n",
       "      <td>0.475035</td>\n",
       "      <td>0.524713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.409956</td>\n",
       "      <td>0.819694</td>\n",
       "      <td>0.817939</td>\n",
       "      <td>0.858141</td>\n",
       "      <td>0.810736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.615500</td>\n",
       "      <td>0.270530</td>\n",
       "      <td>0.892089</td>\n",
       "      <td>0.891395</td>\n",
       "      <td>0.900663</td>\n",
       "      <td>0.891751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.615500</td>\n",
       "      <td>0.320059</td>\n",
       "      <td>0.895689</td>\n",
       "      <td>0.894779</td>\n",
       "      <td>0.907746</td>\n",
       "      <td>0.893785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.234248</td>\n",
       "      <td>0.923227</td>\n",
       "      <td>0.922905</td>\n",
       "      <td>0.925200</td>\n",
       "      <td>0.923208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.306150</td>\n",
       "      <td>0.914177</td>\n",
       "      <td>0.913644</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.913881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.315622</td>\n",
       "      <td>0.915248</td>\n",
       "      <td>0.914649</td>\n",
       "      <td>0.922489</td>\n",
       "      <td>0.915227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.280649</td>\n",
       "      <td>0.910966</td>\n",
       "      <td>0.910295</td>\n",
       "      <td>0.919092</td>\n",
       "      <td>0.910506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.239257</td>\n",
       "      <td>0.921767</td>\n",
       "      <td>0.921898</td>\n",
       "      <td>0.923153</td>\n",
       "      <td>0.922120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.203100</td>\n",
       "      <td>0.320043</td>\n",
       "      <td>0.899582</td>\n",
       "      <td>0.898852</td>\n",
       "      <td>0.910636</td>\n",
       "      <td>0.899763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.203100</td>\n",
       "      <td>0.212970</td>\n",
       "      <td>0.929843</td>\n",
       "      <td>0.929648</td>\n",
       "      <td>0.930898</td>\n",
       "      <td>0.929938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.203100</td>\n",
       "      <td>0.267697</td>\n",
       "      <td>0.920210</td>\n",
       "      <td>0.919945</td>\n",
       "      <td>0.923070</td>\n",
       "      <td>0.920692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.223667</td>\n",
       "      <td>0.925562</td>\n",
       "      <td>0.925262</td>\n",
       "      <td>0.927415</td>\n",
       "      <td>0.925556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.260386</td>\n",
       "      <td>0.927703</td>\n",
       "      <td>0.927566</td>\n",
       "      <td>0.929426</td>\n",
       "      <td>0.928142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.196614</td>\n",
       "      <td>0.932276</td>\n",
       "      <td>0.932074</td>\n",
       "      <td>0.933555</td>\n",
       "      <td>0.932394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.255688</td>\n",
       "      <td>0.925659</td>\n",
       "      <td>0.925215</td>\n",
       "      <td>0.929142</td>\n",
       "      <td>0.925341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.215438</td>\n",
       "      <td>0.929065</td>\n",
       "      <td>0.928669</td>\n",
       "      <td>0.931762</td>\n",
       "      <td>0.928731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.246455</td>\n",
       "      <td>0.929065</td>\n",
       "      <td>0.928712</td>\n",
       "      <td>0.931490</td>\n",
       "      <td>0.928919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.218306</td>\n",
       "      <td>0.931595</td>\n",
       "      <td>0.931224</td>\n",
       "      <td>0.933867</td>\n",
       "      <td>0.931181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>0.185304</td>\n",
       "      <td>0.939866</td>\n",
       "      <td>0.939649</td>\n",
       "      <td>0.941630</td>\n",
       "      <td>0.940076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>0.174522</td>\n",
       "      <td>0.940158</td>\n",
       "      <td>0.939885</td>\n",
       "      <td>0.941381</td>\n",
       "      <td>0.939932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>0.181204</td>\n",
       "      <td>0.941131</td>\n",
       "      <td>0.940897</td>\n",
       "      <td>0.942184</td>\n",
       "      <td>0.941098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.212637</td>\n",
       "      <td>0.932860</td>\n",
       "      <td>0.932578</td>\n",
       "      <td>0.935589</td>\n",
       "      <td>0.933196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.214707</td>\n",
       "      <td>0.935292</td>\n",
       "      <td>0.934971</td>\n",
       "      <td>0.937203</td>\n",
       "      <td>0.935112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.203660</td>\n",
       "      <td>0.938503</td>\n",
       "      <td>0.938212</td>\n",
       "      <td>0.939996</td>\n",
       "      <td>0.938326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.238698</td>\n",
       "      <td>0.932665</td>\n",
       "      <td>0.932352</td>\n",
       "      <td>0.935612</td>\n",
       "      <td>0.932941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.206028</td>\n",
       "      <td>0.941033</td>\n",
       "      <td>0.940785</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.941061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.247127</td>\n",
       "      <td>0.931692</td>\n",
       "      <td>0.931365</td>\n",
       "      <td>0.934336</td>\n",
       "      <td>0.931882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.170451</td>\n",
       "      <td>0.941228</td>\n",
       "      <td>0.941184</td>\n",
       "      <td>0.941127</td>\n",
       "      <td>0.941140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.205718</td>\n",
       "      <td>0.938893</td>\n",
       "      <td>0.938729</td>\n",
       "      <td>0.940183</td>\n",
       "      <td>0.939132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.188281</td>\n",
       "      <td>0.942396</td>\n",
       "      <td>0.942215</td>\n",
       "      <td>0.943138</td>\n",
       "      <td>0.942378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.222884</td>\n",
       "      <td>0.936168</td>\n",
       "      <td>0.935903</td>\n",
       "      <td>0.937595</td>\n",
       "      <td>0.936043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.182291</td>\n",
       "      <td>0.943077</td>\n",
       "      <td>0.942932</td>\n",
       "      <td>0.943622</td>\n",
       "      <td>0.943087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.242556</td>\n",
       "      <td>0.936849</td>\n",
       "      <td>0.936629</td>\n",
       "      <td>0.938869</td>\n",
       "      <td>0.937150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.232077</td>\n",
       "      <td>0.936071</td>\n",
       "      <td>0.935726</td>\n",
       "      <td>0.938658</td>\n",
       "      <td>0.936050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.212764</td>\n",
       "      <td>0.939963</td>\n",
       "      <td>0.939727</td>\n",
       "      <td>0.940858</td>\n",
       "      <td>0.939757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.208121</td>\n",
       "      <td>0.942104</td>\n",
       "      <td>0.941947</td>\n",
       "      <td>0.942516</td>\n",
       "      <td>0.941990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.226632</td>\n",
       "      <td>0.937141</td>\n",
       "      <td>0.936887</td>\n",
       "      <td>0.938687</td>\n",
       "      <td>0.937204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.190075</td>\n",
       "      <td>0.937920</td>\n",
       "      <td>0.937703</td>\n",
       "      <td>0.938897</td>\n",
       "      <td>0.937870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.208402</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.934645</td>\n",
       "      <td>0.937714</td>\n",
       "      <td>0.934933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.230832</td>\n",
       "      <td>0.938114</td>\n",
       "      <td>0.937832</td>\n",
       "      <td>0.940183</td>\n",
       "      <td>0.938244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.203719</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.940479</td>\n",
       "      <td>0.942161</td>\n",
       "      <td>0.940710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.217959</td>\n",
       "      <td>0.937433</td>\n",
       "      <td>0.937111</td>\n",
       "      <td>0.940286</td>\n",
       "      <td>0.937626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.239625</td>\n",
       "      <td>0.931303</td>\n",
       "      <td>0.931017</td>\n",
       "      <td>0.934337</td>\n",
       "      <td>0.931702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.210038</td>\n",
       "      <td>0.940060</td>\n",
       "      <td>0.939826</td>\n",
       "      <td>0.941381</td>\n",
       "      <td>0.940096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.267478</td>\n",
       "      <td>0.932860</td>\n",
       "      <td>0.932514</td>\n",
       "      <td>0.935937</td>\n",
       "      <td>0.933074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.259084</td>\n",
       "      <td>0.936655</td>\n",
       "      <td>0.936259</td>\n",
       "      <td>0.940472</td>\n",
       "      <td>0.936777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.237528</td>\n",
       "      <td>0.938698</td>\n",
       "      <td>0.938362</td>\n",
       "      <td>0.941370</td>\n",
       "      <td>0.938767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.233467</td>\n",
       "      <td>0.939671</td>\n",
       "      <td>0.939473</td>\n",
       "      <td>0.940575</td>\n",
       "      <td>0.939673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.224575</td>\n",
       "      <td>0.943563</td>\n",
       "      <td>0.943437</td>\n",
       "      <td>0.944070</td>\n",
       "      <td>0.943606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.224334</td>\n",
       "      <td>0.937044</td>\n",
       "      <td>0.936790</td>\n",
       "      <td>0.938566</td>\n",
       "      <td>0.937143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.234830</td>\n",
       "      <td>0.938698</td>\n",
       "      <td>0.938404</td>\n",
       "      <td>0.940981</td>\n",
       "      <td>0.938827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.224822</td>\n",
       "      <td>0.940060</td>\n",
       "      <td>0.939793</td>\n",
       "      <td>0.942227</td>\n",
       "      <td>0.940255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.224272</td>\n",
       "      <td>0.939574</td>\n",
       "      <td>0.939252</td>\n",
       "      <td>0.941687</td>\n",
       "      <td>0.939478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.265343</td>\n",
       "      <td>0.935682</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>0.938857</td>\n",
       "      <td>0.935900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.181789</td>\n",
       "      <td>0.944439</td>\n",
       "      <td>0.944228</td>\n",
       "      <td>0.945460</td>\n",
       "      <td>0.944425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.208251</td>\n",
       "      <td>0.940255</td>\n",
       "      <td>0.939951</td>\n",
       "      <td>0.942117</td>\n",
       "      <td>0.940117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.228981</td>\n",
       "      <td>0.939379</td>\n",
       "      <td>0.939030</td>\n",
       "      <td>0.942371</td>\n",
       "      <td>0.939483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.247057</td>\n",
       "      <td>0.940839</td>\n",
       "      <td>0.940554</td>\n",
       "      <td>0.942699</td>\n",
       "      <td>0.940848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.099700</td>\n",
       "      <td>0.230850</td>\n",
       "      <td>0.939185</td>\n",
       "      <td>0.938842</td>\n",
       "      <td>0.942004</td>\n",
       "      <td>0.939246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.099700</td>\n",
       "      <td>0.241221</td>\n",
       "      <td>0.941423</td>\n",
       "      <td>0.941182</td>\n",
       "      <td>0.942939</td>\n",
       "      <td>0.941492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.099700</td>\n",
       "      <td>0.211174</td>\n",
       "      <td>0.945217</td>\n",
       "      <td>0.945030</td>\n",
       "      <td>0.946054</td>\n",
       "      <td>0.945235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.240019</td>\n",
       "      <td>0.939963</td>\n",
       "      <td>0.939710</td>\n",
       "      <td>0.941606</td>\n",
       "      <td>0.940074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.233604</td>\n",
       "      <td>0.943855</td>\n",
       "      <td>0.943575</td>\n",
       "      <td>0.945623</td>\n",
       "      <td>0.943856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.223780</td>\n",
       "      <td>0.942882</td>\n",
       "      <td>0.942617</td>\n",
       "      <td>0.944639</td>\n",
       "      <td>0.942960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.230881</td>\n",
       "      <td>0.945412</td>\n",
       "      <td>0.945140</td>\n",
       "      <td>0.947010</td>\n",
       "      <td>0.945361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.207235</td>\n",
       "      <td>0.944828</td>\n",
       "      <td>0.944570</td>\n",
       "      <td>0.946210</td>\n",
       "      <td>0.944766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.212406</td>\n",
       "      <td>0.945315</td>\n",
       "      <td>0.945120</td>\n",
       "      <td>0.946447</td>\n",
       "      <td>0.945418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.233654</td>\n",
       "      <td>0.943174</td>\n",
       "      <td>0.942896</td>\n",
       "      <td>0.944989</td>\n",
       "      <td>0.943251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.222163</td>\n",
       "      <td>0.946385</td>\n",
       "      <td>0.946169</td>\n",
       "      <td>0.947299</td>\n",
       "      <td>0.946314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.222810</td>\n",
       "      <td>0.945217</td>\n",
       "      <td>0.944941</td>\n",
       "      <td>0.946755</td>\n",
       "      <td>0.945141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.212112</td>\n",
       "      <td>0.946385</td>\n",
       "      <td>0.946131</td>\n",
       "      <td>0.947718</td>\n",
       "      <td>0.946338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.242075</td>\n",
       "      <td>0.944050</td>\n",
       "      <td>0.943811</td>\n",
       "      <td>0.945693</td>\n",
       "      <td>0.944166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.235722</td>\n",
       "      <td>0.944050</td>\n",
       "      <td>0.943841</td>\n",
       "      <td>0.945130</td>\n",
       "      <td>0.944053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.263710</td>\n",
       "      <td>0.943563</td>\n",
       "      <td>0.943360</td>\n",
       "      <td>0.944883</td>\n",
       "      <td>0.943676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.301242</td>\n",
       "      <td>0.939671</td>\n",
       "      <td>0.939392</td>\n",
       "      <td>0.941701</td>\n",
       "      <td>0.939793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.269094</td>\n",
       "      <td>0.942688</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.943650</td>\n",
       "      <td>0.942835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.074300</td>\n",
       "      <td>0.273109</td>\n",
       "      <td>0.941812</td>\n",
       "      <td>0.941585</td>\n",
       "      <td>0.943407</td>\n",
       "      <td>0.941952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.074300</td>\n",
       "      <td>0.255039</td>\n",
       "      <td>0.941715</td>\n",
       "      <td>0.941462</td>\n",
       "      <td>0.943537</td>\n",
       "      <td>0.941840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.258904</td>\n",
       "      <td>0.943077</td>\n",
       "      <td>0.942881</td>\n",
       "      <td>0.944528</td>\n",
       "      <td>0.943271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.221272</td>\n",
       "      <td>0.947066</td>\n",
       "      <td>0.946855</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.947090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.224737</td>\n",
       "      <td>0.944828</td>\n",
       "      <td>0.944549</td>\n",
       "      <td>0.946542</td>\n",
       "      <td>0.944801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>0.223265</td>\n",
       "      <td>0.946580</td>\n",
       "      <td>0.946350</td>\n",
       "      <td>0.947800</td>\n",
       "      <td>0.946580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>0.250114</td>\n",
       "      <td>0.945607</td>\n",
       "      <td>0.945342</td>\n",
       "      <td>0.947138</td>\n",
       "      <td>0.945559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.295743</td>\n",
       "      <td>0.939379</td>\n",
       "      <td>0.939103</td>\n",
       "      <td>0.942134</td>\n",
       "      <td>0.939703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.216084</td>\n",
       "      <td>0.946872</td>\n",
       "      <td>0.946706</td>\n",
       "      <td>0.947477</td>\n",
       "      <td>0.946842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.222326</td>\n",
       "      <td>0.946482</td>\n",
       "      <td>0.946296</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.946514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.245808</td>\n",
       "      <td>0.943369</td>\n",
       "      <td>0.943116</td>\n",
       "      <td>0.945071</td>\n",
       "      <td>0.943467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.270273</td>\n",
       "      <td>0.943855</td>\n",
       "      <td>0.943631</td>\n",
       "      <td>0.945273</td>\n",
       "      <td>0.943963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.237308</td>\n",
       "      <td>0.946093</td>\n",
       "      <td>0.945866</td>\n",
       "      <td>0.947412</td>\n",
       "      <td>0.946133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.232624</td>\n",
       "      <td>0.946191</td>\n",
       "      <td>0.945981</td>\n",
       "      <td>0.947306</td>\n",
       "      <td>0.946225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.265891</td>\n",
       "      <td>0.944147</td>\n",
       "      <td>0.943879</td>\n",
       "      <td>0.945893</td>\n",
       "      <td>0.944205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.281118</td>\n",
       "      <td>0.945315</td>\n",
       "      <td>0.945105</td>\n",
       "      <td>0.946568</td>\n",
       "      <td>0.945420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.282629</td>\n",
       "      <td>0.942590</td>\n",
       "      <td>0.942276</td>\n",
       "      <td>0.944826</td>\n",
       "      <td>0.942619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.277123</td>\n",
       "      <td>0.943758</td>\n",
       "      <td>0.943516</td>\n",
       "      <td>0.945189</td>\n",
       "      <td>0.943807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.267018</td>\n",
       "      <td>0.943369</td>\n",
       "      <td>0.943090</td>\n",
       "      <td>0.945151</td>\n",
       "      <td>0.943396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.259593</td>\n",
       "      <td>0.945315</td>\n",
       "      <td>0.945111</td>\n",
       "      <td>0.946235</td>\n",
       "      <td>0.945270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.262050</td>\n",
       "      <td>0.945217</td>\n",
       "      <td>0.945041</td>\n",
       "      <td>0.945987</td>\n",
       "      <td>0.945226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.273727</td>\n",
       "      <td>0.944731</td>\n",
       "      <td>0.944547</td>\n",
       "      <td>0.945671</td>\n",
       "      <td>0.944791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.259172</td>\n",
       "      <td>0.945412</td>\n",
       "      <td>0.945249</td>\n",
       "      <td>0.946109</td>\n",
       "      <td>0.945452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.268912</td>\n",
       "      <td>0.944926</td>\n",
       "      <td>0.944729</td>\n",
       "      <td>0.946023</td>\n",
       "      <td>0.945013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.945315</td>\n",
       "      <td>0.945115</td>\n",
       "      <td>0.946557</td>\n",
       "      <td>0.945445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.268529</td>\n",
       "      <td>0.945509</td>\n",
       "      <td>0.945283</td>\n",
       "      <td>0.946887</td>\n",
       "      <td>0.945593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.263746</td>\n",
       "      <td>0.946580</td>\n",
       "      <td>0.946386</td>\n",
       "      <td>0.947429</td>\n",
       "      <td>0.946564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.255141</td>\n",
       "      <td>0.946093</td>\n",
       "      <td>0.945896</td>\n",
       "      <td>0.947060</td>\n",
       "      <td>0.946119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.265025</td>\n",
       "      <td>0.946580</td>\n",
       "      <td>0.946372</td>\n",
       "      <td>0.947579</td>\n",
       "      <td>0.946577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.264349</td>\n",
       "      <td>0.946288</td>\n",
       "      <td>0.946098</td>\n",
       "      <td>0.947308</td>\n",
       "      <td>0.946364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.271574</td>\n",
       "      <td>0.945899</td>\n",
       "      <td>0.945672</td>\n",
       "      <td>0.947185</td>\n",
       "      <td>0.945947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.274211</td>\n",
       "      <td>0.944634</td>\n",
       "      <td>0.944385</td>\n",
       "      <td>0.946234</td>\n",
       "      <td>0.944713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.270267</td>\n",
       "      <td>0.945509</td>\n",
       "      <td>0.945276</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.945583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.267815</td>\n",
       "      <td>0.947066</td>\n",
       "      <td>0.946863</td>\n",
       "      <td>0.948225</td>\n",
       "      <td>0.947149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.267663</td>\n",
       "      <td>0.946580</td>\n",
       "      <td>0.946359</td>\n",
       "      <td>0.947761</td>\n",
       "      <td>0.946606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.265294</td>\n",
       "      <td>0.946385</td>\n",
       "      <td>0.946169</td>\n",
       "      <td>0.947528</td>\n",
       "      <td>0.946411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.264403</td>\n",
       "      <td>0.946774</td>\n",
       "      <td>0.946556</td>\n",
       "      <td>0.947918</td>\n",
       "      <td>0.946801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22875, training_loss=0.12231927673673369, metrics={'train_runtime': 14282.6431, 'train_samples_per_second': 51.256, 'train_steps_per_second': 1.602, 'total_flos': 5.552116534613016e+16, 'train_loss': 0.12231927673673369, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\",\n",
    "    tf32=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "    \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c033e6c-1341-4ddd-9c81-10f456146e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_store_path.parent / (model_store_path.stem + \".out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fc1ce-86d3-4168-a4a1-71f521fe37a3",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b699f7bb-e4c9-45ad-9fd1-d7425dd6f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7590-31cc-40ad-b212-d0c47500277f",
   "metadata": {},
   "source": [
    "## Test on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdfafa9a-6265-4792-8513-149ab1b0545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"validation\"])\n",
    "val = generate_doc_df(data[\"validation\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c16081ff-6aa6-4620-8cdf-50f3e07b73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       1.00      0.98      0.99      3428\n",
      "        REFUTES       0.95      0.90      0.92      3384\n",
      "       SUPPORTS       0.90      0.95      0.93      3465\n",
      "\n",
      "       accuracy                           0.95     10277\n",
      "      macro avg       0.95      0.95      0.95     10277\n",
      "   weighted avg       0.95      0.95      0.95     10277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=val[\"actual\"], y_pred=val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b4bf9-0d9a-4143-bc29-4b980f4598f0",
   "metadata": {},
   "source": [
    "## Test on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be065c-4aa2-45b6-8d70-4bd40382b826",
   "metadata": {},
   "source": [
    "### FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebee1beb-fbdc-4169-beef-1fb05b5e35ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"fever_test\"])\n",
    "ftes = generate_doc_df(data[\"fever_test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f275a383-337c-48ce-ab31-7a38cb0cf257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       1.00      0.99      1.00      3333\n",
      "        REFUTES       0.95      0.88      0.92      3333\n",
      "       SUPPORTS       0.89      0.96      0.92      3333\n",
      "\n",
      "       accuracy                           0.95      9999\n",
      "      macro avg       0.95      0.95      0.95      9999\n",
      "   weighted avg       0.95      0.95      0.95      9999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=ftes[\"actual\"], y_pred=ftes[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf531418-5f83-419c-ae50-00bca4dbbee2",
   "metadata": {},
   "source": [
    "### Climate-FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aa5d12b-765d-4c45-86f0-f02528b8cbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"climatefever_test\"])\n",
    "cftes = generate_doc_df(data[\"climatefever_test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bc7c6c3-965d-4a9f-b5f6-1d6dc6dc4505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.91      0.66      0.77        47\n",
      "        REFUTES       0.48      0.60      0.54        25\n",
      "       SUPPORTS       0.71      0.78      0.74        65\n",
      "\n",
      "       accuracy                           0.71       137\n",
      "      macro avg       0.70      0.68      0.68       137\n",
      "   weighted avg       0.74      0.71      0.71       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=cftes[\"actual\"], y_pred=cftes[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae0ef8-63c5-4092-b9a3-1528fe23a2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
