{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402ecfd6-f01c-43ef-aaa4-018f9dc63dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/gen\")\n",
    "sys.path.insert(1, \"../src/rte\")\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from util import read_data, write_jsonl\n",
    "from aggregate import generate_micro_macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1929c84-28f4-4949-b880-a85b8f8b8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "LOOKUP = {\n",
    "    \"verifiable\": {\"no\": \"NOT VERIFIABLE\", \"yes\": \"VERIFIABLE\"},\n",
    "    \"label\": {\"nei\": \"NOT ENOUGH INFO\", \"r\": \"REFUTES\", \"s\": \"SUPPORTS\"}\n",
    "}\n",
    "\n",
    "SEED = 123456789\n",
    "\n",
    "LABEL2ID = {\"SUPPORTS\": 0, \"NOT ENOUGH INFO\": 1, \"REFUTES\": 2}\n",
    "ID2LABEL = {0: \"SUPPORTS\", 1: \"NOT ENOUGH INFO\", 2: \"REFUTES\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05b8ab-6775-4eed-98be-10904943ded1",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aacd87-5b38-45b8-8be3-f7046bfa06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    TextClassificationPipeline,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd986a-1e45-4f38-8487-dddf2ee3ef24",
   "metadata": {},
   "source": [
    "# Huggingface Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b42b8e-2eb3-444b-9275-192519540439",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e5281d-7e7d-410e-b1ba-730e334247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2c86dd-cb1b-4b77-836d-1e5619f4bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        num_labels=3, \n",
    "        id2label=ID2LABEL, \n",
    "        label2id=LABEL2ID\n",
    "    )\n",
    "\n",
    "model = model_init()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, do_lower_case=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"evidence\"], examples[\"claim\"], max_length=512, truncation=\"only_first\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4834e0-770d-49dc-8041-f4d9d2729053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"fever\", \"climatefeverpure\", \"fever-climatefeverpure\", \"climatefever\", \"fever-climatefever\"]\n",
    "task = [\"hp_tune\", \"output\"]\n",
    "doc_sent = [\"doc\", \"sent\"]\n",
    "\n",
    "di = 4\n",
    "ti = 0\n",
    "ds = 1\n",
    "\n",
    "model_store_path = Path(\"/users/k21190024/study/fact-check-transfer-learning/scratch/thesis/models\").joinpath(model_checkpoint)\n",
    "model_store_path.mkdir(exist_ok=True)\n",
    "model_store_path = model_store_path / f\"{dataset[di]}-{model_checkpoint}-{doc_sent[ds]}-{task[ti]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940210d7-1da5-4e6f-b4f6-95304ceb496b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a618a5-0c63-4a99-b7cb-89deea2416f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/228290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datap = Path(f\"/users/k21190024/study/fact-check-transfer-learning/scratch/dumps/bert-data-{doc_sent[ds]}-evidence\")\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.train.n5.jsonl\")),\n",
    "    \"validation\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.dev.n5.jsonl\")),\n",
    "    \"fever_test\": Dataset.from_list(read_data(datap / f\"{dataset[0]}.test.n5.jsonl\")),\n",
    "    \"climatefever_test\": Dataset.from_list(read_data(datap / f\"{dataset[1]}.test.n5.jsonl\"))\n",
    "}).map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d7f9a-4ddc-4a9b-b6cf-d31233fff94b",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06cfa045-016f-44e2-91c9-567ddd56ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "epoch = 4\n",
    "metric_name = \"f1\"\n",
    "warmup_ratio=0.1\n",
    "save_steps=200\n",
    "eval_steps=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666373d-06fa-476c-9aaf-ffb3daadbf4b",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967aecb0-5bc7-488c-aefa-caf4525216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shard the data if the dataset is large for hyperparameter tuning\n",
    "shard = data[\"train\"].num_rows > 50000\n",
    "hp_tune_train = data[\"train\"].shuffle(seed=SEED).shard(num_shards=5, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e23d248-bb05-43da-a056-4e1fcc2a17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy = \"no\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "_ = model.train()\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=hp_tune_train if shard else data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5445d33-24fc-4a6d-8ed2-a89b26aa65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-5, 3e-5, 2e-5]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "    }\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d4edde-28e8-4b30-a700-dc3f4a2a2ce9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-02 22:24:20,485] A new study created in memory with name: no-name-8cdc174e-8dd2-45b5-99f7-65e9bbd09896\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2834' max='2834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2834/2834 11:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.709205</td>\n",
       "      <td>0.664948</td>\n",
       "      <td>0.630959</td>\n",
       "      <td>0.465465</td>\n",
       "      <td>0.520997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.407057</td>\n",
       "      <td>0.833909</td>\n",
       "      <td>0.823710</td>\n",
       "      <td>0.831642</td>\n",
       "      <td>0.817921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.339742</td>\n",
       "      <td>0.873279</td>\n",
       "      <td>0.866250</td>\n",
       "      <td>0.877925</td>\n",
       "      <td>0.862448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.309051</td>\n",
       "      <td>0.890386</td>\n",
       "      <td>0.883398</td>\n",
       "      <td>0.890336</td>\n",
       "      <td>0.881781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.258453</td>\n",
       "      <td>0.911184</td>\n",
       "      <td>0.903836</td>\n",
       "      <td>0.908845</td>\n",
       "      <td>0.904794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.281829</td>\n",
       "      <td>0.904447</td>\n",
       "      <td>0.901484</td>\n",
       "      <td>0.901296</td>\n",
       "      <td>0.898728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.247656</td>\n",
       "      <td>0.914055</td>\n",
       "      <td>0.906301</td>\n",
       "      <td>0.912077</td>\n",
       "      <td>0.908129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.295872</td>\n",
       "      <td>0.911008</td>\n",
       "      <td>0.905235</td>\n",
       "      <td>0.907248</td>\n",
       "      <td>0.904746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.268496</td>\n",
       "      <td>0.914582</td>\n",
       "      <td>0.909214</td>\n",
       "      <td>0.910846</td>\n",
       "      <td>0.908957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>0.295163</td>\n",
       "      <td>0.913352</td>\n",
       "      <td>0.909440</td>\n",
       "      <td>0.909210</td>\n",
       "      <td>0.907789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>0.271577</td>\n",
       "      <td>0.918039</td>\n",
       "      <td>0.913204</td>\n",
       "      <td>0.913844</td>\n",
       "      <td>0.912653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>0.286869</td>\n",
       "      <td>0.914230</td>\n",
       "      <td>0.910143</td>\n",
       "      <td>0.911731</td>\n",
       "      <td>0.908782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.269621</td>\n",
       "      <td>0.919972</td>\n",
       "      <td>0.916189</td>\n",
       "      <td>0.916061</td>\n",
       "      <td>0.915080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.280269</td>\n",
       "      <td>0.917394</td>\n",
       "      <td>0.914291</td>\n",
       "      <td>0.913460</td>\n",
       "      <td>0.912561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-02 22:35:48,568] Trial 0 finished with value: 3.6577068518348232 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}. Best is trial 0 with value: 3.6577068518348232.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11336' max='11336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11336/11336 33:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.815616</td>\n",
       "      <td>0.615150</td>\n",
       "      <td>0.590411</td>\n",
       "      <td>0.446991</td>\n",
       "      <td>0.486206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.736004</td>\n",
       "      <td>0.655750</td>\n",
       "      <td>0.624416</td>\n",
       "      <td>0.464368</td>\n",
       "      <td>0.515318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.610219</td>\n",
       "      <td>0.673736</td>\n",
       "      <td>0.641676</td>\n",
       "      <td>0.744473</td>\n",
       "      <td>0.543583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.405915</td>\n",
       "      <td>0.843576</td>\n",
       "      <td>0.833540</td>\n",
       "      <td>0.831153</td>\n",
       "      <td>0.831706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.374655</td>\n",
       "      <td>0.865839</td>\n",
       "      <td>0.858709</td>\n",
       "      <td>0.859833</td>\n",
       "      <td>0.855843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.360239</td>\n",
       "      <td>0.883824</td>\n",
       "      <td>0.879217</td>\n",
       "      <td>0.880916</td>\n",
       "      <td>0.876090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.290826</td>\n",
       "      <td>0.900873</td>\n",
       "      <td>0.895359</td>\n",
       "      <td>0.893537</td>\n",
       "      <td>0.894380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.250500</td>\n",
       "      <td>0.297646</td>\n",
       "      <td>0.898061</td>\n",
       "      <td>0.890760</td>\n",
       "      <td>0.896296</td>\n",
       "      <td>0.890459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.250500</td>\n",
       "      <td>0.305376</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.895757</td>\n",
       "      <td>0.899722</td>\n",
       "      <td>0.894439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.231300</td>\n",
       "      <td>0.317847</td>\n",
       "      <td>0.900521</td>\n",
       "      <td>0.890517</td>\n",
       "      <td>0.900745</td>\n",
       "      <td>0.892190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.231300</td>\n",
       "      <td>0.301028</td>\n",
       "      <td>0.901576</td>\n",
       "      <td>0.894928</td>\n",
       "      <td>0.900322</td>\n",
       "      <td>0.894108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.231300</td>\n",
       "      <td>0.365808</td>\n",
       "      <td>0.888804</td>\n",
       "      <td>0.885483</td>\n",
       "      <td>0.892590</td>\n",
       "      <td>0.881985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.234300</td>\n",
       "      <td>0.310145</td>\n",
       "      <td>0.895776</td>\n",
       "      <td>0.888084</td>\n",
       "      <td>0.897029</td>\n",
       "      <td>0.887446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.234300</td>\n",
       "      <td>0.276514</td>\n",
       "      <td>0.903978</td>\n",
       "      <td>0.895931</td>\n",
       "      <td>0.904821</td>\n",
       "      <td>0.896379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.334559</td>\n",
       "      <td>0.907142</td>\n",
       "      <td>0.901779</td>\n",
       "      <td>0.903032</td>\n",
       "      <td>0.900768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.344282</td>\n",
       "      <td>0.910774</td>\n",
       "      <td>0.905934</td>\n",
       "      <td>0.905531</td>\n",
       "      <td>0.905151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.305694</td>\n",
       "      <td>0.916340</td>\n",
       "      <td>0.911257</td>\n",
       "      <td>0.911898</td>\n",
       "      <td>0.910884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.165300</td>\n",
       "      <td>0.312829</td>\n",
       "      <td>0.910774</td>\n",
       "      <td>0.907220</td>\n",
       "      <td>0.905678</td>\n",
       "      <td>0.905373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.165300</td>\n",
       "      <td>0.372968</td>\n",
       "      <td>0.902103</td>\n",
       "      <td>0.896530</td>\n",
       "      <td>0.899909</td>\n",
       "      <td>0.894886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.428180</td>\n",
       "      <td>0.901693</td>\n",
       "      <td>0.899891</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.896277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.423633</td>\n",
       "      <td>0.899584</td>\n",
       "      <td>0.894442</td>\n",
       "      <td>0.900721</td>\n",
       "      <td>0.892596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.356783</td>\n",
       "      <td>0.906439</td>\n",
       "      <td>0.901906</td>\n",
       "      <td>0.904385</td>\n",
       "      <td>0.900161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>0.380246</td>\n",
       "      <td>0.910364</td>\n",
       "      <td>0.906080</td>\n",
       "      <td>0.908071</td>\n",
       "      <td>0.904710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>0.353620</td>\n",
       "      <td>0.909602</td>\n",
       "      <td>0.905956</td>\n",
       "      <td>0.906860</td>\n",
       "      <td>0.904090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.342308</td>\n",
       "      <td>0.912004</td>\n",
       "      <td>0.906841</td>\n",
       "      <td>0.910458</td>\n",
       "      <td>0.906116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.352621</td>\n",
       "      <td>0.904447</td>\n",
       "      <td>0.898946</td>\n",
       "      <td>0.905239</td>\n",
       "      <td>0.897986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.311986</td>\n",
       "      <td>0.918156</td>\n",
       "      <td>0.912786</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.912642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.340218</td>\n",
       "      <td>0.915344</td>\n",
       "      <td>0.912793</td>\n",
       "      <td>0.909686</td>\n",
       "      <td>0.910483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.364934</td>\n",
       "      <td>0.913938</td>\n",
       "      <td>0.910515</td>\n",
       "      <td>0.910272</td>\n",
       "      <td>0.908607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.365748</td>\n",
       "      <td>0.919855</td>\n",
       "      <td>0.916860</td>\n",
       "      <td>0.915241</td>\n",
       "      <td>0.915385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.381781</td>\n",
       "      <td>0.920089</td>\n",
       "      <td>0.915788</td>\n",
       "      <td>0.915466</td>\n",
       "      <td>0.915017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.381809</td>\n",
       "      <td>0.918273</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>0.913700</td>\n",
       "      <td>0.912897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.474382</td>\n",
       "      <td>0.911946</td>\n",
       "      <td>0.909534</td>\n",
       "      <td>0.908406</td>\n",
       "      <td>0.906912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.428466</td>\n",
       "      <td>0.913352</td>\n",
       "      <td>0.911210</td>\n",
       "      <td>0.908675</td>\n",
       "      <td>0.908591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.414281</td>\n",
       "      <td>0.915871</td>\n",
       "      <td>0.911639</td>\n",
       "      <td>0.911624</td>\n",
       "      <td>0.910534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.430383</td>\n",
       "      <td>0.913762</td>\n",
       "      <td>0.909046</td>\n",
       "      <td>0.911032</td>\n",
       "      <td>0.907968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.406110</td>\n",
       "      <td>0.917921</td>\n",
       "      <td>0.914320</td>\n",
       "      <td>0.912930</td>\n",
       "      <td>0.912937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.463438</td>\n",
       "      <td>0.910188</td>\n",
       "      <td>0.905402</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.903892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.399889</td>\n",
       "      <td>0.917921</td>\n",
       "      <td>0.911999</td>\n",
       "      <td>0.914648</td>\n",
       "      <td>0.912146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.415955</td>\n",
       "      <td>0.916105</td>\n",
       "      <td>0.910723</td>\n",
       "      <td>0.912758</td>\n",
       "      <td>0.910674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.453298</td>\n",
       "      <td>0.911243</td>\n",
       "      <td>0.906726</td>\n",
       "      <td>0.908621</td>\n",
       "      <td>0.905459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.418133</td>\n",
       "      <td>0.913469</td>\n",
       "      <td>0.909301</td>\n",
       "      <td>0.910692</td>\n",
       "      <td>0.907900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.420712</td>\n",
       "      <td>0.918097</td>\n",
       "      <td>0.914246</td>\n",
       "      <td>0.913797</td>\n",
       "      <td>0.913185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.422602</td>\n",
       "      <td>0.919386</td>\n",
       "      <td>0.914524</td>\n",
       "      <td>0.915855</td>\n",
       "      <td>0.914177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.451397</td>\n",
       "      <td>0.918273</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.914259</td>\n",
       "      <td>0.913303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.458586</td>\n",
       "      <td>0.917746</td>\n",
       "      <td>0.913567</td>\n",
       "      <td>0.914026</td>\n",
       "      <td>0.912615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.460701</td>\n",
       "      <td>0.916574</td>\n",
       "      <td>0.911978</td>\n",
       "      <td>0.913149</td>\n",
       "      <td>0.911270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.467780</td>\n",
       "      <td>0.917394</td>\n",
       "      <td>0.913443</td>\n",
       "      <td>0.913652</td>\n",
       "      <td>0.912432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.449918</td>\n",
       "      <td>0.919796</td>\n",
       "      <td>0.915712</td>\n",
       "      <td>0.915721</td>\n",
       "      <td>0.915014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.480871</td>\n",
       "      <td>0.917277</td>\n",
       "      <td>0.913268</td>\n",
       "      <td>0.913733</td>\n",
       "      <td>0.912232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.455309</td>\n",
       "      <td>0.918742</td>\n",
       "      <td>0.915492</td>\n",
       "      <td>0.914218</td>\n",
       "      <td>0.914187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.470852</td>\n",
       "      <td>0.917160</td>\n",
       "      <td>0.912980</td>\n",
       "      <td>0.913623</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.455368</td>\n",
       "      <td>0.918976</td>\n",
       "      <td>0.914907</td>\n",
       "      <td>0.915034</td>\n",
       "      <td>0.914054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.451407</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.915102</td>\n",
       "      <td>0.915421</td>\n",
       "      <td>0.914376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.454081</td>\n",
       "      <td>0.918917</td>\n",
       "      <td>0.914998</td>\n",
       "      <td>0.914570</td>\n",
       "      <td>0.914048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.451825</td>\n",
       "      <td>0.919386</td>\n",
       "      <td>0.915469</td>\n",
       "      <td>0.915087</td>\n",
       "      <td>0.914566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-02 23:09:34,812] Trial 1 finished with value: 3.6645075218228365 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}. Best is trial 1 with value: 3.6645075218228365.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2834' max='2834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2834/2834 08:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.683713</td>\n",
       "      <td>0.671627</td>\n",
       "      <td>0.637717</td>\n",
       "      <td>0.694770</td>\n",
       "      <td>0.537583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.452704</td>\n",
       "      <td>0.852774</td>\n",
       "      <td>0.837278</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>0.836443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.483500</td>\n",
       "      <td>0.562799</td>\n",
       "      <td>0.822309</td>\n",
       "      <td>0.821450</td>\n",
       "      <td>0.824317</td>\n",
       "      <td>0.812734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.483500</td>\n",
       "      <td>0.309705</td>\n",
       "      <td>0.889449</td>\n",
       "      <td>0.885076</td>\n",
       "      <td>0.882851</td>\n",
       "      <td>0.882855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.893081</td>\n",
       "      <td>0.888372</td>\n",
       "      <td>0.886566</td>\n",
       "      <td>0.886387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.455493</td>\n",
       "      <td>0.861269</td>\n",
       "      <td>0.856195</td>\n",
       "      <td>0.875219</td>\n",
       "      <td>0.851888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.354575</td>\n",
       "      <td>0.895132</td>\n",
       "      <td>0.889138</td>\n",
       "      <td>0.893245</td>\n",
       "      <td>0.887550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.284758</td>\n",
       "      <td>0.900814</td>\n",
       "      <td>0.897475</td>\n",
       "      <td>0.897678</td>\n",
       "      <td>0.894895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.322178</td>\n",
       "      <td>0.901986</td>\n",
       "      <td>0.895329</td>\n",
       "      <td>0.901366</td>\n",
       "      <td>0.894573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>0.300177</td>\n",
       "      <td>0.902923</td>\n",
       "      <td>0.894486</td>\n",
       "      <td>0.902857</td>\n",
       "      <td>0.894935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>0.290667</td>\n",
       "      <td>0.908020</td>\n",
       "      <td>0.902840</td>\n",
       "      <td>0.904245</td>\n",
       "      <td>0.901635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>0.342048</td>\n",
       "      <td>0.898471</td>\n",
       "      <td>0.897134</td>\n",
       "      <td>0.897275</td>\n",
       "      <td>0.893243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.273670</td>\n",
       "      <td>0.912121</td>\n",
       "      <td>0.907265</td>\n",
       "      <td>0.908856</td>\n",
       "      <td>0.906130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.274389</td>\n",
       "      <td>0.915285</td>\n",
       "      <td>0.910015</td>\n",
       "      <td>0.911906</td>\n",
       "      <td>0.909524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-02 23:18:01,616] Trial 2 finished with value: 3.646729813298749 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 1}. Best is trial 1 with value: 3.6645075218228365.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5668' max='5668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5668/5668 22:42, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.723697</td>\n",
       "      <td>0.663484</td>\n",
       "      <td>0.629558</td>\n",
       "      <td>0.463476</td>\n",
       "      <td>0.519598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.402827</td>\n",
       "      <td>0.837952</td>\n",
       "      <td>0.825512</td>\n",
       "      <td>0.832836</td>\n",
       "      <td>0.821273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.544200</td>\n",
       "      <td>0.308936</td>\n",
       "      <td>0.887750</td>\n",
       "      <td>0.880629</td>\n",
       "      <td>0.883056</td>\n",
       "      <td>0.879457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.544200</td>\n",
       "      <td>0.264701</td>\n",
       "      <td>0.905501</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.900985</td>\n",
       "      <td>0.899128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.260209</td>\n",
       "      <td>0.909426</td>\n",
       "      <td>0.902045</td>\n",
       "      <td>0.905931</td>\n",
       "      <td>0.903234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.282013</td>\n",
       "      <td>0.904095</td>\n",
       "      <td>0.901582</td>\n",
       "      <td>0.898971</td>\n",
       "      <td>0.898878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.248275</td>\n",
       "      <td>0.910657</td>\n",
       "      <td>0.902709</td>\n",
       "      <td>0.908403</td>\n",
       "      <td>0.904348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.321022</td>\n",
       "      <td>0.909309</td>\n",
       "      <td>0.903279</td>\n",
       "      <td>0.904720</td>\n",
       "      <td>0.903529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.303957</td>\n",
       "      <td>0.907493</td>\n",
       "      <td>0.904371</td>\n",
       "      <td>0.902773</td>\n",
       "      <td>0.902327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.343673</td>\n",
       "      <td>0.898822</td>\n",
       "      <td>0.893803</td>\n",
       "      <td>0.898185</td>\n",
       "      <td>0.891581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.277786</td>\n",
       "      <td>0.910950</td>\n",
       "      <td>0.905585</td>\n",
       "      <td>0.906554</td>\n",
       "      <td>0.904807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.345766</td>\n",
       "      <td>0.896362</td>\n",
       "      <td>0.892736</td>\n",
       "      <td>0.899350</td>\n",
       "      <td>0.889885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.282185</td>\n",
       "      <td>0.914816</td>\n",
       "      <td>0.910018</td>\n",
       "      <td>0.913017</td>\n",
       "      <td>0.909677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.277337</td>\n",
       "      <td>0.916925</td>\n",
       "      <td>0.913525</td>\n",
       "      <td>0.913539</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.304570</td>\n",
       "      <td>0.919152</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.914481</td>\n",
       "      <td>0.914790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.359838</td>\n",
       "      <td>0.916281</td>\n",
       "      <td>0.911786</td>\n",
       "      <td>0.912285</td>\n",
       "      <td>0.911147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.348040</td>\n",
       "      <td>0.919503</td>\n",
       "      <td>0.915458</td>\n",
       "      <td>0.914520</td>\n",
       "      <td>0.914612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.391243</td>\n",
       "      <td>0.910129</td>\n",
       "      <td>0.905278</td>\n",
       "      <td>0.906956</td>\n",
       "      <td>0.904234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>0.913938</td>\n",
       "      <td>0.910850</td>\n",
       "      <td>0.908979</td>\n",
       "      <td>0.908839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.348330</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.915371</td>\n",
       "      <td>0.915213</td>\n",
       "      <td>0.914454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.343488</td>\n",
       "      <td>0.918566</td>\n",
       "      <td>0.915194</td>\n",
       "      <td>0.914984</td>\n",
       "      <td>0.913994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.373656</td>\n",
       "      <td>0.919679</td>\n",
       "      <td>0.915373</td>\n",
       "      <td>0.915560</td>\n",
       "      <td>0.914758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.387836</td>\n",
       "      <td>0.919035</td>\n",
       "      <td>0.915151</td>\n",
       "      <td>0.914665</td>\n",
       "      <td>0.914181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.397413</td>\n",
       "      <td>0.918859</td>\n",
       "      <td>0.914712</td>\n",
       "      <td>0.914614</td>\n",
       "      <td>0.913932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.448709</td>\n",
       "      <td>0.912942</td>\n",
       "      <td>0.908665</td>\n",
       "      <td>0.910545</td>\n",
       "      <td>0.907366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.395837</td>\n",
       "      <td>0.920733</td>\n",
       "      <td>0.916677</td>\n",
       "      <td>0.916823</td>\n",
       "      <td>0.916023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.399496</td>\n",
       "      <td>0.919445</td>\n",
       "      <td>0.916280</td>\n",
       "      <td>0.914932</td>\n",
       "      <td>0.914863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.410642</td>\n",
       "      <td>0.917921</td>\n",
       "      <td>0.914341</td>\n",
       "      <td>0.914127</td>\n",
       "      <td>0.913017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-02 23:40:45,653] Trial 3 finished with value: 3.6594067661703074 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 1 with value: 3.6645075218228365.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14170' max='14170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14170/14170 42:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.836464</td>\n",
       "      <td>0.604546</td>\n",
       "      <td>0.581877</td>\n",
       "      <td>0.443671</td>\n",
       "      <td>0.478743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.745774</td>\n",
       "      <td>0.651415</td>\n",
       "      <td>0.621068</td>\n",
       "      <td>0.463926</td>\n",
       "      <td>0.512608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.666423</td>\n",
       "      <td>0.662312</td>\n",
       "      <td>0.630149</td>\n",
       "      <td>0.804531</td>\n",
       "      <td>0.521339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.496443</td>\n",
       "      <td>0.784815</td>\n",
       "      <td>0.765550</td>\n",
       "      <td>0.813738</td>\n",
       "      <td>0.747528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0.396364</td>\n",
       "      <td>0.858047</td>\n",
       "      <td>0.848980</td>\n",
       "      <td>0.850459</td>\n",
       "      <td>0.846319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0.354929</td>\n",
       "      <td>0.881481</td>\n",
       "      <td>0.874966</td>\n",
       "      <td>0.877748</td>\n",
       "      <td>0.872377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0.321176</td>\n",
       "      <td>0.892319</td>\n",
       "      <td>0.886009</td>\n",
       "      <td>0.884165</td>\n",
       "      <td>0.884992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.315483</td>\n",
       "      <td>0.887281</td>\n",
       "      <td>0.880167</td>\n",
       "      <td>0.884497</td>\n",
       "      <td>0.878795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.312387</td>\n",
       "      <td>0.897006</td>\n",
       "      <td>0.889903</td>\n",
       "      <td>0.896263</td>\n",
       "      <td>0.889006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.329408</td>\n",
       "      <td>0.896479</td>\n",
       "      <td>0.885678</td>\n",
       "      <td>0.897249</td>\n",
       "      <td>0.887762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.288299</td>\n",
       "      <td>0.903275</td>\n",
       "      <td>0.896910</td>\n",
       "      <td>0.900333</td>\n",
       "      <td>0.896027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.332819</td>\n",
       "      <td>0.891616</td>\n",
       "      <td>0.886636</td>\n",
       "      <td>0.895627</td>\n",
       "      <td>0.884103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.325974</td>\n",
       "      <td>0.896831</td>\n",
       "      <td>0.891866</td>\n",
       "      <td>0.894196</td>\n",
       "      <td>0.890168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.289064</td>\n",
       "      <td>0.904740</td>\n",
       "      <td>0.895759</td>\n",
       "      <td>0.905220</td>\n",
       "      <td>0.897096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.358189</td>\n",
       "      <td>0.909016</td>\n",
       "      <td>0.903663</td>\n",
       "      <td>0.905462</td>\n",
       "      <td>0.902850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.318909</td>\n",
       "      <td>0.910305</td>\n",
       "      <td>0.904781</td>\n",
       "      <td>0.905736</td>\n",
       "      <td>0.904396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.295770</td>\n",
       "      <td>0.916632</td>\n",
       "      <td>0.912353</td>\n",
       "      <td>0.911602</td>\n",
       "      <td>0.911395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.291632</td>\n",
       "      <td>0.916164</td>\n",
       "      <td>0.913147</td>\n",
       "      <td>0.910486</td>\n",
       "      <td>0.911526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.357021</td>\n",
       "      <td>0.901342</td>\n",
       "      <td>0.895533</td>\n",
       "      <td>0.899847</td>\n",
       "      <td>0.894088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.388445</td>\n",
       "      <td>0.906146</td>\n",
       "      <td>0.905306</td>\n",
       "      <td>0.900151</td>\n",
       "      <td>0.901618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.384533</td>\n",
       "      <td>0.905033</td>\n",
       "      <td>0.901393</td>\n",
       "      <td>0.903536</td>\n",
       "      <td>0.899006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.339506</td>\n",
       "      <td>0.911301</td>\n",
       "      <td>0.905160</td>\n",
       "      <td>0.909923</td>\n",
       "      <td>0.904716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.356856</td>\n",
       "      <td>0.908313</td>\n",
       "      <td>0.904227</td>\n",
       "      <td>0.906553</td>\n",
       "      <td>0.902573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.371320</td>\n",
       "      <td>0.901635</td>\n",
       "      <td>0.896523</td>\n",
       "      <td>0.903073</td>\n",
       "      <td>0.894647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.325493</td>\n",
       "      <td>0.914523</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>0.913048</td>\n",
       "      <td>0.908630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.340227</td>\n",
       "      <td>0.906732</td>\n",
       "      <td>0.904109</td>\n",
       "      <td>0.905212</td>\n",
       "      <td>0.901720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.311493</td>\n",
       "      <td>0.916808</td>\n",
       "      <td>0.912266</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>0.911382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.333340</td>\n",
       "      <td>0.915285</td>\n",
       "      <td>0.912670</td>\n",
       "      <td>0.909981</td>\n",
       "      <td>0.910527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.374510</td>\n",
       "      <td>0.915754</td>\n",
       "      <td>0.912614</td>\n",
       "      <td>0.912395</td>\n",
       "      <td>0.910799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.375849</td>\n",
       "      <td>0.918039</td>\n",
       "      <td>0.916123</td>\n",
       "      <td>0.913047</td>\n",
       "      <td>0.913796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.395654</td>\n",
       "      <td>0.919738</td>\n",
       "      <td>0.915687</td>\n",
       "      <td>0.915018</td>\n",
       "      <td>0.914931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.403548</td>\n",
       "      <td>0.915871</td>\n",
       "      <td>0.910741</td>\n",
       "      <td>0.912471</td>\n",
       "      <td>0.910118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.408575</td>\n",
       "      <td>0.919796</td>\n",
       "      <td>0.915527</td>\n",
       "      <td>0.916170</td>\n",
       "      <td>0.914965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.410055</td>\n",
       "      <td>0.914406</td>\n",
       "      <td>0.911120</td>\n",
       "      <td>0.909402</td>\n",
       "      <td>0.909320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.386756</td>\n",
       "      <td>0.919386</td>\n",
       "      <td>0.913771</td>\n",
       "      <td>0.915776</td>\n",
       "      <td>0.914104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.434428</td>\n",
       "      <td>0.914113</td>\n",
       "      <td>0.908274</td>\n",
       "      <td>0.911727</td>\n",
       "      <td>0.908127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.397372</td>\n",
       "      <td>0.914641</td>\n",
       "      <td>0.909770</td>\n",
       "      <td>0.909778</td>\n",
       "      <td>0.909208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.444917</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.908545</td>\n",
       "      <td>0.909721</td>\n",
       "      <td>0.907284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.375930</td>\n",
       "      <td>0.919855</td>\n",
       "      <td>0.914404</td>\n",
       "      <td>0.916061</td>\n",
       "      <td>0.914813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.401344</td>\n",
       "      <td>0.916340</td>\n",
       "      <td>0.912040</td>\n",
       "      <td>0.913293</td>\n",
       "      <td>0.911138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.431774</td>\n",
       "      <td>0.915519</td>\n",
       "      <td>0.910076</td>\n",
       "      <td>0.913091</td>\n",
       "      <td>0.909829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.396754</td>\n",
       "      <td>0.918331</td>\n",
       "      <td>0.914581</td>\n",
       "      <td>0.914657</td>\n",
       "      <td>0.913560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.427919</td>\n",
       "      <td>0.918273</td>\n",
       "      <td>0.913053</td>\n",
       "      <td>0.915712</td>\n",
       "      <td>0.912703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.435092</td>\n",
       "      <td>0.920558</td>\n",
       "      <td>0.916182</td>\n",
       "      <td>0.916856</td>\n",
       "      <td>0.915533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.483273</td>\n",
       "      <td>0.916281</td>\n",
       "      <td>0.912350</td>\n",
       "      <td>0.912541</td>\n",
       "      <td>0.911153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.478485</td>\n",
       "      <td>0.915519</td>\n",
       "      <td>0.910262</td>\n",
       "      <td>0.912140</td>\n",
       "      <td>0.909801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.491787</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.907615</td>\n",
       "      <td>0.910340</td>\n",
       "      <td>0.906779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.477266</td>\n",
       "      <td>0.915812</td>\n",
       "      <td>0.911967</td>\n",
       "      <td>0.911389</td>\n",
       "      <td>0.910522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.490011</td>\n",
       "      <td>0.914582</td>\n",
       "      <td>0.910388</td>\n",
       "      <td>0.911114</td>\n",
       "      <td>0.909085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.471981</td>\n",
       "      <td>0.919562</td>\n",
       "      <td>0.915509</td>\n",
       "      <td>0.915301</td>\n",
       "      <td>0.914742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.468785</td>\n",
       "      <td>0.916047</td>\n",
       "      <td>0.912524</td>\n",
       "      <td>0.911875</td>\n",
       "      <td>0.911109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.475064</td>\n",
       "      <td>0.916750</td>\n",
       "      <td>0.911847</td>\n",
       "      <td>0.913777</td>\n",
       "      <td>0.911348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.454384</td>\n",
       "      <td>0.919562</td>\n",
       "      <td>0.914866</td>\n",
       "      <td>0.915328</td>\n",
       "      <td>0.914777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.468236</td>\n",
       "      <td>0.919269</td>\n",
       "      <td>0.915501</td>\n",
       "      <td>0.914466</td>\n",
       "      <td>0.914737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.492054</td>\n",
       "      <td>0.915168</td>\n",
       "      <td>0.911223</td>\n",
       "      <td>0.910980</td>\n",
       "      <td>0.909973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.461591</td>\n",
       "      <td>0.919562</td>\n",
       "      <td>0.915627</td>\n",
       "      <td>0.915316</td>\n",
       "      <td>0.914743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.489390</td>\n",
       "      <td>0.916691</td>\n",
       "      <td>0.912196</td>\n",
       "      <td>0.913127</td>\n",
       "      <td>0.911304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.504410</td>\n",
       "      <td>0.916867</td>\n",
       "      <td>0.912536</td>\n",
       "      <td>0.913159</td>\n",
       "      <td>0.911551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.513943</td>\n",
       "      <td>0.916691</td>\n",
       "      <td>0.912584</td>\n",
       "      <td>0.912461</td>\n",
       "      <td>0.911547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.492661</td>\n",
       "      <td>0.920499</td>\n",
       "      <td>0.916395</td>\n",
       "      <td>0.915971</td>\n",
       "      <td>0.915792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.500257</td>\n",
       "      <td>0.919562</td>\n",
       "      <td>0.915079</td>\n",
       "      <td>0.915366</td>\n",
       "      <td>0.914593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.514943</td>\n",
       "      <td>0.918273</td>\n",
       "      <td>0.913364</td>\n",
       "      <td>0.914653</td>\n",
       "      <td>0.913050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.543568</td>\n",
       "      <td>0.913293</td>\n",
       "      <td>0.908684</td>\n",
       "      <td>0.910202</td>\n",
       "      <td>0.907617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.522559</td>\n",
       "      <td>0.916984</td>\n",
       "      <td>0.912036</td>\n",
       "      <td>0.913548</td>\n",
       "      <td>0.911491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.514305</td>\n",
       "      <td>0.917804</td>\n",
       "      <td>0.913382</td>\n",
       "      <td>0.913757</td>\n",
       "      <td>0.912596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.533975</td>\n",
       "      <td>0.916047</td>\n",
       "      <td>0.912253</td>\n",
       "      <td>0.911679</td>\n",
       "      <td>0.910944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.514465</td>\n",
       "      <td>0.917980</td>\n",
       "      <td>0.914573</td>\n",
       "      <td>0.913063</td>\n",
       "      <td>0.913316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.520775</td>\n",
       "      <td>0.917453</td>\n",
       "      <td>0.913890</td>\n",
       "      <td>0.912615</td>\n",
       "      <td>0.912620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.527007</td>\n",
       "      <td>0.916691</td>\n",
       "      <td>0.912630</td>\n",
       "      <td>0.912264</td>\n",
       "      <td>0.911555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.520665</td>\n",
       "      <td>0.917511</td>\n",
       "      <td>0.913356</td>\n",
       "      <td>0.913124</td>\n",
       "      <td>0.912392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:22:49,296] Trial 4 finished with value: 3.6563836493369033 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5}. Best is trial 1 with value: 3.6645075218228365.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='14170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/14170 00:35 < 41:50, 5.57 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.756087</td>\n",
       "      <td>0.636827</td>\n",
       "      <td>0.608787</td>\n",
       "      <td>0.457892</td>\n",
       "      <td>0.502235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:23:26,131] Trial 5 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='7085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/7085 00:47 < 27:44, 4.14 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.757413</td>\n",
       "      <td>0.647841</td>\n",
       "      <td>0.616564</td>\n",
       "      <td>0.456821</td>\n",
       "      <td>0.508590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:24:15,289] Trial 6 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1417' max='1417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1417/1417 05:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.633942</td>\n",
       "      <td>0.672037</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.701281</td>\n",
       "      <td>0.530283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.366410</td>\n",
       "      <td>0.861328</td>\n",
       "      <td>0.852011</td>\n",
       "      <td>0.857701</td>\n",
       "      <td>0.848544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.346905</td>\n",
       "      <td>0.874158</td>\n",
       "      <td>0.866454</td>\n",
       "      <td>0.879494</td>\n",
       "      <td>0.863147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.290487</td>\n",
       "      <td>0.896069</td>\n",
       "      <td>0.889695</td>\n",
       "      <td>0.893903</td>\n",
       "      <td>0.888251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.260377</td>\n",
       "      <td>0.909309</td>\n",
       "      <td>0.904372</td>\n",
       "      <td>0.906201</td>\n",
       "      <td>0.903228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.279347</td>\n",
       "      <td>0.900228</td>\n",
       "      <td>0.897425</td>\n",
       "      <td>0.898547</td>\n",
       "      <td>0.894289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.908899</td>\n",
       "      <td>0.903337</td>\n",
       "      <td>0.906714</td>\n",
       "      <td>0.902379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 00:29:56,500] Trial 7 finished with value: 3.621329622441646 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 1}. Best is trial 1 with value: 3.6645075218228365.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1417' max='1417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1417/1417 05:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.633942</td>\n",
       "      <td>0.672037</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.701281</td>\n",
       "      <td>0.530283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.366410</td>\n",
       "      <td>0.861328</td>\n",
       "      <td>0.852011</td>\n",
       "      <td>0.857701</td>\n",
       "      <td>0.848544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.346905</td>\n",
       "      <td>0.874158</td>\n",
       "      <td>0.866454</td>\n",
       "      <td>0.879494</td>\n",
       "      <td>0.863147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.290487</td>\n",
       "      <td>0.896069</td>\n",
       "      <td>0.889695</td>\n",
       "      <td>0.893903</td>\n",
       "      <td>0.888251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.260377</td>\n",
       "      <td>0.909309</td>\n",
       "      <td>0.904372</td>\n",
       "      <td>0.906201</td>\n",
       "      <td>0.903228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.279347</td>\n",
       "      <td>0.900228</td>\n",
       "      <td>0.897425</td>\n",
       "      <td>0.898547</td>\n",
       "      <td>0.894289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.908899</td>\n",
       "      <td>0.903337</td>\n",
       "      <td>0.906714</td>\n",
       "      <td>0.902379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 00:35:37,926] Trial 8 finished with value: 3.621329622441646 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 1}. Best is trial 1 with value: 3.6645075218228365.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='11336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/11336 00:35 < 33:24, 5.56 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.815616</td>\n",
       "      <td>0.615150</td>\n",
       "      <td>0.590411</td>\n",
       "      <td>0.446991</td>\n",
       "      <td>0.486206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:36:14,940] Trial 9 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='8502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/8502 00:35 < 24:50, 5.57 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.756260</td>\n",
       "      <td>0.636886</td>\n",
       "      <td>0.608833</td>\n",
       "      <td>0.457880</td>\n",
       "      <td>0.502267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:36:51,652] Trial 10 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='5668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/5668 00:47 < 21:59, 4.15 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.723697</td>\n",
       "      <td>0.663484</td>\n",
       "      <td>0.629558</td>\n",
       "      <td>0.463476</td>\n",
       "      <td>0.519598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:37:40,611] Trial 11 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='4251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 800/4251 03:12 < 13:52, 4.15 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.709205</td>\n",
       "      <td>0.664948</td>\n",
       "      <td>0.630959</td>\n",
       "      <td>0.465465</td>\n",
       "      <td>0.520997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.397458</td>\n",
       "      <td>0.841350</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.830490</td>\n",
       "      <td>0.831048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.521200</td>\n",
       "      <td>0.345510</td>\n",
       "      <td>0.876501</td>\n",
       "      <td>0.870952</td>\n",
       "      <td>0.878675</td>\n",
       "      <td>0.867261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.521200</td>\n",
       "      <td>0.328682</td>\n",
       "      <td>0.880368</td>\n",
       "      <td>0.877690</td>\n",
       "      <td>0.881969</td>\n",
       "      <td>0.873647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:40:54,350] Trial 12 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='11336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/11336 00:35 < 33:16, 5.58 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.784135</td>\n",
       "      <td>0.627336</td>\n",
       "      <td>0.600736</td>\n",
       "      <td>0.453199</td>\n",
       "      <td>0.495255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:41:31,044] Trial 13 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='11336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/11336 00:35 < 33:15, 5.58 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.784135</td>\n",
       "      <td>0.627336</td>\n",
       "      <td>0.600736</td>\n",
       "      <td>0.453199</td>\n",
       "      <td>0.495255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:42:07,877] Trial 14 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='4251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 800/4251 03:12 < 13:52, 4.14 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.655432</td>\n",
       "      <td>0.668873</td>\n",
       "      <td>0.635156</td>\n",
       "      <td>0.805765</td>\n",
       "      <td>0.526179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.362243</td>\n",
       "      <td>0.859336</td>\n",
       "      <td>0.851693</td>\n",
       "      <td>0.855282</td>\n",
       "      <td>0.847287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.317437</td>\n",
       "      <td>0.884235</td>\n",
       "      <td>0.877474</td>\n",
       "      <td>0.886348</td>\n",
       "      <td>0.874951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.309116</td>\n",
       "      <td>0.888570</td>\n",
       "      <td>0.884793</td>\n",
       "      <td>0.886818</td>\n",
       "      <td>0.881791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 00:45:21,819] Trial 15 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='5668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/5668 00:35 < 16:21, 5.57 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.720584</td>\n",
       "      <td>0.649364</td>\n",
       "      <td>0.619179</td>\n",
       "      <td>0.463095</td>\n",
       "      <td>0.511156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:45:58,724] Trial 16 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='5668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/5668 00:47 < 22:01, 4.14 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.723697</td>\n",
       "      <td>0.663484</td>\n",
       "      <td>0.629558</td>\n",
       "      <td>0.463476</td>\n",
       "      <td>0.519598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:46:47,793] Trial 17 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='2834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/2834 01:36 < 09:47, 4.14 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.709205</td>\n",
       "      <td>0.664948</td>\n",
       "      <td>0.630959</td>\n",
       "      <td>0.465465</td>\n",
       "      <td>0.520997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.407057</td>\n",
       "      <td>0.833909</td>\n",
       "      <td>0.823710</td>\n",
       "      <td>0.831642</td>\n",
       "      <td>0.817921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:48:25,015] Trial 18 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='14170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/14170 00:35 < 41:46, 5.57 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.756087</td>\n",
       "      <td>0.636827</td>\n",
       "      <td>0.608787</td>\n",
       "      <td>0.457892</td>\n",
       "      <td>0.502235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:49:01,721] Trial 19 pruned. \n"
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    n_trials=20, \n",
    "    hp_space=optuna_hp_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a73ebc-2b56-45fd-93e8-c7f8caf940c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='1', objective=3.6645075218228365, hyperparameters={'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}, run_summary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94567a32-b8bc-47b6-a670-1235f2784399",
   "metadata": {},
   "source": [
    "## Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b313c5e4-bd9f-41d2-a073-223e41db8f9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57076' max='57076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57076/57076 2:57:00, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.001673</td>\n",
       "      <td>0.509411</td>\n",
       "      <td>0.504776</td>\n",
       "      <td>0.417991</td>\n",
       "      <td>0.405170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.867453</td>\n",
       "      <td>0.588125</td>\n",
       "      <td>0.564187</td>\n",
       "      <td>0.433682</td>\n",
       "      <td>0.462664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.819500</td>\n",
       "      <td>0.791864</td>\n",
       "      <td>0.623317</td>\n",
       "      <td>0.592895</td>\n",
       "      <td>0.447455</td>\n",
       "      <td>0.487745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.819500</td>\n",
       "      <td>0.744661</td>\n",
       "      <td>0.649270</td>\n",
       "      <td>0.611227</td>\n",
       "      <td>0.450570</td>\n",
       "      <td>0.503980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.668647</td>\n",
       "      <td>0.668264</td>\n",
       "      <td>0.626258</td>\n",
       "      <td>0.459496</td>\n",
       "      <td>0.517273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.607271</td>\n",
       "      <td>0.680185</td>\n",
       "      <td>0.637090</td>\n",
       "      <td>0.787927</td>\n",
       "      <td>0.536506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.480579</td>\n",
       "      <td>0.801164</td>\n",
       "      <td>0.778971</td>\n",
       "      <td>0.796453</td>\n",
       "      <td>0.768701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.452885</td>\n",
       "      <td>0.820557</td>\n",
       "      <td>0.805028</td>\n",
       "      <td>0.814087</td>\n",
       "      <td>0.798281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.442032</td>\n",
       "      <td>0.830253</td>\n",
       "      <td>0.816483</td>\n",
       "      <td>0.826452</td>\n",
       "      <td>0.810767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.352652</td>\n",
       "      <td>0.868640</td>\n",
       "      <td>0.857138</td>\n",
       "      <td>0.864303</td>\n",
       "      <td>0.855617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.372543</td>\n",
       "      <td>0.865104</td>\n",
       "      <td>0.855680</td>\n",
       "      <td>0.863640</td>\n",
       "      <td>0.852577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.319390</td>\n",
       "      <td>0.885581</td>\n",
       "      <td>0.878893</td>\n",
       "      <td>0.877764</td>\n",
       "      <td>0.877454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>0.301946</td>\n",
       "      <td>0.891912</td>\n",
       "      <td>0.885891</td>\n",
       "      <td>0.883494</td>\n",
       "      <td>0.884529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>0.329751</td>\n",
       "      <td>0.891570</td>\n",
       "      <td>0.880846</td>\n",
       "      <td>0.889963</td>\n",
       "      <td>0.881859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.427363</td>\n",
       "      <td>0.864705</td>\n",
       "      <td>0.850517</td>\n",
       "      <td>0.873701</td>\n",
       "      <td>0.848894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.292449</td>\n",
       "      <td>0.902806</td>\n",
       "      <td>0.894479</td>\n",
       "      <td>0.897758</td>\n",
       "      <td>0.895404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.307475</td>\n",
       "      <td>0.900125</td>\n",
       "      <td>0.892360</td>\n",
       "      <td>0.895110</td>\n",
       "      <td>0.892583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.227400</td>\n",
       "      <td>0.324982</td>\n",
       "      <td>0.891684</td>\n",
       "      <td>0.876004</td>\n",
       "      <td>0.896077</td>\n",
       "      <td>0.880741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.227400</td>\n",
       "      <td>0.295606</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.898301</td>\n",
       "      <td>0.902167</td>\n",
       "      <td>0.899434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.324797</td>\n",
       "      <td>0.899954</td>\n",
       "      <td>0.888568</td>\n",
       "      <td>0.901315</td>\n",
       "      <td>0.890678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.301434</td>\n",
       "      <td>0.904289</td>\n",
       "      <td>0.896480</td>\n",
       "      <td>0.901563</td>\n",
       "      <td>0.896834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.316549</td>\n",
       "      <td>0.905031</td>\n",
       "      <td>0.900112</td>\n",
       "      <td>0.898229</td>\n",
       "      <td>0.899121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.202300</td>\n",
       "      <td>0.369036</td>\n",
       "      <td>0.894251</td>\n",
       "      <td>0.886730</td>\n",
       "      <td>0.892772</td>\n",
       "      <td>0.885612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.202300</td>\n",
       "      <td>0.320234</td>\n",
       "      <td>0.900582</td>\n",
       "      <td>0.894226</td>\n",
       "      <td>0.896568</td>\n",
       "      <td>0.893733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.308897</td>\n",
       "      <td>0.912161</td>\n",
       "      <td>0.904059</td>\n",
       "      <td>0.908072</td>\n",
       "      <td>0.905648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.337213</td>\n",
       "      <td>0.890999</td>\n",
       "      <td>0.883177</td>\n",
       "      <td>0.891037</td>\n",
       "      <td>0.882294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.323292</td>\n",
       "      <td>0.899555</td>\n",
       "      <td>0.886374</td>\n",
       "      <td>0.902077</td>\n",
       "      <td>0.889996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.216900</td>\n",
       "      <td>0.302086</td>\n",
       "      <td>0.905430</td>\n",
       "      <td>0.895300</td>\n",
       "      <td>0.905195</td>\n",
       "      <td>0.897439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.216900</td>\n",
       "      <td>0.297550</td>\n",
       "      <td>0.904232</td>\n",
       "      <td>0.894903</td>\n",
       "      <td>0.901904</td>\n",
       "      <td>0.895859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.300598</td>\n",
       "      <td>0.905943</td>\n",
       "      <td>0.900497</td>\n",
       "      <td>0.902188</td>\n",
       "      <td>0.899304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.292896</td>\n",
       "      <td>0.910392</td>\n",
       "      <td>0.902300</td>\n",
       "      <td>0.907596</td>\n",
       "      <td>0.903262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.285330</td>\n",
       "      <td>0.917579</td>\n",
       "      <td>0.910302</td>\n",
       "      <td>0.913359</td>\n",
       "      <td>0.911474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.281643</td>\n",
       "      <td>0.906571</td>\n",
       "      <td>0.902258</td>\n",
       "      <td>0.900922</td>\n",
       "      <td>0.900257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.284534</td>\n",
       "      <td>0.913587</td>\n",
       "      <td>0.901652</td>\n",
       "      <td>0.913535</td>\n",
       "      <td>0.905815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.284644</td>\n",
       "      <td>0.909309</td>\n",
       "      <td>0.903912</td>\n",
       "      <td>0.903817</td>\n",
       "      <td>0.902918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.294137</td>\n",
       "      <td>0.913073</td>\n",
       "      <td>0.902211</td>\n",
       "      <td>0.911951</td>\n",
       "      <td>0.905819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.319214</td>\n",
       "      <td>0.908168</td>\n",
       "      <td>0.896413</td>\n",
       "      <td>0.909265</td>\n",
       "      <td>0.899594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.197800</td>\n",
       "      <td>0.281348</td>\n",
       "      <td>0.917408</td>\n",
       "      <td>0.909763</td>\n",
       "      <td>0.914126</td>\n",
       "      <td>0.911163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.197800</td>\n",
       "      <td>0.324772</td>\n",
       "      <td>0.898985</td>\n",
       "      <td>0.893786</td>\n",
       "      <td>0.894851</td>\n",
       "      <td>0.891760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>0.329492</td>\n",
       "      <td>0.907369</td>\n",
       "      <td>0.900542</td>\n",
       "      <td>0.905362</td>\n",
       "      <td>0.900093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>0.270548</td>\n",
       "      <td>0.916096</td>\n",
       "      <td>0.909817</td>\n",
       "      <td>0.911879</td>\n",
       "      <td>0.910449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>0.245104</td>\n",
       "      <td>0.919405</td>\n",
       "      <td>0.911056</td>\n",
       "      <td>0.916829</td>\n",
       "      <td>0.913145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.190500</td>\n",
       "      <td>0.283340</td>\n",
       "      <td>0.918036</td>\n",
       "      <td>0.910158</td>\n",
       "      <td>0.914466</td>\n",
       "      <td>0.911551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.190500</td>\n",
       "      <td>0.258767</td>\n",
       "      <td>0.916324</td>\n",
       "      <td>0.909170</td>\n",
       "      <td>0.913454</td>\n",
       "      <td>0.910010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.189400</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>0.919462</td>\n",
       "      <td>0.913509</td>\n",
       "      <td>0.914898</td>\n",
       "      <td>0.914036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.189400</td>\n",
       "      <td>0.284646</td>\n",
       "      <td>0.914043</td>\n",
       "      <td>0.910865</td>\n",
       "      <td>0.909420</td>\n",
       "      <td>0.908824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.189400</td>\n",
       "      <td>0.254859</td>\n",
       "      <td>0.920830</td>\n",
       "      <td>0.916553</td>\n",
       "      <td>0.915747</td>\n",
       "      <td>0.915798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.278304</td>\n",
       "      <td>0.915298</td>\n",
       "      <td>0.907725</td>\n",
       "      <td>0.912102</td>\n",
       "      <td>0.908607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.300711</td>\n",
       "      <td>0.910278</td>\n",
       "      <td>0.901504</td>\n",
       "      <td>0.909604</td>\n",
       "      <td>0.902774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.328956</td>\n",
       "      <td>0.901608</td>\n",
       "      <td>0.895626</td>\n",
       "      <td>0.902356</td>\n",
       "      <td>0.894169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.298155</td>\n",
       "      <td>0.904917</td>\n",
       "      <td>0.902123</td>\n",
       "      <td>0.903580</td>\n",
       "      <td>0.899463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.276427</td>\n",
       "      <td>0.910564</td>\n",
       "      <td>0.906417</td>\n",
       "      <td>0.904120</td>\n",
       "      <td>0.905025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.274905</td>\n",
       "      <td>0.920488</td>\n",
       "      <td>0.911653</td>\n",
       "      <td>0.918563</td>\n",
       "      <td>0.914239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.246940</td>\n",
       "      <td>0.923340</td>\n",
       "      <td>0.915808</td>\n",
       "      <td>0.920931</td>\n",
       "      <td>0.917548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.180500</td>\n",
       "      <td>0.304612</td>\n",
       "      <td>0.910735</td>\n",
       "      <td>0.907223</td>\n",
       "      <td>0.906808</td>\n",
       "      <td>0.905124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.180500</td>\n",
       "      <td>0.296051</td>\n",
       "      <td>0.913073</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.908529</td>\n",
       "      <td>0.908142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.180500</td>\n",
       "      <td>0.291736</td>\n",
       "      <td>0.912617</td>\n",
       "      <td>0.908081</td>\n",
       "      <td>0.906701</td>\n",
       "      <td>0.907302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.176700</td>\n",
       "      <td>0.331124</td>\n",
       "      <td>0.899954</td>\n",
       "      <td>0.893518</td>\n",
       "      <td>0.898209</td>\n",
       "      <td>0.891743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.176700</td>\n",
       "      <td>0.281370</td>\n",
       "      <td>0.915469</td>\n",
       "      <td>0.907010</td>\n",
       "      <td>0.912658</td>\n",
       "      <td>0.908886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.274722</td>\n",
       "      <td>0.916496</td>\n",
       "      <td>0.909960</td>\n",
       "      <td>0.913007</td>\n",
       "      <td>0.910323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.256490</td>\n",
       "      <td>0.922428</td>\n",
       "      <td>0.912064</td>\n",
       "      <td>0.922527</td>\n",
       "      <td>0.916253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.273799</td>\n",
       "      <td>0.922142</td>\n",
       "      <td>0.914440</td>\n",
       "      <td>0.919323</td>\n",
       "      <td>0.916583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.175400</td>\n",
       "      <td>0.265689</td>\n",
       "      <td>0.919747</td>\n",
       "      <td>0.909225</td>\n",
       "      <td>0.920448</td>\n",
       "      <td>0.913529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.175400</td>\n",
       "      <td>0.255925</td>\n",
       "      <td>0.920488</td>\n",
       "      <td>0.915664</td>\n",
       "      <td>0.915751</td>\n",
       "      <td>0.915260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>0.261783</td>\n",
       "      <td>0.923854</td>\n",
       "      <td>0.915404</td>\n",
       "      <td>0.922369</td>\n",
       "      <td>0.917872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>0.275324</td>\n",
       "      <td>0.916895</td>\n",
       "      <td>0.911458</td>\n",
       "      <td>0.911944</td>\n",
       "      <td>0.911585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>0.262539</td>\n",
       "      <td>0.920488</td>\n",
       "      <td>0.914265</td>\n",
       "      <td>0.916668</td>\n",
       "      <td>0.914809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.171500</td>\n",
       "      <td>0.302943</td>\n",
       "      <td>0.908567</td>\n",
       "      <td>0.900593</td>\n",
       "      <td>0.906578</td>\n",
       "      <td>0.901020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.171500</td>\n",
       "      <td>0.277204</td>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.914511</td>\n",
       "      <td>0.917945</td>\n",
       "      <td>0.915292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>0.259064</td>\n",
       "      <td>0.924538</td>\n",
       "      <td>0.918964</td>\n",
       "      <td>0.920054</td>\n",
       "      <td>0.919362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>0.263029</td>\n",
       "      <td>0.917465</td>\n",
       "      <td>0.906542</td>\n",
       "      <td>0.918062</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>0.300663</td>\n",
       "      <td>0.922770</td>\n",
       "      <td>0.914424</td>\n",
       "      <td>0.921240</td>\n",
       "      <td>0.916752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.275910</td>\n",
       "      <td>0.924880</td>\n",
       "      <td>0.916602</td>\n",
       "      <td>0.922926</td>\n",
       "      <td>0.919076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.296958</td>\n",
       "      <td>0.922371</td>\n",
       "      <td>0.916296</td>\n",
       "      <td>0.919634</td>\n",
       "      <td>0.916928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.317684</td>\n",
       "      <td>0.923968</td>\n",
       "      <td>0.916718</td>\n",
       "      <td>0.921233</td>\n",
       "      <td>0.918272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.400240</td>\n",
       "      <td>0.898642</td>\n",
       "      <td>0.897842</td>\n",
       "      <td>0.895195</td>\n",
       "      <td>0.893137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.325629</td>\n",
       "      <td>0.919290</td>\n",
       "      <td>0.913878</td>\n",
       "      <td>0.915668</td>\n",
       "      <td>0.913567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.129400</td>\n",
       "      <td>0.279006</td>\n",
       "      <td>0.923682</td>\n",
       "      <td>0.917513</td>\n",
       "      <td>0.919788</td>\n",
       "      <td>0.918156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.129400</td>\n",
       "      <td>0.278082</td>\n",
       "      <td>0.923796</td>\n",
       "      <td>0.916792</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.918113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.334841</td>\n",
       "      <td>0.916610</td>\n",
       "      <td>0.911615</td>\n",
       "      <td>0.912433</td>\n",
       "      <td>0.910675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.294297</td>\n",
       "      <td>0.918093</td>\n",
       "      <td>0.913930</td>\n",
       "      <td>0.913280</td>\n",
       "      <td>0.912932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.290686</td>\n",
       "      <td>0.926135</td>\n",
       "      <td>0.918823</td>\n",
       "      <td>0.923455</td>\n",
       "      <td>0.920763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.305799</td>\n",
       "      <td>0.922941</td>\n",
       "      <td>0.916644</td>\n",
       "      <td>0.919796</td>\n",
       "      <td>0.917246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.277550</td>\n",
       "      <td>0.925165</td>\n",
       "      <td>0.917668</td>\n",
       "      <td>0.922511</td>\n",
       "      <td>0.919722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.281337</td>\n",
       "      <td>0.923796</td>\n",
       "      <td>0.916695</td>\n",
       "      <td>0.920609</td>\n",
       "      <td>0.918185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.334894</td>\n",
       "      <td>0.918777</td>\n",
       "      <td>0.912236</td>\n",
       "      <td>0.915258</td>\n",
       "      <td>0.912445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.326509</td>\n",
       "      <td>0.918663</td>\n",
       "      <td>0.911648</td>\n",
       "      <td>0.916360</td>\n",
       "      <td>0.912129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.302568</td>\n",
       "      <td>0.925850</td>\n",
       "      <td>0.921145</td>\n",
       "      <td>0.920955</td>\n",
       "      <td>0.920929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.305214</td>\n",
       "      <td>0.922142</td>\n",
       "      <td>0.915910</td>\n",
       "      <td>0.918462</td>\n",
       "      <td>0.916566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.301128</td>\n",
       "      <td>0.920545</td>\n",
       "      <td>0.912423</td>\n",
       "      <td>0.918951</td>\n",
       "      <td>0.914295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.303907</td>\n",
       "      <td>0.919633</td>\n",
       "      <td>0.913513</td>\n",
       "      <td>0.916710</td>\n",
       "      <td>0.913833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.282354</td>\n",
       "      <td>0.921173</td>\n",
       "      <td>0.914413</td>\n",
       "      <td>0.917762</td>\n",
       "      <td>0.915592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.309350</td>\n",
       "      <td>0.922142</td>\n",
       "      <td>0.916176</td>\n",
       "      <td>0.918486</td>\n",
       "      <td>0.916496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.313244</td>\n",
       "      <td>0.914841</td>\n",
       "      <td>0.913245</td>\n",
       "      <td>0.909598</td>\n",
       "      <td>0.910258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.316968</td>\n",
       "      <td>0.917579</td>\n",
       "      <td>0.915245</td>\n",
       "      <td>0.911369</td>\n",
       "      <td>0.912916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.311034</td>\n",
       "      <td>0.920945</td>\n",
       "      <td>0.916411</td>\n",
       "      <td>0.915965</td>\n",
       "      <td>0.915800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.328747</td>\n",
       "      <td>0.916610</td>\n",
       "      <td>0.910747</td>\n",
       "      <td>0.913878</td>\n",
       "      <td>0.910536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.125600</td>\n",
       "      <td>0.288719</td>\n",
       "      <td>0.922713</td>\n",
       "      <td>0.916507</td>\n",
       "      <td>0.918505</td>\n",
       "      <td>0.917108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.125600</td>\n",
       "      <td>0.265917</td>\n",
       "      <td>0.925222</td>\n",
       "      <td>0.917888</td>\n",
       "      <td>0.922126</td>\n",
       "      <td>0.919647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.303935</td>\n",
       "      <td>0.920716</td>\n",
       "      <td>0.916512</td>\n",
       "      <td>0.915513</td>\n",
       "      <td>0.915550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.319759</td>\n",
       "      <td>0.917408</td>\n",
       "      <td>0.910359</td>\n",
       "      <td>0.914113</td>\n",
       "      <td>0.910938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.294228</td>\n",
       "      <td>0.919405</td>\n",
       "      <td>0.913518</td>\n",
       "      <td>0.915581</td>\n",
       "      <td>0.913576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.305853</td>\n",
       "      <td>0.923568</td>\n",
       "      <td>0.917583</td>\n",
       "      <td>0.919125</td>\n",
       "      <td>0.918158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.324822</td>\n",
       "      <td>0.919519</td>\n",
       "      <td>0.914521</td>\n",
       "      <td>0.916112</td>\n",
       "      <td>0.913982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>0.294615</td>\n",
       "      <td>0.921800</td>\n",
       "      <td>0.918424</td>\n",
       "      <td>0.916331</td>\n",
       "      <td>0.917048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>0.308952</td>\n",
       "      <td>0.925736</td>\n",
       "      <td>0.916427</td>\n",
       "      <td>0.924838</td>\n",
       "      <td>0.919905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>0.310512</td>\n",
       "      <td>0.919690</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.916560</td>\n",
       "      <td>0.913857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.290692</td>\n",
       "      <td>0.924367</td>\n",
       "      <td>0.916873</td>\n",
       "      <td>0.921859</td>\n",
       "      <td>0.918787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.921173</td>\n",
       "      <td>0.916301</td>\n",
       "      <td>0.917166</td>\n",
       "      <td>0.915808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.285459</td>\n",
       "      <td>0.923739</td>\n",
       "      <td>0.918396</td>\n",
       "      <td>0.919221</td>\n",
       "      <td>0.918645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.320801</td>\n",
       "      <td>0.919918</td>\n",
       "      <td>0.913056</td>\n",
       "      <td>0.917078</td>\n",
       "      <td>0.913856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.289714</td>\n",
       "      <td>0.928074</td>\n",
       "      <td>0.920194</td>\n",
       "      <td>0.926195</td>\n",
       "      <td>0.922530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.301678</td>\n",
       "      <td>0.924823</td>\n",
       "      <td>0.917108</td>\n",
       "      <td>0.922454</td>\n",
       "      <td>0.918840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.327976</td>\n",
       "      <td>0.920659</td>\n",
       "      <td>0.914975</td>\n",
       "      <td>0.918527</td>\n",
       "      <td>0.915124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.267138</td>\n",
       "      <td>0.923739</td>\n",
       "      <td>0.919039</td>\n",
       "      <td>0.920320</td>\n",
       "      <td>0.918720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.325181</td>\n",
       "      <td>0.920888</td>\n",
       "      <td>0.916109</td>\n",
       "      <td>0.917030</td>\n",
       "      <td>0.915667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.302417</td>\n",
       "      <td>0.925736</td>\n",
       "      <td>0.919481</td>\n",
       "      <td>0.922874</td>\n",
       "      <td>0.920514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.273982</td>\n",
       "      <td>0.927048</td>\n",
       "      <td>0.919008</td>\n",
       "      <td>0.925409</td>\n",
       "      <td>0.921507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.273367</td>\n",
       "      <td>0.927219</td>\n",
       "      <td>0.918485</td>\n",
       "      <td>0.926105</td>\n",
       "      <td>0.921774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.302389</td>\n",
       "      <td>0.923169</td>\n",
       "      <td>0.916485</td>\n",
       "      <td>0.919869</td>\n",
       "      <td>0.917511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.294895</td>\n",
       "      <td>0.924880</td>\n",
       "      <td>0.917833</td>\n",
       "      <td>0.922288</td>\n",
       "      <td>0.919288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.315215</td>\n",
       "      <td>0.920203</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.917775</td>\n",
       "      <td>0.914776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.300102</td>\n",
       "      <td>0.925451</td>\n",
       "      <td>0.919012</td>\n",
       "      <td>0.922404</td>\n",
       "      <td>0.919991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.314142</td>\n",
       "      <td>0.925222</td>\n",
       "      <td>0.919359</td>\n",
       "      <td>0.921389</td>\n",
       "      <td>0.920133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.324434</td>\n",
       "      <td>0.924196</td>\n",
       "      <td>0.918692</td>\n",
       "      <td>0.919780</td>\n",
       "      <td>0.919174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.304612</td>\n",
       "      <td>0.923511</td>\n",
       "      <td>0.917868</td>\n",
       "      <td>0.919993</td>\n",
       "      <td>0.918158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.295515</td>\n",
       "      <td>0.926249</td>\n",
       "      <td>0.918765</td>\n",
       "      <td>0.924370</td>\n",
       "      <td>0.920727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.320678</td>\n",
       "      <td>0.922371</td>\n",
       "      <td>0.916840</td>\n",
       "      <td>0.918280</td>\n",
       "      <td>0.916739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.322228</td>\n",
       "      <td>0.922827</td>\n",
       "      <td>0.916478</td>\n",
       "      <td>0.919428</td>\n",
       "      <td>0.917163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>0.310466</td>\n",
       "      <td>0.919176</td>\n",
       "      <td>0.915114</td>\n",
       "      <td>0.915075</td>\n",
       "      <td>0.913986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>0.310894</td>\n",
       "      <td>0.924367</td>\n",
       "      <td>0.917017</td>\n",
       "      <td>0.921942</td>\n",
       "      <td>0.918713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>0.306804</td>\n",
       "      <td>0.923112</td>\n",
       "      <td>0.914815</td>\n",
       "      <td>0.921797</td>\n",
       "      <td>0.917181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.273360</td>\n",
       "      <td>0.922428</td>\n",
       "      <td>0.914344</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>0.916444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.295098</td>\n",
       "      <td>0.922713</td>\n",
       "      <td>0.914036</td>\n",
       "      <td>0.921261</td>\n",
       "      <td>0.916391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.295889</td>\n",
       "      <td>0.922256</td>\n",
       "      <td>0.913222</td>\n",
       "      <td>0.921293</td>\n",
       "      <td>0.915919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.320861</td>\n",
       "      <td>0.919975</td>\n",
       "      <td>0.913945</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>0.913855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.312691</td>\n",
       "      <td>0.927447</td>\n",
       "      <td>0.919622</td>\n",
       "      <td>0.925572</td>\n",
       "      <td>0.921843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.292599</td>\n",
       "      <td>0.925736</td>\n",
       "      <td>0.920169</td>\n",
       "      <td>0.921654</td>\n",
       "      <td>0.920607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.304359</td>\n",
       "      <td>0.924766</td>\n",
       "      <td>0.916387</td>\n",
       "      <td>0.923624</td>\n",
       "      <td>0.918706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.279520</td>\n",
       "      <td>0.922941</td>\n",
       "      <td>0.917184</td>\n",
       "      <td>0.919776</td>\n",
       "      <td>0.917295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.305107</td>\n",
       "      <td>0.921857</td>\n",
       "      <td>0.914687</td>\n",
       "      <td>0.919775</td>\n",
       "      <td>0.915902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.324554</td>\n",
       "      <td>0.922199</td>\n",
       "      <td>0.915057</td>\n",
       "      <td>0.920151</td>\n",
       "      <td>0.916443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>0.298323</td>\n",
       "      <td>0.926477</td>\n",
       "      <td>0.917679</td>\n",
       "      <td>0.925867</td>\n",
       "      <td>0.920803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>0.353958</td>\n",
       "      <td>0.921173</td>\n",
       "      <td>0.916887</td>\n",
       "      <td>0.916592</td>\n",
       "      <td>0.915941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.334194</td>\n",
       "      <td>0.923454</td>\n",
       "      <td>0.919269</td>\n",
       "      <td>0.918290</td>\n",
       "      <td>0.918579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.326104</td>\n",
       "      <td>0.922713</td>\n",
       "      <td>0.916554</td>\n",
       "      <td>0.918826</td>\n",
       "      <td>0.917137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.360215</td>\n",
       "      <td>0.924253</td>\n",
       "      <td>0.916638</td>\n",
       "      <td>0.922350</td>\n",
       "      <td>0.918453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.347257</td>\n",
       "      <td>0.925222</td>\n",
       "      <td>0.920101</td>\n",
       "      <td>0.920848</td>\n",
       "      <td>0.920203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.350949</td>\n",
       "      <td>0.925508</td>\n",
       "      <td>0.920104</td>\n",
       "      <td>0.921260</td>\n",
       "      <td>0.920542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.359633</td>\n",
       "      <td>0.921686</td>\n",
       "      <td>0.915418</td>\n",
       "      <td>0.918754</td>\n",
       "      <td>0.915836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.375826</td>\n",
       "      <td>0.920488</td>\n",
       "      <td>0.916331</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.915414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.395374</td>\n",
       "      <td>0.921572</td>\n",
       "      <td>0.916808</td>\n",
       "      <td>0.917633</td>\n",
       "      <td>0.916261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.364573</td>\n",
       "      <td>0.923226</td>\n",
       "      <td>0.918542</td>\n",
       "      <td>0.919732</td>\n",
       "      <td>0.918180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.358791</td>\n",
       "      <td>0.922485</td>\n",
       "      <td>0.918838</td>\n",
       "      <td>0.918243</td>\n",
       "      <td>0.917651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.359538</td>\n",
       "      <td>0.926249</td>\n",
       "      <td>0.920003</td>\n",
       "      <td>0.923182</td>\n",
       "      <td>0.920992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.368092</td>\n",
       "      <td>0.927618</td>\n",
       "      <td>0.920370</td>\n",
       "      <td>0.925669</td>\n",
       "      <td>0.922442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.356572</td>\n",
       "      <td>0.923854</td>\n",
       "      <td>0.917571</td>\n",
       "      <td>0.920996</td>\n",
       "      <td>0.918429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.359967</td>\n",
       "      <td>0.924253</td>\n",
       "      <td>0.916907</td>\n",
       "      <td>0.921939</td>\n",
       "      <td>0.918315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.350436</td>\n",
       "      <td>0.928987</td>\n",
       "      <td>0.921567</td>\n",
       "      <td>0.926766</td>\n",
       "      <td>0.923756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.380011</td>\n",
       "      <td>0.922313</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.917797</td>\n",
       "      <td>0.916935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.376220</td>\n",
       "      <td>0.920602</td>\n",
       "      <td>0.916433</td>\n",
       "      <td>0.915429</td>\n",
       "      <td>0.915544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.317648</td>\n",
       "      <td>0.926306</td>\n",
       "      <td>0.920555</td>\n",
       "      <td>0.922172</td>\n",
       "      <td>0.921107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.387340</td>\n",
       "      <td>0.924424</td>\n",
       "      <td>0.919129</td>\n",
       "      <td>0.920453</td>\n",
       "      <td>0.919203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.347756</td>\n",
       "      <td>0.926591</td>\n",
       "      <td>0.920291</td>\n",
       "      <td>0.923263</td>\n",
       "      <td>0.921413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.078200</td>\n",
       "      <td>0.391792</td>\n",
       "      <td>0.923169</td>\n",
       "      <td>0.919095</td>\n",
       "      <td>0.918159</td>\n",
       "      <td>0.918292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>0.078200</td>\n",
       "      <td>0.380298</td>\n",
       "      <td>0.922599</td>\n",
       "      <td>0.916450</td>\n",
       "      <td>0.919423</td>\n",
       "      <td>0.916971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>0.078200</td>\n",
       "      <td>0.356650</td>\n",
       "      <td>0.925451</td>\n",
       "      <td>0.919537</td>\n",
       "      <td>0.922101</td>\n",
       "      <td>0.920329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.364924</td>\n",
       "      <td>0.924538</td>\n",
       "      <td>0.919003</td>\n",
       "      <td>0.920836</td>\n",
       "      <td>0.919329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.335981</td>\n",
       "      <td>0.926820</td>\n",
       "      <td>0.921478</td>\n",
       "      <td>0.922895</td>\n",
       "      <td>0.921933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.379693</td>\n",
       "      <td>0.921914</td>\n",
       "      <td>0.916564</td>\n",
       "      <td>0.918378</td>\n",
       "      <td>0.916598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.382575</td>\n",
       "      <td>0.926021</td>\n",
       "      <td>0.919619</td>\n",
       "      <td>0.922887</td>\n",
       "      <td>0.920677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.345676</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.920932</td>\n",
       "      <td>0.923223</td>\n",
       "      <td>0.921712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.351151</td>\n",
       "      <td>0.926648</td>\n",
       "      <td>0.920075</td>\n",
       "      <td>0.923978</td>\n",
       "      <td>0.921286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.403783</td>\n",
       "      <td>0.924025</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.919737</td>\n",
       "      <td>0.918998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.349626</td>\n",
       "      <td>0.926363</td>\n",
       "      <td>0.919527</td>\n",
       "      <td>0.923600</td>\n",
       "      <td>0.920863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.395658</td>\n",
       "      <td>0.923169</td>\n",
       "      <td>0.917648</td>\n",
       "      <td>0.919514</td>\n",
       "      <td>0.917680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.341849</td>\n",
       "      <td>0.927219</td>\n",
       "      <td>0.920877</td>\n",
       "      <td>0.924188</td>\n",
       "      <td>0.922080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.369435</td>\n",
       "      <td>0.926591</td>\n",
       "      <td>0.918903</td>\n",
       "      <td>0.924625</td>\n",
       "      <td>0.920968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.373655</td>\n",
       "      <td>0.924652</td>\n",
       "      <td>0.917020</td>\n",
       "      <td>0.923006</td>\n",
       "      <td>0.918956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.074200</td>\n",
       "      <td>0.408741</td>\n",
       "      <td>0.920830</td>\n",
       "      <td>0.916432</td>\n",
       "      <td>0.916387</td>\n",
       "      <td>0.915656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>0.074200</td>\n",
       "      <td>0.379188</td>\n",
       "      <td>0.924082</td>\n",
       "      <td>0.918928</td>\n",
       "      <td>0.919695</td>\n",
       "      <td>0.918916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>0.074200</td>\n",
       "      <td>0.363256</td>\n",
       "      <td>0.923340</td>\n",
       "      <td>0.918431</td>\n",
       "      <td>0.919043</td>\n",
       "      <td>0.918119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>0.083600</td>\n",
       "      <td>0.380387</td>\n",
       "      <td>0.924823</td>\n",
       "      <td>0.917388</td>\n",
       "      <td>0.922871</td>\n",
       "      <td>0.919070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>0.083600</td>\n",
       "      <td>0.354345</td>\n",
       "      <td>0.922656</td>\n",
       "      <td>0.917792</td>\n",
       "      <td>0.918323</td>\n",
       "      <td>0.917421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.369328</td>\n",
       "      <td>0.924310</td>\n",
       "      <td>0.918932</td>\n",
       "      <td>0.920602</td>\n",
       "      <td>0.919207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.381142</td>\n",
       "      <td>0.923454</td>\n",
       "      <td>0.919312</td>\n",
       "      <td>0.919036</td>\n",
       "      <td>0.918541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.361603</td>\n",
       "      <td>0.927960</td>\n",
       "      <td>0.920665</td>\n",
       "      <td>0.925982</td>\n",
       "      <td>0.922634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.353028</td>\n",
       "      <td>0.925850</td>\n",
       "      <td>0.920325</td>\n",
       "      <td>0.922371</td>\n",
       "      <td>0.920761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.338723</td>\n",
       "      <td>0.924880</td>\n",
       "      <td>0.919959</td>\n",
       "      <td>0.920586</td>\n",
       "      <td>0.920086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.351673</td>\n",
       "      <td>0.925108</td>\n",
       "      <td>0.919841</td>\n",
       "      <td>0.921475</td>\n",
       "      <td>0.920135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.371394</td>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.917458</td>\n",
       "      <td>0.915879</td>\n",
       "      <td>0.916139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.365673</td>\n",
       "      <td>0.926648</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.922795</td>\n",
       "      <td>0.921644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.360892</td>\n",
       "      <td>0.926877</td>\n",
       "      <td>0.920106</td>\n",
       "      <td>0.924018</td>\n",
       "      <td>0.921646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.353884</td>\n",
       "      <td>0.925736</td>\n",
       "      <td>0.920202</td>\n",
       "      <td>0.921502</td>\n",
       "      <td>0.920643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.382221</td>\n",
       "      <td>0.922256</td>\n",
       "      <td>0.916141</td>\n",
       "      <td>0.918709</td>\n",
       "      <td>0.916432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.360199</td>\n",
       "      <td>0.925964</td>\n",
       "      <td>0.919835</td>\n",
       "      <td>0.922711</td>\n",
       "      <td>0.920732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39400</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.361844</td>\n",
       "      <td>0.926762</td>\n",
       "      <td>0.920811</td>\n",
       "      <td>0.923258</td>\n",
       "      <td>0.921659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.363299</td>\n",
       "      <td>0.928417</td>\n",
       "      <td>0.921593</td>\n",
       "      <td>0.925346</td>\n",
       "      <td>0.923233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39800</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.365077</td>\n",
       "      <td>0.923511</td>\n",
       "      <td>0.919048</td>\n",
       "      <td>0.918752</td>\n",
       "      <td>0.918549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.386322</td>\n",
       "      <td>0.927162</td>\n",
       "      <td>0.922067</td>\n",
       "      <td>0.923183</td>\n",
       "      <td>0.922342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.370439</td>\n",
       "      <td>0.927390</td>\n",
       "      <td>0.921395</td>\n",
       "      <td>0.923743</td>\n",
       "      <td>0.922358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40400</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.382947</td>\n",
       "      <td>0.925850</td>\n",
       "      <td>0.919642</td>\n",
       "      <td>0.922464</td>\n",
       "      <td>0.920488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40600</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.374705</td>\n",
       "      <td>0.924196</td>\n",
       "      <td>0.919161</td>\n",
       "      <td>0.920241</td>\n",
       "      <td>0.918919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.397930</td>\n",
       "      <td>0.924082</td>\n",
       "      <td>0.917864</td>\n",
       "      <td>0.921048</td>\n",
       "      <td>0.918483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.408706</td>\n",
       "      <td>0.924709</td>\n",
       "      <td>0.918533</td>\n",
       "      <td>0.921792</td>\n",
       "      <td>0.919183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41200</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.380375</td>\n",
       "      <td>0.924937</td>\n",
       "      <td>0.920065</td>\n",
       "      <td>0.920468</td>\n",
       "      <td>0.919766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.374316</td>\n",
       "      <td>0.925793</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.921915</td>\n",
       "      <td>0.920548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41600</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.376166</td>\n",
       "      <td>0.926820</td>\n",
       "      <td>0.920202</td>\n",
       "      <td>0.924013</td>\n",
       "      <td>0.921389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41800</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.357271</td>\n",
       "      <td>0.926249</td>\n",
       "      <td>0.920501</td>\n",
       "      <td>0.922607</td>\n",
       "      <td>0.920944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.355985</td>\n",
       "      <td>0.927219</td>\n",
       "      <td>0.921866</td>\n",
       "      <td>0.923491</td>\n",
       "      <td>0.922211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42200</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.335097</td>\n",
       "      <td>0.927903</td>\n",
       "      <td>0.921950</td>\n",
       "      <td>0.924510</td>\n",
       "      <td>0.922840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42400</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.364864</td>\n",
       "      <td>0.925736</td>\n",
       "      <td>0.921061</td>\n",
       "      <td>0.921580</td>\n",
       "      <td>0.920710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42600</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.346351</td>\n",
       "      <td>0.928588</td>\n",
       "      <td>0.922682</td>\n",
       "      <td>0.925288</td>\n",
       "      <td>0.923629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42800</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.347158</td>\n",
       "      <td>0.926934</td>\n",
       "      <td>0.920018</td>\n",
       "      <td>0.924839</td>\n",
       "      <td>0.921612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.375790</td>\n",
       "      <td>0.928531</td>\n",
       "      <td>0.922181</td>\n",
       "      <td>0.925639</td>\n",
       "      <td>0.923376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43200</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.378352</td>\n",
       "      <td>0.929900</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.928146</td>\n",
       "      <td>0.924897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43400</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.387934</td>\n",
       "      <td>0.929671</td>\n",
       "      <td>0.923790</td>\n",
       "      <td>0.926365</td>\n",
       "      <td>0.924912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43600</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.409313</td>\n",
       "      <td>0.924823</td>\n",
       "      <td>0.921733</td>\n",
       "      <td>0.919711</td>\n",
       "      <td>0.920243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43800</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.425138</td>\n",
       "      <td>0.923169</td>\n",
       "      <td>0.917707</td>\n",
       "      <td>0.920138</td>\n",
       "      <td>0.917718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.407854</td>\n",
       "      <td>0.927846</td>\n",
       "      <td>0.920836</td>\n",
       "      <td>0.925768</td>\n",
       "      <td>0.922612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44200</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.417530</td>\n",
       "      <td>0.927447</td>\n",
       "      <td>0.921768</td>\n",
       "      <td>0.924018</td>\n",
       "      <td>0.922410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44400</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.432372</td>\n",
       "      <td>0.924880</td>\n",
       "      <td>0.920352</td>\n",
       "      <td>0.920937</td>\n",
       "      <td>0.919789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44600</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.411426</td>\n",
       "      <td>0.925165</td>\n",
       "      <td>0.920042</td>\n",
       "      <td>0.921315</td>\n",
       "      <td>0.919944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44800</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.411756</td>\n",
       "      <td>0.925451</td>\n",
       "      <td>0.920752</td>\n",
       "      <td>0.920922</td>\n",
       "      <td>0.920421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.418481</td>\n",
       "      <td>0.927048</td>\n",
       "      <td>0.921924</td>\n",
       "      <td>0.923231</td>\n",
       "      <td>0.922025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45200</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.393012</td>\n",
       "      <td>0.927618</td>\n",
       "      <td>0.920932</td>\n",
       "      <td>0.925328</td>\n",
       "      <td>0.922393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45400</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.407659</td>\n",
       "      <td>0.928017</td>\n",
       "      <td>0.922134</td>\n",
       "      <td>0.924369</td>\n",
       "      <td>0.922811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45600</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.402894</td>\n",
       "      <td>0.928873</td>\n",
       "      <td>0.922556</td>\n",
       "      <td>0.925583</td>\n",
       "      <td>0.923769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45800</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.406574</td>\n",
       "      <td>0.928188</td>\n",
       "      <td>0.921722</td>\n",
       "      <td>0.925302</td>\n",
       "      <td>0.923032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.418469</td>\n",
       "      <td>0.926877</td>\n",
       "      <td>0.920729</td>\n",
       "      <td>0.923660</td>\n",
       "      <td>0.921562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46200</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.426496</td>\n",
       "      <td>0.926648</td>\n",
       "      <td>0.920164</td>\n",
       "      <td>0.923850</td>\n",
       "      <td>0.921289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46400</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.424523</td>\n",
       "      <td>0.928702</td>\n",
       "      <td>0.923701</td>\n",
       "      <td>0.924698</td>\n",
       "      <td>0.923979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46600</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.448885</td>\n",
       "      <td>0.925394</td>\n",
       "      <td>0.920221</td>\n",
       "      <td>0.921519</td>\n",
       "      <td>0.920236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46800</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.440643</td>\n",
       "      <td>0.926078</td>\n",
       "      <td>0.921522</td>\n",
       "      <td>0.921767</td>\n",
       "      <td>0.921248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.437685</td>\n",
       "      <td>0.928702</td>\n",
       "      <td>0.922780</td>\n",
       "      <td>0.925771</td>\n",
       "      <td>0.923739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47200</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.448524</td>\n",
       "      <td>0.924139</td>\n",
       "      <td>0.919024</td>\n",
       "      <td>0.920383</td>\n",
       "      <td>0.918907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47400</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.461690</td>\n",
       "      <td>0.925679</td>\n",
       "      <td>0.920233</td>\n",
       "      <td>0.922204</td>\n",
       "      <td>0.920558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47600</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.460081</td>\n",
       "      <td>0.925108</td>\n",
       "      <td>0.920154</td>\n",
       "      <td>0.921099</td>\n",
       "      <td>0.920069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47800</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.454691</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.921740</td>\n",
       "      <td>0.923225</td>\n",
       "      <td>0.922041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.436775</td>\n",
       "      <td>0.929786</td>\n",
       "      <td>0.924281</td>\n",
       "      <td>0.926420</td>\n",
       "      <td>0.925080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48200</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.461472</td>\n",
       "      <td>0.925508</td>\n",
       "      <td>0.920587</td>\n",
       "      <td>0.921464</td>\n",
       "      <td>0.920405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48400</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.455564</td>\n",
       "      <td>0.926934</td>\n",
       "      <td>0.922032</td>\n",
       "      <td>0.922955</td>\n",
       "      <td>0.922021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48600</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.463531</td>\n",
       "      <td>0.925508</td>\n",
       "      <td>0.920274</td>\n",
       "      <td>0.922118</td>\n",
       "      <td>0.920467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48800</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.436318</td>\n",
       "      <td>0.928245</td>\n",
       "      <td>0.922214</td>\n",
       "      <td>0.925038</td>\n",
       "      <td>0.923241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.421261</td>\n",
       "      <td>0.928188</td>\n",
       "      <td>0.922375</td>\n",
       "      <td>0.924760</td>\n",
       "      <td>0.923224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49200</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.439851</td>\n",
       "      <td>0.926135</td>\n",
       "      <td>0.920928</td>\n",
       "      <td>0.922259</td>\n",
       "      <td>0.921109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49400</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.427823</td>\n",
       "      <td>0.927789</td>\n",
       "      <td>0.922309</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>0.922824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49600</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.447232</td>\n",
       "      <td>0.925964</td>\n",
       "      <td>0.920362</td>\n",
       "      <td>0.922419</td>\n",
       "      <td>0.920821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49800</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.447886</td>\n",
       "      <td>0.927390</td>\n",
       "      <td>0.921635</td>\n",
       "      <td>0.923808</td>\n",
       "      <td>0.922244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.438444</td>\n",
       "      <td>0.927162</td>\n",
       "      <td>0.921258</td>\n",
       "      <td>0.923795</td>\n",
       "      <td>0.921987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50200</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.424906</td>\n",
       "      <td>0.928987</td>\n",
       "      <td>0.923329</td>\n",
       "      <td>0.925513</td>\n",
       "      <td>0.924121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50400</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.436103</td>\n",
       "      <td>0.928017</td>\n",
       "      <td>0.922665</td>\n",
       "      <td>0.924009</td>\n",
       "      <td>0.923030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50600</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.462843</td>\n",
       "      <td>0.925736</td>\n",
       "      <td>0.920302</td>\n",
       "      <td>0.921784</td>\n",
       "      <td>0.920543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50800</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.446724</td>\n",
       "      <td>0.927390</td>\n",
       "      <td>0.922006</td>\n",
       "      <td>0.923473</td>\n",
       "      <td>0.922398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.451134</td>\n",
       "      <td>0.927618</td>\n",
       "      <td>0.922203</td>\n",
       "      <td>0.923759</td>\n",
       "      <td>0.922638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51200</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.443821</td>\n",
       "      <td>0.928417</td>\n",
       "      <td>0.923289</td>\n",
       "      <td>0.924350</td>\n",
       "      <td>0.923634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51400</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.441568</td>\n",
       "      <td>0.928759</td>\n",
       "      <td>0.922714</td>\n",
       "      <td>0.925494</td>\n",
       "      <td>0.923784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51600</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.449341</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.921450</td>\n",
       "      <td>0.923175</td>\n",
       "      <td>0.921917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51800</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.433916</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.921791</td>\n",
       "      <td>0.922861</td>\n",
       "      <td>0.921988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>0.429057</td>\n",
       "      <td>0.927618</td>\n",
       "      <td>0.921825</td>\n",
       "      <td>0.924224</td>\n",
       "      <td>0.922563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52200</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>0.442442</td>\n",
       "      <td>0.927276</td>\n",
       "      <td>0.921883</td>\n",
       "      <td>0.923608</td>\n",
       "      <td>0.922324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52400</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>0.428511</td>\n",
       "      <td>0.927618</td>\n",
       "      <td>0.921998</td>\n",
       "      <td>0.923863</td>\n",
       "      <td>0.922567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52600</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.444999</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.921830</td>\n",
       "      <td>0.922749</td>\n",
       "      <td>0.921985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52800</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.442609</td>\n",
       "      <td>0.927276</td>\n",
       "      <td>0.921543</td>\n",
       "      <td>0.923712</td>\n",
       "      <td>0.922160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.441649</td>\n",
       "      <td>0.927333</td>\n",
       "      <td>0.922027</td>\n",
       "      <td>0.923368</td>\n",
       "      <td>0.922370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53200</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.444981</td>\n",
       "      <td>0.927219</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>0.923643</td>\n",
       "      <td>0.922175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53400</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.435088</td>\n",
       "      <td>0.927276</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.923394</td>\n",
       "      <td>0.922275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53600</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.438086</td>\n",
       "      <td>0.927390</td>\n",
       "      <td>0.921815</td>\n",
       "      <td>0.923668</td>\n",
       "      <td>0.922396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53800</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.440457</td>\n",
       "      <td>0.928303</td>\n",
       "      <td>0.922839</td>\n",
       "      <td>0.924588</td>\n",
       "      <td>0.923380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.441598</td>\n",
       "      <td>0.927447</td>\n",
       "      <td>0.921718</td>\n",
       "      <td>0.923901</td>\n",
       "      <td>0.922350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54200</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.440788</td>\n",
       "      <td>0.926762</td>\n",
       "      <td>0.921398</td>\n",
       "      <td>0.922959</td>\n",
       "      <td>0.921742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54400</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.450313</td>\n",
       "      <td>0.927276</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.923721</td>\n",
       "      <td>0.922203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54600</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.453868</td>\n",
       "      <td>0.926249</td>\n",
       "      <td>0.920895</td>\n",
       "      <td>0.922582</td>\n",
       "      <td>0.921133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54800</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.453186</td>\n",
       "      <td>0.926705</td>\n",
       "      <td>0.921413</td>\n",
       "      <td>0.922870</td>\n",
       "      <td>0.921638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.448890</td>\n",
       "      <td>0.926477</td>\n",
       "      <td>0.920992</td>\n",
       "      <td>0.922882</td>\n",
       "      <td>0.921376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55200</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.449962</td>\n",
       "      <td>0.926078</td>\n",
       "      <td>0.920725</td>\n",
       "      <td>0.922486</td>\n",
       "      <td>0.920957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55400</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.444295</td>\n",
       "      <td>0.926363</td>\n",
       "      <td>0.921040</td>\n",
       "      <td>0.922608</td>\n",
       "      <td>0.921281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55600</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.446101</td>\n",
       "      <td>0.926762</td>\n",
       "      <td>0.921688</td>\n",
       "      <td>0.922808</td>\n",
       "      <td>0.921755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55800</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.440661</td>\n",
       "      <td>0.927105</td>\n",
       "      <td>0.922048</td>\n",
       "      <td>0.922974</td>\n",
       "      <td>0.922118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.443235</td>\n",
       "      <td>0.926648</td>\n",
       "      <td>0.921632</td>\n",
       "      <td>0.922486</td>\n",
       "      <td>0.921643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56200</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.926762</td>\n",
       "      <td>0.921741</td>\n",
       "      <td>0.922627</td>\n",
       "      <td>0.921773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56400</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.441103</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.921870</td>\n",
       "      <td>0.922861</td>\n",
       "      <td>0.921964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56600</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.438643</td>\n",
       "      <td>0.927561</td>\n",
       "      <td>0.922350</td>\n",
       "      <td>0.923475</td>\n",
       "      <td>0.922533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56800</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.440223</td>\n",
       "      <td>0.927105</td>\n",
       "      <td>0.921912</td>\n",
       "      <td>0.923054</td>\n",
       "      <td>0.922073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.440290</td>\n",
       "      <td>0.927105</td>\n",
       "      <td>0.921935</td>\n",
       "      <td>0.923032</td>\n",
       "      <td>0.922077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=57076, training_loss=0.12359596175146266, metrics={'train_runtime': 10621.027, 'train_samples_per_second': 85.977, 'train_steps_per_second': 5.374, 'total_flos': 4.132935820790885e+16, 'train_loss': 0.12359596175146266, 'epoch': 4.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "    \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c033e6c-1341-4ddd-9c81-10f456146e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_store_path.parent / (model_store_path.stem + \".out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fc1ce-86d3-4168-a4a1-71f521fe37a3",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b699f7bb-e4c9-45ad-9fd1-d7425dd6f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7590-31cc-40ad-b212-d0c47500277f",
   "metadata": {},
   "source": [
    "## Test on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdfafa9a-6265-4792-8513-149ab1b0545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"validation\"])\n",
    "micro_val, macro_val = generate_micro_macro_df(data[\"validation\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c16081ff-6aa6-4620-8cdf-50f3e07b73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.95      0.96      0.96      7604\n",
      "        REFUTES       0.94      0.89      0.91      5020\n",
      "       SUPPORTS       0.89      0.92      0.91      4908\n",
      "\n",
      "       accuracy                           0.93     17532\n",
      "      macro avg       0.93      0.92      0.93     17532\n",
      "   weighted avg       0.93      0.93      0.93     17532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=micro_val[\"actual\"], y_pred=micro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d4cf6-3cfe-458f-9ad7-e58994554cbc",
   "metadata": {},
   "source": [
    "## Test on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9714a94-7d27-4b35-95ce-d53a47fc15dd",
   "metadata": {},
   "source": [
    "### FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b9b87e1-0054-4bbe-a6e8-b76867f03b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"fever_test\"])\n",
    "micro_val, macro_val = generate_micro_macro_df(data[\"fever_test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6548fc2f-a81f-4e65-b697-0b9e80ba9402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.98      0.98      0.98      6666\n",
      "        REFUTES       0.93      0.87      0.90      4909\n",
      "       SUPPORTS       0.89      0.94      0.91      4631\n",
      "\n",
      "       accuracy                           0.94     16206\n",
      "      macro avg       0.93      0.93      0.93     16206\n",
      "   weighted avg       0.94      0.94      0.94     16206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=micro_val[\"actual\"], y_pred=micro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d884051-0c43-49e8-bbd7-5db17f9db858",
   "metadata": {},
   "source": [
    "### Climate-FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "252a4cf4-2ef4-482a-bb2f-dd525be6ed6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"climatefever_test\"])\n",
    "micro_val, macro_val = generate_micro_macro_df(data[\"climatefever_test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a5e65f6-690a-4506-b890-d268cb69d891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.65      0.86      0.74       235\n",
      "        REFUTES       0.58      0.31      0.41        48\n",
      "       SUPPORTS       0.74      0.52      0.61       176\n",
      "\n",
      "       accuracy                           0.67       459\n",
      "      macro avg       0.66      0.56      0.59       459\n",
      "   weighted avg       0.68      0.67      0.66       459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=micro_val[\"actual\"], y_pred=micro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae0ef8-63c5-4092-b9a3-1528fe23a2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
