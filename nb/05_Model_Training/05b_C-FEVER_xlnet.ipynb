{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402ecfd6-f01c-43ef-aaa4-018f9dc63dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/gen\")\n",
    "sys.path.insert(1, \"../src/rte\")\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from util import read_data, write_jsonl\n",
    "from aggregate import generate_micro_macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1929c84-28f4-4949-b880-a85b8f8b8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "LOOKUP = {\n",
    "    \"verifiable\": {\"no\": \"NOT VERIFIABLE\", \"yes\": \"VERIFIABLE\"},\n",
    "    \"label\": {\"nei\": \"NOT ENOUGH INFO\", \"r\": \"REFUTES\", \"s\": \"SUPPORTS\"}\n",
    "}\n",
    "\n",
    "SEED = 123456789\n",
    "\n",
    "LABEL2ID = {\"SUPPORTS\": 0, \"NOT ENOUGH INFO\": 1, \"REFUTES\": 2}\n",
    "ID2LABEL = {0: \"SUPPORTS\", 1: \"NOT ENOUGH INFO\", 2: \"REFUTES\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05b8ab-6775-4eed-98be-10904943ded1",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aacd87-5b38-45b8-8be3-f7046bfa06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    TextClassificationPipeline,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd986a-1e45-4f38-8487-dddf2ee3ef24",
   "metadata": {},
   "source": [
    "# Huggingface Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b42b8e-2eb3-444b-9275-192519540439",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e5281d-7e7d-410e-b1ba-730e334247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2c86dd-cb1b-4b77-836d-1e5619f4bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"xlnet-base-cased\"\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        num_labels=3, \n",
    "        id2label=ID2LABEL, \n",
    "        label2id=LABEL2ID\n",
    "    )\n",
    "\n",
    "model = model_init()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, do_lower_case=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"evidence\"], examples[\"claim\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4834e0-770d-49dc-8041-f4d9d2729053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"fever\", \"climatefeverpure\", \"fever-climatefeverpure\", \"climatefever\", \"fever-climatefever\"]\n",
    "task = [\"hp_tune\", \"output\"]\n",
    "doc_sent = [\"doc\", \"sent\"]\n",
    "\n",
    "di = 4\n",
    "ti = 0\n",
    "ds = 1\n",
    "\n",
    "model_store_path = Path(\"/users/k21190024/study/fact-check-transfer-learning/scratch/thesis/models\").joinpath(model_checkpoint)\n",
    "model_store_path.mkdir(exist_ok=True)\n",
    "model_store_path = model_store_path / f\"{dataset[di]}-{model_checkpoint}-{doc_sent[ds]}-{task[ti]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940210d7-1da5-4e6f-b4f6-95304ceb496b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a618a5-0c63-4a99-b7cb-89deea2416f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/228290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datap = Path(f\"/users/k21190024/study/fact-check-transfer-learning/scratch/dumps/bert-data-{doc_sent[ds]}-evidence\")\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.train.n5.jsonl\")),\n",
    "    \"validation\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.dev.n5.jsonl\")),\n",
    "    \"fever_test\": Dataset.from_list(read_data(datap / f\"{dataset[0]}.test.n5.jsonl\")),\n",
    "    \"climatefever_test\": Dataset.from_list(read_data(datap / f\"{dataset[1]}.test.n5.jsonl\"))\n",
    "}).map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d7f9a-4ddc-4a9b-b6cf-d31233fff94b",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06cfa045-016f-44e2-91c9-567ddd56ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 4e-4\n",
    "epoch = 4\n",
    "metric_name = \"f1\"\n",
    "warmup_ratio=0.1\n",
    "save_steps=200\n",
    "eval_steps=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666373d-06fa-476c-9aaf-ffb3daadbf4b",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967aecb0-5bc7-488c-aefa-caf4525216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shard the data if the dataset is large for hyperparameter tuning\n",
    "shard = data[\"train\"].num_rows > 50000\n",
    "hp_tune_train = data[\"train\"].shuffle(seed=SEED).shard(num_shards=5, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e23d248-bb05-43da-a056-4e1fcc2a17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy = \"no\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "_ = model.train()\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=hp_tune_train if shard else data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5445d33-24fc-4a6d-8ed2-a89b26aa65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [1e-5, 3e-5, 2e-5]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "    }\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d4edde-28e8-4b30-a700-dc3f4a2a2ce9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 13:02:16,683] A new study created in memory with name: no-name-0ca0b7ce-7d62-488a-b0b3-121379fdd2e1\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a XLNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1417' max='1417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1417/1417 09:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.716896</td>\n",
       "      <td>0.687972</td>\n",
       "      <td>0.667642</td>\n",
       "      <td>0.787427</td>\n",
       "      <td>0.616972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.377311</td>\n",
       "      <td>0.861386</td>\n",
       "      <td>0.858889</td>\n",
       "      <td>0.856307</td>\n",
       "      <td>0.854226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.552400</td>\n",
       "      <td>0.335807</td>\n",
       "      <td>0.878376</td>\n",
       "      <td>0.872998</td>\n",
       "      <td>0.878234</td>\n",
       "      <td>0.870160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.552400</td>\n",
       "      <td>0.339101</td>\n",
       "      <td>0.880427</td>\n",
       "      <td>0.877289</td>\n",
       "      <td>0.878673</td>\n",
       "      <td>0.874102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.283244</td>\n",
       "      <td>0.897885</td>\n",
       "      <td>0.892081</td>\n",
       "      <td>0.893379</td>\n",
       "      <td>0.891712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.325783</td>\n",
       "      <td>0.889976</td>\n",
       "      <td>0.890187</td>\n",
       "      <td>0.885488</td>\n",
       "      <td>0.885494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.295013</td>\n",
       "      <td>0.896596</td>\n",
       "      <td>0.894413</td>\n",
       "      <td>0.891339</td>\n",
       "      <td>0.891438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 13:11:32,540] Trial 0 finished with value: 3.573787006087937 and parameters: {'learning_rate': 1e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 1}. Best is trial 0 with value: 3.573787006087937.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7085' max='7085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7085/7085 46:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.991626</td>\n",
       "      <td>0.542914</td>\n",
       "      <td>0.527758</td>\n",
       "      <td>0.573669</td>\n",
       "      <td>0.431203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.560934</td>\n",
       "      <td>0.761146</td>\n",
       "      <td>0.745409</td>\n",
       "      <td>0.792976</td>\n",
       "      <td>0.727212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.760900</td>\n",
       "      <td>0.395433</td>\n",
       "      <td>0.850021</td>\n",
       "      <td>0.839189</td>\n",
       "      <td>0.858914</td>\n",
       "      <td>0.835378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.760900</td>\n",
       "      <td>0.367791</td>\n",
       "      <td>0.870233</td>\n",
       "      <td>0.863724</td>\n",
       "      <td>0.873036</td>\n",
       "      <td>0.861422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.302653</td>\n",
       "      <td>0.894428</td>\n",
       "      <td>0.887027</td>\n",
       "      <td>0.891225</td>\n",
       "      <td>0.887340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.318581</td>\n",
       "      <td>0.892730</td>\n",
       "      <td>0.892704</td>\n",
       "      <td>0.887653</td>\n",
       "      <td>0.888290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.900580</td>\n",
       "      <td>0.894967</td>\n",
       "      <td>0.898088</td>\n",
       "      <td>0.894205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.239300</td>\n",
       "      <td>0.314854</td>\n",
       "      <td>0.892905</td>\n",
       "      <td>0.893231</td>\n",
       "      <td>0.887952</td>\n",
       "      <td>0.888542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.239300</td>\n",
       "      <td>0.279520</td>\n",
       "      <td>0.904915</td>\n",
       "      <td>0.900820</td>\n",
       "      <td>0.900270</td>\n",
       "      <td>0.899611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.342002</td>\n",
       "      <td>0.895190</td>\n",
       "      <td>0.894341</td>\n",
       "      <td>0.893667</td>\n",
       "      <td>0.890263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.265601</td>\n",
       "      <td>0.908899</td>\n",
       "      <td>0.907567</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>0.904787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.370008</td>\n",
       "      <td>0.882067</td>\n",
       "      <td>0.881869</td>\n",
       "      <td>0.886165</td>\n",
       "      <td>0.877478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.275680</td>\n",
       "      <td>0.908489</td>\n",
       "      <td>0.906377</td>\n",
       "      <td>0.905647</td>\n",
       "      <td>0.904221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.273044</td>\n",
       "      <td>0.912121</td>\n",
       "      <td>0.910110</td>\n",
       "      <td>0.907576</td>\n",
       "      <td>0.907569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.293350</td>\n",
       "      <td>0.911594</td>\n",
       "      <td>0.910043</td>\n",
       "      <td>0.907736</td>\n",
       "      <td>0.907362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.298332</td>\n",
       "      <td>0.921554</td>\n",
       "      <td>0.915661</td>\n",
       "      <td>0.917447</td>\n",
       "      <td>0.916392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.289023</td>\n",
       "      <td>0.908313</td>\n",
       "      <td>0.909787</td>\n",
       "      <td>0.902772</td>\n",
       "      <td>0.905056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.307689</td>\n",
       "      <td>0.907962</td>\n",
       "      <td>0.904514</td>\n",
       "      <td>0.906779</td>\n",
       "      <td>0.902616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.337759</td>\n",
       "      <td>0.907727</td>\n",
       "      <td>0.905973</td>\n",
       "      <td>0.904425</td>\n",
       "      <td>0.903139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.337239</td>\n",
       "      <td>0.905560</td>\n",
       "      <td>0.906513</td>\n",
       "      <td>0.900647</td>\n",
       "      <td>0.902069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.283913</td>\n",
       "      <td>0.916222</td>\n",
       "      <td>0.914027</td>\n",
       "      <td>0.912191</td>\n",
       "      <td>0.911984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.292401</td>\n",
       "      <td>0.918449</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.913537</td>\n",
       "      <td>0.913987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.337629</td>\n",
       "      <td>0.911301</td>\n",
       "      <td>0.911429</td>\n",
       "      <td>0.905997</td>\n",
       "      <td>0.907586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.306013</td>\n",
       "      <td>0.918976</td>\n",
       "      <td>0.916451</td>\n",
       "      <td>0.914250</td>\n",
       "      <td>0.914733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.330825</td>\n",
       "      <td>0.919503</td>\n",
       "      <td>0.916046</td>\n",
       "      <td>0.915176</td>\n",
       "      <td>0.915010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.304566</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.915463</td>\n",
       "      <td>0.914898</td>\n",
       "      <td>0.914914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.324962</td>\n",
       "      <td>0.917336</td>\n",
       "      <td>0.915854</td>\n",
       "      <td>0.912467</td>\n",
       "      <td>0.913522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.336195</td>\n",
       "      <td>0.916222</td>\n",
       "      <td>0.914262</td>\n",
       "      <td>0.912530</td>\n",
       "      <td>0.912064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.350926</td>\n",
       "      <td>0.916750</td>\n",
       "      <td>0.915034</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.912719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.372870</td>\n",
       "      <td>0.913117</td>\n",
       "      <td>0.913080</td>\n",
       "      <td>0.907935</td>\n",
       "      <td>0.909518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.377020</td>\n",
       "      <td>0.913820</td>\n",
       "      <td>0.913148</td>\n",
       "      <td>0.909320</td>\n",
       "      <td>0.909992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.371996</td>\n",
       "      <td>0.914699</td>\n",
       "      <td>0.914056</td>\n",
       "      <td>0.909798</td>\n",
       "      <td>0.910961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>0.373390</td>\n",
       "      <td>0.915754</td>\n",
       "      <td>0.914749</td>\n",
       "      <td>0.911161</td>\n",
       "      <td>0.911925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>0.367018</td>\n",
       "      <td>0.917277</td>\n",
       "      <td>0.915794</td>\n",
       "      <td>0.912289</td>\n",
       "      <td>0.913355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.369464</td>\n",
       "      <td>0.917101</td>\n",
       "      <td>0.915778</td>\n",
       "      <td>0.912557</td>\n",
       "      <td>0.913210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 13:58:01,520] Trial 1 finished with value: 3.658646782667355 and parameters: {'learning_rate': 1e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 5}. Best is trial 1 with value: 3.658646782667355.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5668' max='5668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5668/5668 37:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.808979</td>\n",
       "      <td>0.615912</td>\n",
       "      <td>0.595405</td>\n",
       "      <td>0.727615</td>\n",
       "      <td>0.504961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.344738</td>\n",
       "      <td>0.869998</td>\n",
       "      <td>0.864352</td>\n",
       "      <td>0.863474</td>\n",
       "      <td>0.862290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>0.335789</td>\n",
       "      <td>0.878845</td>\n",
       "      <td>0.871105</td>\n",
       "      <td>0.880043</td>\n",
       "      <td>0.869344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>0.418749</td>\n",
       "      <td>0.851016</td>\n",
       "      <td>0.846806</td>\n",
       "      <td>0.863699</td>\n",
       "      <td>0.841680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.356141</td>\n",
       "      <td>0.872400</td>\n",
       "      <td>0.864598</td>\n",
       "      <td>0.878567</td>\n",
       "      <td>0.861954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.305681</td>\n",
       "      <td>0.902513</td>\n",
       "      <td>0.899428</td>\n",
       "      <td>0.897116</td>\n",
       "      <td>0.897409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.262440</td>\n",
       "      <td>0.912473</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>0.908583</td>\n",
       "      <td>0.907086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.263554</td>\n",
       "      <td>0.910657</td>\n",
       "      <td>0.906165</td>\n",
       "      <td>0.905734</td>\n",
       "      <td>0.905548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.388181</td>\n",
       "      <td>0.877673</td>\n",
       "      <td>0.880185</td>\n",
       "      <td>0.875383</td>\n",
       "      <td>0.873752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.368201</td>\n",
       "      <td>0.897944</td>\n",
       "      <td>0.896731</td>\n",
       "      <td>0.895872</td>\n",
       "      <td>0.893011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.277372</td>\n",
       "      <td>0.912180</td>\n",
       "      <td>0.909491</td>\n",
       "      <td>0.907142</td>\n",
       "      <td>0.907589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.402563</td>\n",
       "      <td>0.882477</td>\n",
       "      <td>0.876315</td>\n",
       "      <td>0.887593</td>\n",
       "      <td>0.873702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.300686</td>\n",
       "      <td>0.907317</td>\n",
       "      <td>0.904993</td>\n",
       "      <td>0.905642</td>\n",
       "      <td>0.903039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.314004</td>\n",
       "      <td>0.912121</td>\n",
       "      <td>0.910579</td>\n",
       "      <td>0.907917</td>\n",
       "      <td>0.907681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.328064</td>\n",
       "      <td>0.908723</td>\n",
       "      <td>0.909601</td>\n",
       "      <td>0.902975</td>\n",
       "      <td>0.905344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.342033</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.909374</td>\n",
       "      <td>0.906127</td>\n",
       "      <td>0.906575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.315755</td>\n",
       "      <td>0.915344</td>\n",
       "      <td>0.912922</td>\n",
       "      <td>0.910651</td>\n",
       "      <td>0.910914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.389136</td>\n",
       "      <td>0.908723</td>\n",
       "      <td>0.905160</td>\n",
       "      <td>0.905646</td>\n",
       "      <td>0.903472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.398846</td>\n",
       "      <td>0.908372</td>\n",
       "      <td>0.907364</td>\n",
       "      <td>0.903266</td>\n",
       "      <td>0.904155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.372487</td>\n",
       "      <td>0.908841</td>\n",
       "      <td>0.909462</td>\n",
       "      <td>0.903603</td>\n",
       "      <td>0.905355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.316217</td>\n",
       "      <td>0.917511</td>\n",
       "      <td>0.913364</td>\n",
       "      <td>0.914308</td>\n",
       "      <td>0.912742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.408256</td>\n",
       "      <td>0.911418</td>\n",
       "      <td>0.910276</td>\n",
       "      <td>0.907027</td>\n",
       "      <td>0.907214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.406409</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.909271</td>\n",
       "      <td>0.906391</td>\n",
       "      <td>0.906432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.411937</td>\n",
       "      <td>0.914406</td>\n",
       "      <td>0.913353</td>\n",
       "      <td>0.909296</td>\n",
       "      <td>0.910349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.422991</td>\n",
       "      <td>0.917980</td>\n",
       "      <td>0.915106</td>\n",
       "      <td>0.913569</td>\n",
       "      <td>0.913485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.415182</td>\n",
       "      <td>0.915402</td>\n",
       "      <td>0.912977</td>\n",
       "      <td>0.910774</td>\n",
       "      <td>0.911083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.423443</td>\n",
       "      <td>0.915285</td>\n",
       "      <td>0.913448</td>\n",
       "      <td>0.910458</td>\n",
       "      <td>0.911132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.420893</td>\n",
       "      <td>0.914172</td>\n",
       "      <td>0.912561</td>\n",
       "      <td>0.909303</td>\n",
       "      <td>0.910090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 14:35:11,646] Trial 2 finished with value: 3.646126066394134 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 1 with value: 3.658646782667355.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1417' max='1417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1417/1417 09:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.882749</td>\n",
       "      <td>0.722479</td>\n",
       "      <td>0.735403</td>\n",
       "      <td>0.789105</td>\n",
       "      <td>0.721355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.390005</td>\n",
       "      <td>0.864608</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.861326</td>\n",
       "      <td>0.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.463600</td>\n",
       "      <td>0.333252</td>\n",
       "      <td>0.881833</td>\n",
       "      <td>0.878264</td>\n",
       "      <td>0.881228</td>\n",
       "      <td>0.874615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.463600</td>\n",
       "      <td>0.307227</td>\n",
       "      <td>0.898002</td>\n",
       "      <td>0.893006</td>\n",
       "      <td>0.894276</td>\n",
       "      <td>0.891952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.231600</td>\n",
       "      <td>0.262070</td>\n",
       "      <td>0.910833</td>\n",
       "      <td>0.905072</td>\n",
       "      <td>0.906392</td>\n",
       "      <td>0.905245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.231600</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>0.897944</td>\n",
       "      <td>0.899082</td>\n",
       "      <td>0.894488</td>\n",
       "      <td>0.894169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.231600</td>\n",
       "      <td>0.260332</td>\n",
       "      <td>0.910715</td>\n",
       "      <td>0.908934</td>\n",
       "      <td>0.905414</td>\n",
       "      <td>0.906469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 14:44:29,860] Trial 3 finished with value: 3.631532024816712 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 1}. Best is trial 1 with value: 3.658646782667355.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11336' max='11336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11336/11336 53:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.945668</td>\n",
       "      <td>0.580643</td>\n",
       "      <td>0.557776</td>\n",
       "      <td>0.753864</td>\n",
       "      <td>0.458678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.524797</td>\n",
       "      <td>0.780011</td>\n",
       "      <td>0.764878</td>\n",
       "      <td>0.814697</td>\n",
       "      <td>0.750506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.716400</td>\n",
       "      <td>0.553814</td>\n",
       "      <td>0.796825</td>\n",
       "      <td>0.784358</td>\n",
       "      <td>0.831344</td>\n",
       "      <td>0.772320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.716400</td>\n",
       "      <td>0.333038</td>\n",
       "      <td>0.877732</td>\n",
       "      <td>0.866222</td>\n",
       "      <td>0.872450</td>\n",
       "      <td>0.868616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>0.448578</td>\n",
       "      <td>0.852833</td>\n",
       "      <td>0.851860</td>\n",
       "      <td>0.853679</td>\n",
       "      <td>0.846213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>0.395922</td>\n",
       "      <td>0.863788</td>\n",
       "      <td>0.863590</td>\n",
       "      <td>0.859414</td>\n",
       "      <td>0.858036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>0.355090</td>\n",
       "      <td>0.882418</td>\n",
       "      <td>0.880305</td>\n",
       "      <td>0.875614</td>\n",
       "      <td>0.877327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>0.391661</td>\n",
       "      <td>0.876560</td>\n",
       "      <td>0.877893</td>\n",
       "      <td>0.870532</td>\n",
       "      <td>0.872342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>0.903041</td>\n",
       "      <td>0.896567</td>\n",
       "      <td>0.899136</td>\n",
       "      <td>0.896867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.279200</td>\n",
       "      <td>0.372483</td>\n",
       "      <td>0.888101</td>\n",
       "      <td>0.884943</td>\n",
       "      <td>0.883500</td>\n",
       "      <td>0.882365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.279200</td>\n",
       "      <td>0.411802</td>\n",
       "      <td>0.891382</td>\n",
       "      <td>0.892464</td>\n",
       "      <td>0.885722</td>\n",
       "      <td>0.887186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.279200</td>\n",
       "      <td>0.339152</td>\n",
       "      <td>0.892378</td>\n",
       "      <td>0.889720</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.886667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.324035</td>\n",
       "      <td>0.906028</td>\n",
       "      <td>0.899315</td>\n",
       "      <td>0.902585</td>\n",
       "      <td>0.900261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.298045</td>\n",
       "      <td>0.902689</td>\n",
       "      <td>0.896015</td>\n",
       "      <td>0.902672</td>\n",
       "      <td>0.896993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.580849</td>\n",
       "      <td>0.854004</td>\n",
       "      <td>0.860402</td>\n",
       "      <td>0.857369</td>\n",
       "      <td>0.851306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.395063</td>\n",
       "      <td>0.892085</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.890089</td>\n",
       "      <td>0.887929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.393730</td>\n",
       "      <td>0.893784</td>\n",
       "      <td>0.892600</td>\n",
       "      <td>0.891270</td>\n",
       "      <td>0.888870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.195100</td>\n",
       "      <td>0.449904</td>\n",
       "      <td>0.879431</td>\n",
       "      <td>0.882367</td>\n",
       "      <td>0.875701</td>\n",
       "      <td>0.875704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.195100</td>\n",
       "      <td>0.375749</td>\n",
       "      <td>0.904681</td>\n",
       "      <td>0.900316</td>\n",
       "      <td>0.901790</td>\n",
       "      <td>0.898825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.183300</td>\n",
       "      <td>0.538932</td>\n",
       "      <td>0.886695</td>\n",
       "      <td>0.889021</td>\n",
       "      <td>0.886029</td>\n",
       "      <td>0.883299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.183300</td>\n",
       "      <td>0.420062</td>\n",
       "      <td>0.897709</td>\n",
       "      <td>0.897368</td>\n",
       "      <td>0.894800</td>\n",
       "      <td>0.893549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.183300</td>\n",
       "      <td>0.414528</td>\n",
       "      <td>0.889859</td>\n",
       "      <td>0.890453</td>\n",
       "      <td>0.889200</td>\n",
       "      <td>0.885475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.318447</td>\n",
       "      <td>0.905618</td>\n",
       "      <td>0.899957</td>\n",
       "      <td>0.903941</td>\n",
       "      <td>0.899352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.442495</td>\n",
       "      <td>0.894897</td>\n",
       "      <td>0.893745</td>\n",
       "      <td>0.893508</td>\n",
       "      <td>0.890032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.385152</td>\n",
       "      <td>0.904447</td>\n",
       "      <td>0.900745</td>\n",
       "      <td>0.901334</td>\n",
       "      <td>0.898979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.379237</td>\n",
       "      <td>0.902689</td>\n",
       "      <td>0.900618</td>\n",
       "      <td>0.900277</td>\n",
       "      <td>0.897951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.358451</td>\n",
       "      <td>0.911946</td>\n",
       "      <td>0.907238</td>\n",
       "      <td>0.910018</td>\n",
       "      <td>0.906701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.516757</td>\n",
       "      <td>0.896362</td>\n",
       "      <td>0.899257</td>\n",
       "      <td>0.890963</td>\n",
       "      <td>0.893223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.448919</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.903061</td>\n",
       "      <td>0.901087</td>\n",
       "      <td>0.899225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.493754</td>\n",
       "      <td>0.903861</td>\n",
       "      <td>0.902583</td>\n",
       "      <td>0.901660</td>\n",
       "      <td>0.899255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.439623</td>\n",
       "      <td>0.913117</td>\n",
       "      <td>0.909711</td>\n",
       "      <td>0.908151</td>\n",
       "      <td>0.908365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.384394</td>\n",
       "      <td>0.915812</td>\n",
       "      <td>0.910763</td>\n",
       "      <td>0.911866</td>\n",
       "      <td>0.910547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>0.443889</td>\n",
       "      <td>0.911125</td>\n",
       "      <td>0.908793</td>\n",
       "      <td>0.905359</td>\n",
       "      <td>0.906848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>0.413275</td>\n",
       "      <td>0.907552</td>\n",
       "      <td>0.907149</td>\n",
       "      <td>0.903179</td>\n",
       "      <td>0.903701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.435079</td>\n",
       "      <td>0.909895</td>\n",
       "      <td>0.908181</td>\n",
       "      <td>0.904558</td>\n",
       "      <td>0.905690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.432770</td>\n",
       "      <td>0.910481</td>\n",
       "      <td>0.906690</td>\n",
       "      <td>0.908337</td>\n",
       "      <td>0.905331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.474583</td>\n",
       "      <td>0.903744</td>\n",
       "      <td>0.904160</td>\n",
       "      <td>0.898958</td>\n",
       "      <td>0.900054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.505237</td>\n",
       "      <td>0.909544</td>\n",
       "      <td>0.907857</td>\n",
       "      <td>0.906255</td>\n",
       "      <td>0.905081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.395316</td>\n",
       "      <td>0.913996</td>\n",
       "      <td>0.910742</td>\n",
       "      <td>0.909978</td>\n",
       "      <td>0.909198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.452375</td>\n",
       "      <td>0.910012</td>\n",
       "      <td>0.909692</td>\n",
       "      <td>0.905066</td>\n",
       "      <td>0.906260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.408656</td>\n",
       "      <td>0.910540</td>\n",
       "      <td>0.908499</td>\n",
       "      <td>0.907302</td>\n",
       "      <td>0.906205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.449562</td>\n",
       "      <td>0.911828</td>\n",
       "      <td>0.910929</td>\n",
       "      <td>0.908829</td>\n",
       "      <td>0.907943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.463755</td>\n",
       "      <td>0.911418</td>\n",
       "      <td>0.910581</td>\n",
       "      <td>0.907311</td>\n",
       "      <td>0.907538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.429231</td>\n",
       "      <td>0.917453</td>\n",
       "      <td>0.913922</td>\n",
       "      <td>0.913442</td>\n",
       "      <td>0.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>0.438233</td>\n",
       "      <td>0.916808</td>\n",
       "      <td>0.915058</td>\n",
       "      <td>0.912228</td>\n",
       "      <td>0.912938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>0.487549</td>\n",
       "      <td>0.909426</td>\n",
       "      <td>0.908613</td>\n",
       "      <td>0.906116</td>\n",
       "      <td>0.905441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>0.501018</td>\n",
       "      <td>0.909544</td>\n",
       "      <td>0.909170</td>\n",
       "      <td>0.906050</td>\n",
       "      <td>0.905728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>0.453441</td>\n",
       "      <td>0.914348</td>\n",
       "      <td>0.911498</td>\n",
       "      <td>0.910554</td>\n",
       "      <td>0.909708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>0.464796</td>\n",
       "      <td>0.914406</td>\n",
       "      <td>0.911302</td>\n",
       "      <td>0.910859</td>\n",
       "      <td>0.909769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.484692</td>\n",
       "      <td>0.912942</td>\n",
       "      <td>0.911121</td>\n",
       "      <td>0.909369</td>\n",
       "      <td>0.908680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.506322</td>\n",
       "      <td>0.913410</td>\n",
       "      <td>0.911976</td>\n",
       "      <td>0.909161</td>\n",
       "      <td>0.909361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.502334</td>\n",
       "      <td>0.913645</td>\n",
       "      <td>0.911460</td>\n",
       "      <td>0.910371</td>\n",
       "      <td>0.909294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.506936</td>\n",
       "      <td>0.912473</td>\n",
       "      <td>0.911208</td>\n",
       "      <td>0.908203</td>\n",
       "      <td>0.908367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.483521</td>\n",
       "      <td>0.915461</td>\n",
       "      <td>0.913637</td>\n",
       "      <td>0.911096</td>\n",
       "      <td>0.911371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.475356</td>\n",
       "      <td>0.915226</td>\n",
       "      <td>0.913135</td>\n",
       "      <td>0.910885</td>\n",
       "      <td>0.910997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.477286</td>\n",
       "      <td>0.915109</td>\n",
       "      <td>0.913096</td>\n",
       "      <td>0.910667</td>\n",
       "      <td>0.910900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:37:54,682] Trial 4 finished with value: 3.6497724435794066 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}. Best is trial 1 with value: 3.658646782667355.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='5668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/5668 00:56 < 25:58, 3.51 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.072086</td>\n",
       "      <td>0.448943</td>\n",
       "      <td>0.457304</td>\n",
       "      <td>0.403392</td>\n",
       "      <td>0.356058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 15:38:52,129] Trial 5 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='2834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/2834 00:56 < 12:33, 3.50 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.919595</td>\n",
       "      <td>0.606011</td>\n",
       "      <td>0.578329</td>\n",
       "      <td>0.429888</td>\n",
       "      <td>0.476465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 15:39:49,727] Trial 6 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='2834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 600/2834 03:56 < 14:43, 2.53 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.813023</td>\n",
       "      <td>0.705900</td>\n",
       "      <td>0.705350</td>\n",
       "      <td>0.795207</td>\n",
       "      <td>0.686981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.375171</td>\n",
       "      <td>0.870584</td>\n",
       "      <td>0.870604</td>\n",
       "      <td>0.863855</td>\n",
       "      <td>0.865782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.517400</td>\n",
       "      <td>0.351520</td>\n",
       "      <td>0.875212</td>\n",
       "      <td>0.869923</td>\n",
       "      <td>0.876546</td>\n",
       "      <td>0.866983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:43:47,176] Trial 7 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='5668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/5668 01:54 < 25:12, 3.48 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.754960</td>\n",
       "      <td>0.634952</td>\n",
       "      <td>0.603868</td>\n",
       "      <td>0.705535</td>\n",
       "      <td>0.501305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.510898</td>\n",
       "      <td>0.803914</td>\n",
       "      <td>0.793435</td>\n",
       "      <td>0.829767</td>\n",
       "      <td>0.783594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:45:42,439] Trial 8 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='1417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/1417 09:14 < 00:06, 2.52 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.716896</td>\n",
       "      <td>0.687972</td>\n",
       "      <td>0.667642</td>\n",
       "      <td>0.787427</td>\n",
       "      <td>0.616972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.377311</td>\n",
       "      <td>0.861386</td>\n",
       "      <td>0.858889</td>\n",
       "      <td>0.856307</td>\n",
       "      <td>0.854226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.552400</td>\n",
       "      <td>0.335807</td>\n",
       "      <td>0.878376</td>\n",
       "      <td>0.872998</td>\n",
       "      <td>0.878234</td>\n",
       "      <td>0.870160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.552400</td>\n",
       "      <td>0.339101</td>\n",
       "      <td>0.880427</td>\n",
       "      <td>0.877289</td>\n",
       "      <td>0.878673</td>\n",
       "      <td>0.874102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.283244</td>\n",
       "      <td>0.897885</td>\n",
       "      <td>0.892081</td>\n",
       "      <td>0.893379</td>\n",
       "      <td>0.891712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.325783</td>\n",
       "      <td>0.889976</td>\n",
       "      <td>0.890187</td>\n",
       "      <td>0.885488</td>\n",
       "      <td>0.885494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.295013</td>\n",
       "      <td>0.896596</td>\n",
       "      <td>0.894413</td>\n",
       "      <td>0.891339</td>\n",
       "      <td>0.891438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:54:57,958] Trial 9 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='7085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/7085 02:36 < 43:49, 2.54 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.793861</td>\n",
       "      <td>0.626926</td>\n",
       "      <td>0.596050</td>\n",
       "      <td>0.773903</td>\n",
       "      <td>0.492172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.398127</td>\n",
       "      <td>0.848614</td>\n",
       "      <td>0.841237</td>\n",
       "      <td>0.845938</td>\n",
       "      <td>0.837561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:57:35,886] Trial 10 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='14170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/14170 00:56 < 1:06:49, 3.48 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.133977</td>\n",
       "      <td>0.382096</td>\n",
       "      <td>0.405636</td>\n",
       "      <td>0.393715</td>\n",
       "      <td>0.286959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 15:58:33,713] Trial 11 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='11336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/11336 00:56 < 53:13, 3.49 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.107083</td>\n",
       "      <td>0.387545</td>\n",
       "      <td>0.407514</td>\n",
       "      <td>0.367730</td>\n",
       "      <td>0.295036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:59:31,492] Trial 12 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='11336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/11336 00:56 < 52:45, 3.52 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.945668</td>\n",
       "      <td>0.580643</td>\n",
       "      <td>0.557776</td>\n",
       "      <td>0.753864</td>\n",
       "      <td>0.458678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 16:00:28,758] Trial 13 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='7085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/7085 02:36 < 43:48, 2.54 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.793861</td>\n",
       "      <td>0.626926</td>\n",
       "      <td>0.596050</td>\n",
       "      <td>0.773903</td>\n",
       "      <td>0.492172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.398127</td>\n",
       "      <td>0.848614</td>\n",
       "      <td>0.841237</td>\n",
       "      <td>0.845938</td>\n",
       "      <td>0.837561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 16:03:06,357] Trial 14 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='8502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/8502 00:56 < 39:26, 3.51 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.935640</td>\n",
       "      <td>0.592536</td>\n",
       "      <td>0.568188</td>\n",
       "      <td>0.428723</td>\n",
       "      <td>0.467923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 16:04:03,812] Trial 15 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='11336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/11336 00:56 < 53:10, 3.49 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.107083</td>\n",
       "      <td>0.387545</td>\n",
       "      <td>0.407514</td>\n",
       "      <td>0.367730</td>\n",
       "      <td>0.295036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 16:05:01,540] Trial 16 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='4251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/4251 01:18 < 26:41, 2.53 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.826915</td>\n",
       "      <td>0.613568</td>\n",
       "      <td>0.585120</td>\n",
       "      <td>0.716916</td>\n",
       "      <td>0.483484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 16:06:20,907] Trial 17 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='7085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/7085 02:36 < 43:47, 2.54 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.733386</td>\n",
       "      <td>0.638877</td>\n",
       "      <td>0.610234</td>\n",
       "      <td>0.735968</td>\n",
       "      <td>0.508581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.352569</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>0.853291</td>\n",
       "      <td>0.858876</td>\n",
       "      <td>0.851588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 16:08:58,443] Trial 18 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='11336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/11336 00:56 < 53:08, 3.49 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.072086</td>\n",
       "      <td>0.448943</td>\n",
       "      <td>0.457304</td>\n",
       "      <td>0.403392</td>\n",
       "      <td>0.356058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 16:09:56,131] Trial 19 pruned. \n"
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    n_trials=20, \n",
    "    hp_space=optuna_hp_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a73ebc-2b56-45fd-93e8-c7f8caf940c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='1', objective=3.658646782667355, hyperparameters={'learning_rate': 1e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 5}, run_summary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94567a32-b8bc-47b6-a670-1235f2784399",
   "metadata": {},
   "source": [
    "## Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b313c5e4-bd9f-41d2-a073-223e41db8f9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a XLNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35675' max='35675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35675/35675 4:00:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.100191</td>\n",
       "      <td>0.392710</td>\n",
       "      <td>0.409501</td>\n",
       "      <td>0.496755</td>\n",
       "      <td>0.304353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.980168</td>\n",
       "      <td>0.552989</td>\n",
       "      <td>0.528826</td>\n",
       "      <td>0.564965</td>\n",
       "      <td>0.433166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.979300</td>\n",
       "      <td>0.707610</td>\n",
       "      <td>0.656799</td>\n",
       "      <td>0.611384</td>\n",
       "      <td>0.722983</td>\n",
       "      <td>0.520126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.979300</td>\n",
       "      <td>0.504342</td>\n",
       "      <td>0.792836</td>\n",
       "      <td>0.764480</td>\n",
       "      <td>0.818734</td>\n",
       "      <td>0.754901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.542200</td>\n",
       "      <td>0.407479</td>\n",
       "      <td>0.843714</td>\n",
       "      <td>0.825957</td>\n",
       "      <td>0.847512</td>\n",
       "      <td>0.826394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.542200</td>\n",
       "      <td>0.366818</td>\n",
       "      <td>0.866302</td>\n",
       "      <td>0.859251</td>\n",
       "      <td>0.861783</td>\n",
       "      <td>0.857247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.542200</td>\n",
       "      <td>0.391505</td>\n",
       "      <td>0.861853</td>\n",
       "      <td>0.851124</td>\n",
       "      <td>0.864349</td>\n",
       "      <td>0.849856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.324167</td>\n",
       "      <td>0.886037</td>\n",
       "      <td>0.878445</td>\n",
       "      <td>0.880949</td>\n",
       "      <td>0.878179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.309643</td>\n",
       "      <td>0.890771</td>\n",
       "      <td>0.877427</td>\n",
       "      <td>0.890538</td>\n",
       "      <td>0.880998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.329724</td>\n",
       "      <td>0.888832</td>\n",
       "      <td>0.876832</td>\n",
       "      <td>0.890840</td>\n",
       "      <td>0.878796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.307714</td>\n",
       "      <td>0.889916</td>\n",
       "      <td>0.887801</td>\n",
       "      <td>0.882265</td>\n",
       "      <td>0.884577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.302009</td>\n",
       "      <td>0.895905</td>\n",
       "      <td>0.890530</td>\n",
       "      <td>0.890854</td>\n",
       "      <td>0.889264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>0.312125</td>\n",
       "      <td>0.891056</td>\n",
       "      <td>0.890126</td>\n",
       "      <td>0.884723</td>\n",
       "      <td>0.885978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>0.262184</td>\n",
       "      <td>0.908909</td>\n",
       "      <td>0.902041</td>\n",
       "      <td>0.903769</td>\n",
       "      <td>0.902878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.895505</td>\n",
       "      <td>0.895010</td>\n",
       "      <td>0.889008</td>\n",
       "      <td>0.890869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.273942</td>\n",
       "      <td>0.907540</td>\n",
       "      <td>0.903763</td>\n",
       "      <td>0.903264</td>\n",
       "      <td>0.902159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.290672</td>\n",
       "      <td>0.906514</td>\n",
       "      <td>0.896487</td>\n",
       "      <td>0.905272</td>\n",
       "      <td>0.899141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.305076</td>\n",
       "      <td>0.902977</td>\n",
       "      <td>0.895927</td>\n",
       "      <td>0.899425</td>\n",
       "      <td>0.895852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.300111</td>\n",
       "      <td>0.900183</td>\n",
       "      <td>0.901613</td>\n",
       "      <td>0.894443</td>\n",
       "      <td>0.896137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.292963</td>\n",
       "      <td>0.907312</td>\n",
       "      <td>0.906116</td>\n",
       "      <td>0.902258</td>\n",
       "      <td>0.902792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.262481</td>\n",
       "      <td>0.912104</td>\n",
       "      <td>0.908567</td>\n",
       "      <td>0.906621</td>\n",
       "      <td>0.907164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.255091</td>\n",
       "      <td>0.913415</td>\n",
       "      <td>0.910715</td>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.908606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.199700</td>\n",
       "      <td>0.265586</td>\n",
       "      <td>0.910050</td>\n",
       "      <td>0.909490</td>\n",
       "      <td>0.903071</td>\n",
       "      <td>0.905838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.199700</td>\n",
       "      <td>0.237390</td>\n",
       "      <td>0.919576</td>\n",
       "      <td>0.914543</td>\n",
       "      <td>0.915408</td>\n",
       "      <td>0.914652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.229749</td>\n",
       "      <td>0.919462</td>\n",
       "      <td>0.915303</td>\n",
       "      <td>0.916234</td>\n",
       "      <td>0.914799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.254161</td>\n",
       "      <td>0.912389</td>\n",
       "      <td>0.911112</td>\n",
       "      <td>0.906535</td>\n",
       "      <td>0.908177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.241592</td>\n",
       "      <td>0.920602</td>\n",
       "      <td>0.915797</td>\n",
       "      <td>0.915782</td>\n",
       "      <td>0.915786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.271692</td>\n",
       "      <td>0.904403</td>\n",
       "      <td>0.906100</td>\n",
       "      <td>0.899993</td>\n",
       "      <td>0.900841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.273116</td>\n",
       "      <td>0.906799</td>\n",
       "      <td>0.903686</td>\n",
       "      <td>0.904263</td>\n",
       "      <td>0.901299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.263632</td>\n",
       "      <td>0.913758</td>\n",
       "      <td>0.912581</td>\n",
       "      <td>0.908199</td>\n",
       "      <td>0.909667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.272596</td>\n",
       "      <td>0.913929</td>\n",
       "      <td>0.911671</td>\n",
       "      <td>0.907634</td>\n",
       "      <td>0.909489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.256060</td>\n",
       "      <td>0.914670</td>\n",
       "      <td>0.911499</td>\n",
       "      <td>0.911020</td>\n",
       "      <td>0.909689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>0.278063</td>\n",
       "      <td>0.914043</td>\n",
       "      <td>0.911737</td>\n",
       "      <td>0.908565</td>\n",
       "      <td>0.909719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>0.270460</td>\n",
       "      <td>0.911932</td>\n",
       "      <td>0.909395</td>\n",
       "      <td>0.907996</td>\n",
       "      <td>0.907223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>0.235880</td>\n",
       "      <td>0.918435</td>\n",
       "      <td>0.916829</td>\n",
       "      <td>0.912768</td>\n",
       "      <td>0.914377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>0.255511</td>\n",
       "      <td>0.920317</td>\n",
       "      <td>0.917624</td>\n",
       "      <td>0.915628</td>\n",
       "      <td>0.916053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>0.293482</td>\n",
       "      <td>0.911305</td>\n",
       "      <td>0.912799</td>\n",
       "      <td>0.906479</td>\n",
       "      <td>0.907825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.268528</td>\n",
       "      <td>0.914898</td>\n",
       "      <td>0.912612</td>\n",
       "      <td>0.911089</td>\n",
       "      <td>0.910300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.287340</td>\n",
       "      <td>0.912845</td>\n",
       "      <td>0.913227</td>\n",
       "      <td>0.907212</td>\n",
       "      <td>0.909229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.271391</td>\n",
       "      <td>0.916781</td>\n",
       "      <td>0.916346</td>\n",
       "      <td>0.911155</td>\n",
       "      <td>0.912840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.234423</td>\n",
       "      <td>0.924310</td>\n",
       "      <td>0.919597</td>\n",
       "      <td>0.919777</td>\n",
       "      <td>0.919592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.231352</td>\n",
       "      <td>0.922713</td>\n",
       "      <td>0.919242</td>\n",
       "      <td>0.917918</td>\n",
       "      <td>0.918113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.270090</td>\n",
       "      <td>0.919176</td>\n",
       "      <td>0.916306</td>\n",
       "      <td>0.915676</td>\n",
       "      <td>0.914490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.252633</td>\n",
       "      <td>0.923911</td>\n",
       "      <td>0.919257</td>\n",
       "      <td>0.919918</td>\n",
       "      <td>0.919157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.257449</td>\n",
       "      <td>0.920431</td>\n",
       "      <td>0.914815</td>\n",
       "      <td>0.917703</td>\n",
       "      <td>0.914899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.259001</td>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.916986</td>\n",
       "      <td>0.916329</td>\n",
       "      <td>0.916185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.291206</td>\n",
       "      <td>0.909081</td>\n",
       "      <td>0.910931</td>\n",
       "      <td>0.905067</td>\n",
       "      <td>0.905695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.138700</td>\n",
       "      <td>0.255971</td>\n",
       "      <td>0.920659</td>\n",
       "      <td>0.918545</td>\n",
       "      <td>0.915151</td>\n",
       "      <td>0.916215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.138700</td>\n",
       "      <td>0.240947</td>\n",
       "      <td>0.923511</td>\n",
       "      <td>0.918862</td>\n",
       "      <td>0.919423</td>\n",
       "      <td>0.918643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.347114</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.902935</td>\n",
       "      <td>0.895169</td>\n",
       "      <td>0.896293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.245644</td>\n",
       "      <td>0.922941</td>\n",
       "      <td>0.919950</td>\n",
       "      <td>0.917799</td>\n",
       "      <td>0.918524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.267180</td>\n",
       "      <td>0.916496</td>\n",
       "      <td>0.915542</td>\n",
       "      <td>0.912003</td>\n",
       "      <td>0.912395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.251946</td>\n",
       "      <td>0.926249</td>\n",
       "      <td>0.919424</td>\n",
       "      <td>0.923694</td>\n",
       "      <td>0.920896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.263962</td>\n",
       "      <td>0.919576</td>\n",
       "      <td>0.917974</td>\n",
       "      <td>0.914473</td>\n",
       "      <td>0.915560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.281245</td>\n",
       "      <td>0.914956</td>\n",
       "      <td>0.915112</td>\n",
       "      <td>0.908606</td>\n",
       "      <td>0.911080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.265579</td>\n",
       "      <td>0.923169</td>\n",
       "      <td>0.920211</td>\n",
       "      <td>0.917956</td>\n",
       "      <td>0.918898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.285385</td>\n",
       "      <td>0.916439</td>\n",
       "      <td>0.913203</td>\n",
       "      <td>0.913714</td>\n",
       "      <td>0.911256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>0.255335</td>\n",
       "      <td>0.920146</td>\n",
       "      <td>0.917287</td>\n",
       "      <td>0.915616</td>\n",
       "      <td>0.915465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>0.227240</td>\n",
       "      <td>0.926420</td>\n",
       "      <td>0.922230</td>\n",
       "      <td>0.922395</td>\n",
       "      <td>0.921993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.252808</td>\n",
       "      <td>0.920888</td>\n",
       "      <td>0.918728</td>\n",
       "      <td>0.916129</td>\n",
       "      <td>0.916750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.272616</td>\n",
       "      <td>0.908624</td>\n",
       "      <td>0.912231</td>\n",
       "      <td>0.904434</td>\n",
       "      <td>0.905871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.245691</td>\n",
       "      <td>0.925451</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.920534</td>\n",
       "      <td>0.921492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>0.259372</td>\n",
       "      <td>0.917408</td>\n",
       "      <td>0.916916</td>\n",
       "      <td>0.912532</td>\n",
       "      <td>0.913464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>0.265227</td>\n",
       "      <td>0.922599</td>\n",
       "      <td>0.918812</td>\n",
       "      <td>0.918552</td>\n",
       "      <td>0.917874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.244846</td>\n",
       "      <td>0.923854</td>\n",
       "      <td>0.921174</td>\n",
       "      <td>0.919180</td>\n",
       "      <td>0.919517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.246437</td>\n",
       "      <td>0.922656</td>\n",
       "      <td>0.919310</td>\n",
       "      <td>0.917905</td>\n",
       "      <td>0.918089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.231371</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.923966</td>\n",
       "      <td>0.922417</td>\n",
       "      <td>0.922806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.252544</td>\n",
       "      <td>0.926249</td>\n",
       "      <td>0.923730</td>\n",
       "      <td>0.921351</td>\n",
       "      <td>0.922075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.255851</td>\n",
       "      <td>0.924196</td>\n",
       "      <td>0.922214</td>\n",
       "      <td>0.918617</td>\n",
       "      <td>0.920103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.266303</td>\n",
       "      <td>0.917522</td>\n",
       "      <td>0.915036</td>\n",
       "      <td>0.913753</td>\n",
       "      <td>0.912980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.243818</td>\n",
       "      <td>0.926877</td>\n",
       "      <td>0.922891</td>\n",
       "      <td>0.922905</td>\n",
       "      <td>0.922489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.270782</td>\n",
       "      <td>0.921458</td>\n",
       "      <td>0.919531</td>\n",
       "      <td>0.916067</td>\n",
       "      <td>0.917241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.286379</td>\n",
       "      <td>0.920888</td>\n",
       "      <td>0.918327</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.916353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.299239</td>\n",
       "      <td>0.919062</td>\n",
       "      <td>0.919400</td>\n",
       "      <td>0.913212</td>\n",
       "      <td>0.915402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.283321</td>\n",
       "      <td>0.921971</td>\n",
       "      <td>0.920408</td>\n",
       "      <td>0.916441</td>\n",
       "      <td>0.918203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.278590</td>\n",
       "      <td>0.926648</td>\n",
       "      <td>0.922574</td>\n",
       "      <td>0.923143</td>\n",
       "      <td>0.922195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.295592</td>\n",
       "      <td>0.921800</td>\n",
       "      <td>0.920649</td>\n",
       "      <td>0.916526</td>\n",
       "      <td>0.917914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.278183</td>\n",
       "      <td>0.924310</td>\n",
       "      <td>0.921325</td>\n",
       "      <td>0.919791</td>\n",
       "      <td>0.919957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.274394</td>\n",
       "      <td>0.926249</td>\n",
       "      <td>0.922902</td>\n",
       "      <td>0.921836</td>\n",
       "      <td>0.921931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.285985</td>\n",
       "      <td>0.924880</td>\n",
       "      <td>0.922413</td>\n",
       "      <td>0.919620</td>\n",
       "      <td>0.920547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.250079</td>\n",
       "      <td>0.930527</td>\n",
       "      <td>0.926237</td>\n",
       "      <td>0.926527</td>\n",
       "      <td>0.926285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.268864</td>\n",
       "      <td>0.925679</td>\n",
       "      <td>0.922273</td>\n",
       "      <td>0.921326</td>\n",
       "      <td>0.921314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.279300</td>\n",
       "      <td>0.924082</td>\n",
       "      <td>0.921301</td>\n",
       "      <td>0.919119</td>\n",
       "      <td>0.919619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.285125</td>\n",
       "      <td>0.920945</td>\n",
       "      <td>0.920476</td>\n",
       "      <td>0.915308</td>\n",
       "      <td>0.917273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.287699</td>\n",
       "      <td>0.920374</td>\n",
       "      <td>0.920917</td>\n",
       "      <td>0.915753</td>\n",
       "      <td>0.917143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.291775</td>\n",
       "      <td>0.921971</td>\n",
       "      <td>0.922468</td>\n",
       "      <td>0.915817</td>\n",
       "      <td>0.918568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.278388</td>\n",
       "      <td>0.928645</td>\n",
       "      <td>0.925366</td>\n",
       "      <td>0.923957</td>\n",
       "      <td>0.924564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.285694</td>\n",
       "      <td>0.926021</td>\n",
       "      <td>0.923974</td>\n",
       "      <td>0.921295</td>\n",
       "      <td>0.922128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.271393</td>\n",
       "      <td>0.930527</td>\n",
       "      <td>0.925904</td>\n",
       "      <td>0.926915</td>\n",
       "      <td>0.926140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.311739</td>\n",
       "      <td>0.920773</td>\n",
       "      <td>0.920291</td>\n",
       "      <td>0.916039</td>\n",
       "      <td>0.916919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.285496</td>\n",
       "      <td>0.924025</td>\n",
       "      <td>0.922339</td>\n",
       "      <td>0.919045</td>\n",
       "      <td>0.920114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.281779</td>\n",
       "      <td>0.925451</td>\n",
       "      <td>0.924199</td>\n",
       "      <td>0.920102</td>\n",
       "      <td>0.921736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.303872</td>\n",
       "      <td>0.924196</td>\n",
       "      <td>0.922691</td>\n",
       "      <td>0.919229</td>\n",
       "      <td>0.920193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.271485</td>\n",
       "      <td>0.927504</td>\n",
       "      <td>0.924193</td>\n",
       "      <td>0.922811</td>\n",
       "      <td>0.923193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.283244</td>\n",
       "      <td>0.925165</td>\n",
       "      <td>0.923937</td>\n",
       "      <td>0.920549</td>\n",
       "      <td>0.921433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.267293</td>\n",
       "      <td>0.928474</td>\n",
       "      <td>0.925135</td>\n",
       "      <td>0.923767</td>\n",
       "      <td>0.924355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.273470</td>\n",
       "      <td>0.928759</td>\n",
       "      <td>0.925902</td>\n",
       "      <td>0.923986</td>\n",
       "      <td>0.924612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.249483</td>\n",
       "      <td>0.932752</td>\n",
       "      <td>0.926495</td>\n",
       "      <td>0.930534</td>\n",
       "      <td>0.928273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.257597</td>\n",
       "      <td>0.930356</td>\n",
       "      <td>0.925780</td>\n",
       "      <td>0.926706</td>\n",
       "      <td>0.925951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.273541</td>\n",
       "      <td>0.928987</td>\n",
       "      <td>0.924395</td>\n",
       "      <td>0.925563</td>\n",
       "      <td>0.924571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.286038</td>\n",
       "      <td>0.928816</td>\n",
       "      <td>0.926071</td>\n",
       "      <td>0.924644</td>\n",
       "      <td>0.924944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.276067</td>\n",
       "      <td>0.926021</td>\n",
       "      <td>0.924534</td>\n",
       "      <td>0.921039</td>\n",
       "      <td>0.922127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.098500</td>\n",
       "      <td>0.348361</td>\n",
       "      <td>0.915697</td>\n",
       "      <td>0.917450</td>\n",
       "      <td>0.909823</td>\n",
       "      <td>0.912360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.098500</td>\n",
       "      <td>0.273219</td>\n",
       "      <td>0.930584</td>\n",
       "      <td>0.926318</td>\n",
       "      <td>0.926933</td>\n",
       "      <td>0.926265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.263137</td>\n",
       "      <td>0.929786</td>\n",
       "      <td>0.925235</td>\n",
       "      <td>0.926467</td>\n",
       "      <td>0.925320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.280898</td>\n",
       "      <td>0.928588</td>\n",
       "      <td>0.926491</td>\n",
       "      <td>0.923106</td>\n",
       "      <td>0.924679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.274060</td>\n",
       "      <td>0.928188</td>\n",
       "      <td>0.925344</td>\n",
       "      <td>0.924210</td>\n",
       "      <td>0.924217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.288962</td>\n",
       "      <td>0.930641</td>\n",
       "      <td>0.925573</td>\n",
       "      <td>0.927454</td>\n",
       "      <td>0.926159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.319127</td>\n",
       "      <td>0.924196</td>\n",
       "      <td>0.923524</td>\n",
       "      <td>0.918914</td>\n",
       "      <td>0.920492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.302336</td>\n",
       "      <td>0.929044</td>\n",
       "      <td>0.924205</td>\n",
       "      <td>0.925951</td>\n",
       "      <td>0.924528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.318104</td>\n",
       "      <td>0.924367</td>\n",
       "      <td>0.924005</td>\n",
       "      <td>0.919301</td>\n",
       "      <td>0.920830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.319091</td>\n",
       "      <td>0.926877</td>\n",
       "      <td>0.925029</td>\n",
       "      <td>0.921884</td>\n",
       "      <td>0.922987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.319974</td>\n",
       "      <td>0.926021</td>\n",
       "      <td>0.924203</td>\n",
       "      <td>0.921589</td>\n",
       "      <td>0.922304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.295018</td>\n",
       "      <td>0.931440</td>\n",
       "      <td>0.927666</td>\n",
       "      <td>0.927555</td>\n",
       "      <td>0.927409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.324651</td>\n",
       "      <td>0.928417</td>\n",
       "      <td>0.924270</td>\n",
       "      <td>0.924890</td>\n",
       "      <td>0.924054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.318295</td>\n",
       "      <td>0.930071</td>\n",
       "      <td>0.924497</td>\n",
       "      <td>0.927454</td>\n",
       "      <td>0.925394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.303985</td>\n",
       "      <td>0.928531</td>\n",
       "      <td>0.925489</td>\n",
       "      <td>0.923842</td>\n",
       "      <td>0.924379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.320979</td>\n",
       "      <td>0.926877</td>\n",
       "      <td>0.924969</td>\n",
       "      <td>0.922679</td>\n",
       "      <td>0.923134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.338912</td>\n",
       "      <td>0.923169</td>\n",
       "      <td>0.922608</td>\n",
       "      <td>0.917282</td>\n",
       "      <td>0.919485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.083600</td>\n",
       "      <td>0.316827</td>\n",
       "      <td>0.928474</td>\n",
       "      <td>0.925410</td>\n",
       "      <td>0.924660</td>\n",
       "      <td>0.924278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.083600</td>\n",
       "      <td>0.310457</td>\n",
       "      <td>0.931097</td>\n",
       "      <td>0.927772</td>\n",
       "      <td>0.926604</td>\n",
       "      <td>0.926962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.083600</td>\n",
       "      <td>0.300837</td>\n",
       "      <td>0.930413</td>\n",
       "      <td>0.927591</td>\n",
       "      <td>0.925518</td>\n",
       "      <td>0.926420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.319028</td>\n",
       "      <td>0.926135</td>\n",
       "      <td>0.925336</td>\n",
       "      <td>0.921076</td>\n",
       "      <td>0.922547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.313668</td>\n",
       "      <td>0.928645</td>\n",
       "      <td>0.926563</td>\n",
       "      <td>0.923892</td>\n",
       "      <td>0.924768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>0.330244</td>\n",
       "      <td>0.922713</td>\n",
       "      <td>0.922357</td>\n",
       "      <td>0.917894</td>\n",
       "      <td>0.919069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>0.328083</td>\n",
       "      <td>0.925451</td>\n",
       "      <td>0.924442</td>\n",
       "      <td>0.920801</td>\n",
       "      <td>0.921856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>0.312184</td>\n",
       "      <td>0.926705</td>\n",
       "      <td>0.925755</td>\n",
       "      <td>0.921589</td>\n",
       "      <td>0.923092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.330007</td>\n",
       "      <td>0.925679</td>\n",
       "      <td>0.925266</td>\n",
       "      <td>0.920228</td>\n",
       "      <td>0.922086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.371042</td>\n",
       "      <td>0.918378</td>\n",
       "      <td>0.919826</td>\n",
       "      <td>0.912562</td>\n",
       "      <td>0.914957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.310123</td>\n",
       "      <td>0.927048</td>\n",
       "      <td>0.925776</td>\n",
       "      <td>0.922087</td>\n",
       "      <td>0.923324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.297372</td>\n",
       "      <td>0.931097</td>\n",
       "      <td>0.927978</td>\n",
       "      <td>0.926699</td>\n",
       "      <td>0.927098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.323847</td>\n",
       "      <td>0.927732</td>\n",
       "      <td>0.924326</td>\n",
       "      <td>0.923383</td>\n",
       "      <td>0.923533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>0.326633</td>\n",
       "      <td>0.926249</td>\n",
       "      <td>0.923530</td>\n",
       "      <td>0.921990</td>\n",
       "      <td>0.921920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>0.288623</td>\n",
       "      <td>0.931097</td>\n",
       "      <td>0.926470</td>\n",
       "      <td>0.928043</td>\n",
       "      <td>0.926804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.931211</td>\n",
       "      <td>0.927169</td>\n",
       "      <td>0.927336</td>\n",
       "      <td>0.926984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.319107</td>\n",
       "      <td>0.930755</td>\n",
       "      <td>0.927574</td>\n",
       "      <td>0.927085</td>\n",
       "      <td>0.926769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.308035</td>\n",
       "      <td>0.931725</td>\n",
       "      <td>0.927738</td>\n",
       "      <td>0.928050</td>\n",
       "      <td>0.927599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.342338</td>\n",
       "      <td>0.921914</td>\n",
       "      <td>0.922175</td>\n",
       "      <td>0.916880</td>\n",
       "      <td>0.918486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.282606</td>\n",
       "      <td>0.932181</td>\n",
       "      <td>0.928737</td>\n",
       "      <td>0.928142</td>\n",
       "      <td>0.928153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.306581</td>\n",
       "      <td>0.927846</td>\n",
       "      <td>0.925523</td>\n",
       "      <td>0.923846</td>\n",
       "      <td>0.923917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.300824</td>\n",
       "      <td>0.930869</td>\n",
       "      <td>0.926878</td>\n",
       "      <td>0.927329</td>\n",
       "      <td>0.926569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.310594</td>\n",
       "      <td>0.928531</td>\n",
       "      <td>0.926064</td>\n",
       "      <td>0.924434</td>\n",
       "      <td>0.924565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.305290</td>\n",
       "      <td>0.931040</td>\n",
       "      <td>0.927287</td>\n",
       "      <td>0.926980</td>\n",
       "      <td>0.926750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.324866</td>\n",
       "      <td>0.928702</td>\n",
       "      <td>0.926624</td>\n",
       "      <td>0.923803</td>\n",
       "      <td>0.924720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.326353</td>\n",
       "      <td>0.929500</td>\n",
       "      <td>0.926342</td>\n",
       "      <td>0.925392</td>\n",
       "      <td>0.925284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.337221</td>\n",
       "      <td>0.928759</td>\n",
       "      <td>0.926928</td>\n",
       "      <td>0.923507</td>\n",
       "      <td>0.924988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.348368</td>\n",
       "      <td>0.928816</td>\n",
       "      <td>0.926637</td>\n",
       "      <td>0.924333</td>\n",
       "      <td>0.924889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.345633</td>\n",
       "      <td>0.928759</td>\n",
       "      <td>0.926957</td>\n",
       "      <td>0.924177</td>\n",
       "      <td>0.924973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.331816</td>\n",
       "      <td>0.930470</td>\n",
       "      <td>0.927657</td>\n",
       "      <td>0.926294</td>\n",
       "      <td>0.926521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.928588</td>\n",
       "      <td>0.926696</td>\n",
       "      <td>0.923849</td>\n",
       "      <td>0.924788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.331951</td>\n",
       "      <td>0.930413</td>\n",
       "      <td>0.927389</td>\n",
       "      <td>0.925995</td>\n",
       "      <td>0.926373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.354789</td>\n",
       "      <td>0.927561</td>\n",
       "      <td>0.926154</td>\n",
       "      <td>0.922768</td>\n",
       "      <td>0.923734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.344903</td>\n",
       "      <td>0.928987</td>\n",
       "      <td>0.926621</td>\n",
       "      <td>0.924541</td>\n",
       "      <td>0.925088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.339574</td>\n",
       "      <td>0.929786</td>\n",
       "      <td>0.927292</td>\n",
       "      <td>0.925393</td>\n",
       "      <td>0.925897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.346277</td>\n",
       "      <td>0.928417</td>\n",
       "      <td>0.925435</td>\n",
       "      <td>0.924289</td>\n",
       "      <td>0.924258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.331141</td>\n",
       "      <td>0.931839</td>\n",
       "      <td>0.927004</td>\n",
       "      <td>0.928516</td>\n",
       "      <td>0.927485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.355662</td>\n",
       "      <td>0.928245</td>\n",
       "      <td>0.926370</td>\n",
       "      <td>0.923859</td>\n",
       "      <td>0.924433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.347509</td>\n",
       "      <td>0.929614</td>\n",
       "      <td>0.927410</td>\n",
       "      <td>0.925036</td>\n",
       "      <td>0.925778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.353149</td>\n",
       "      <td>0.927561</td>\n",
       "      <td>0.926100</td>\n",
       "      <td>0.922885</td>\n",
       "      <td>0.923769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.338550</td>\n",
       "      <td>0.929957</td>\n",
       "      <td>0.927629</td>\n",
       "      <td>0.925222</td>\n",
       "      <td>0.926074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.338476</td>\n",
       "      <td>0.929614</td>\n",
       "      <td>0.927231</td>\n",
       "      <td>0.925166</td>\n",
       "      <td>0.925726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.340095</td>\n",
       "      <td>0.929557</td>\n",
       "      <td>0.927768</td>\n",
       "      <td>0.924684</td>\n",
       "      <td>0.925814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.362017</td>\n",
       "      <td>0.925508</td>\n",
       "      <td>0.925512</td>\n",
       "      <td>0.920134</td>\n",
       "      <td>0.922103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.342421</td>\n",
       "      <td>0.930470</td>\n",
       "      <td>0.928696</td>\n",
       "      <td>0.925337</td>\n",
       "      <td>0.926762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.349176</td>\n",
       "      <td>0.930014</td>\n",
       "      <td>0.928320</td>\n",
       "      <td>0.925202</td>\n",
       "      <td>0.926301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.336590</td>\n",
       "      <td>0.931211</td>\n",
       "      <td>0.928639</td>\n",
       "      <td>0.926830</td>\n",
       "      <td>0.927321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.335699</td>\n",
       "      <td>0.931554</td>\n",
       "      <td>0.928610</td>\n",
       "      <td>0.927245</td>\n",
       "      <td>0.927685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.331444</td>\n",
       "      <td>0.932124</td>\n",
       "      <td>0.928731</td>\n",
       "      <td>0.927956</td>\n",
       "      <td>0.928078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.334554</td>\n",
       "      <td>0.931211</td>\n",
       "      <td>0.928482</td>\n",
       "      <td>0.926632</td>\n",
       "      <td>0.927324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.343369</td>\n",
       "      <td>0.930584</td>\n",
       "      <td>0.928558</td>\n",
       "      <td>0.925875</td>\n",
       "      <td>0.926845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.339643</td>\n",
       "      <td>0.931040</td>\n",
       "      <td>0.928531</td>\n",
       "      <td>0.926453</td>\n",
       "      <td>0.927166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.339287</td>\n",
       "      <td>0.931269</td>\n",
       "      <td>0.928722</td>\n",
       "      <td>0.926596</td>\n",
       "      <td>0.927376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.343278</td>\n",
       "      <td>0.931097</td>\n",
       "      <td>0.928512</td>\n",
       "      <td>0.926539</td>\n",
       "      <td>0.927203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.342130</td>\n",
       "      <td>0.931554</td>\n",
       "      <td>0.928887</td>\n",
       "      <td>0.927162</td>\n",
       "      <td>0.927689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.340509</td>\n",
       "      <td>0.931497</td>\n",
       "      <td>0.928739</td>\n",
       "      <td>0.927278</td>\n",
       "      <td>0.927624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.343189</td>\n",
       "      <td>0.930869</td>\n",
       "      <td>0.928403</td>\n",
       "      <td>0.926538</td>\n",
       "      <td>0.927035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.343671</td>\n",
       "      <td>0.930812</td>\n",
       "      <td>0.928386</td>\n",
       "      <td>0.926475</td>\n",
       "      <td>0.926977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.342862</td>\n",
       "      <td>0.930641</td>\n",
       "      <td>0.928179</td>\n",
       "      <td>0.926248</td>\n",
       "      <td>0.926783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35675, training_loss=0.13544854105980814, metrics={'train_runtime': 14443.645, 'train_samples_per_second': 79.028, 'train_steps_per_second': 2.47, 'total_flos': 7.670724643977622e+16, 'train_loss': 0.13544854105980814, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "    \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c033e6c-1341-4ddd-9c81-10f456146e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_store_path.parent / (model_store_path.stem + \".out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fc1ce-86d3-4168-a4a1-71f521fe37a3",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b699f7bb-e4c9-45ad-9fd1-d7425dd6f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7590-31cc-40ad-b212-d0c47500277f",
   "metadata": {},
   "source": [
    "## Test on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdfafa9a-6265-4792-8513-149ab1b0545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"validation\"])\n",
    "micro_val, macro_val = generate_micro_macro_df(data[\"validation\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c16081ff-6aa6-4620-8cdf-50f3e07b73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.94      0.97      0.96      7604\n",
      "        REFUTES       0.94      0.90      0.92      5020\n",
      "       SUPPORTS       0.91      0.91      0.91      4908\n",
      "\n",
      "       accuracy                           0.93     17532\n",
      "      macro avg       0.93      0.93      0.93     17532\n",
      "   weighted avg       0.93      0.93      0.93     17532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=micro_val[\"actual\"], y_pred=micro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b4bf9-0d9a-4143-bc29-4b980f4598f0",
   "metadata": {},
   "source": [
    "## Test on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be065c-4aa2-45b6-8d70-4bd40382b826",
   "metadata": {},
   "source": [
    "### FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebee1beb-fbdc-4169-beef-1fb05b5e35ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"fever_test\"])\n",
    "micro_val, macro_val = generate_micro_macro_df(data[\"fever_test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f275a383-337c-48ce-ab31-7a38cb0cf257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.97      0.98      0.98      6666\n",
      "        REFUTES       0.94      0.89      0.91      4909\n",
      "       SUPPORTS       0.90      0.94      0.92      4631\n",
      "\n",
      "       accuracy                           0.94     16206\n",
      "      macro avg       0.94      0.94      0.94     16206\n",
      "   weighted avg       0.94      0.94      0.94     16206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=micro_val[\"actual\"], y_pred=micro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf531418-5f83-419c-ae50-00bca4dbbee2",
   "metadata": {},
   "source": [
    "### Climate-FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aa5d12b-765d-4c45-86f0-f02528b8cbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"climatefever_test\"])\n",
    "micro_val, macro_val = generate_micro_macro_df(data[\"climatefever_test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bc7c6c3-965d-4a9f-b5f6-1d6dc6dc4505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.60      0.92      0.73       235\n",
      "        REFUTES       0.77      0.35      0.49        48\n",
      "       SUPPORTS       0.81      0.36      0.50       176\n",
      "\n",
      "       accuracy                           0.65       459\n",
      "      macro avg       0.73      0.55      0.57       459\n",
      "   weighted avg       0.70      0.65      0.62       459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=micro_val[\"actual\"], y_pred=micro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae0ef8-63c5-4092-b9a3-1528fe23a2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
