{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402ecfd6-f01c-43ef-aaa4-018f9dc63dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/gen\")\n",
    "sys.path.insert(1, \"../src/rte\")\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from util import read_data, write_jsonl\n",
    "from aggregate import generate_micro_macro_df, generate_doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1929c84-28f4-4949-b880-a85b8f8b8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "LOOKUP = {\n",
    "    \"verifiable\": {\"no\": \"NOT VERIFIABLE\", \"yes\": \"VERIFIABLE\"},\n",
    "    \"label\": {\"nei\": \"NOT ENOUGH INFO\", \"r\": \"REFUTES\", \"s\": \"SUPPORTS\"}\n",
    "}\n",
    "\n",
    "SEED = 123456789\n",
    "\n",
    "LABEL2ID = {\"SUPPORTS\": 0, \"NOT ENOUGH INFO\": 1, \"REFUTES\": 2}\n",
    "ID2LABEL = {0: \"SUPPORTS\", 1: \"NOT ENOUGH INFO\", 2: \"REFUTES\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05b8ab-6775-4eed-98be-10904943ded1",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aacd87-5b38-45b8-8be3-f7046bfa06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    TextClassificationPipeline,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd986a-1e45-4f38-8487-dddf2ee3ef24",
   "metadata": {},
   "source": [
    "# Huggingface Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b42b8e-2eb3-444b-9275-192519540439",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e5281d-7e7d-410e-b1ba-730e334247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2c86dd-cb1b-4b77-836d-1e5619f4bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        num_labels=3, \n",
    "        id2label=ID2LABEL, \n",
    "        label2id=LABEL2ID\n",
    "    )\n",
    "\n",
    "model = model_init()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, do_lower_case=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"evidence\"], examples[\"claim\"], max_length=512, truncation=\"only_first\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4834e0-770d-49dc-8041-f4d9d2729053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"fever\", \"climatefever\", \"fever-climatefever\"]\n",
    "task = [\"hp_tune\", \"output\"]\n",
    "doc_sent = [\"doc\", \"sent\"]\n",
    "\n",
    "di = 2\n",
    "ti = 0\n",
    "ds = 0\n",
    "\n",
    "model_store_path = Path(\"/users/k21190024/study/fact-check-transfer-learning/scratch/thesis/models\").joinpath(model_checkpoint)\n",
    "model_store_path.mkdir(exist_ok=True)\n",
    "model_store_path = model_store_path / f\"{dataset[di]}-{model_checkpoint}-{doc_sent[ds]}-{task[ti]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940210d7-1da5-4e6f-b4f6-95304ceb496b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a618a5-0c63-4a99-b7cb-89deea2416f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/146415 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10277 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datap = Path(f\"/users/k21190024/study/fact-check-transfer-learning/scratch/dumps/bert-data-{doc_sent[ds]}-evidence\")\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.train.n5.jsonl\")),\n",
    "    \"validation\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.dev.n5.jsonl\")),\n",
    "    \"fever_test\": Dataset.from_list(read_data(datap / f\"{dataset[0]}.test.n5.jsonl\")),\n",
    "    \"climatefever_test\": Dataset.from_list(read_data(datap / f\"{dataset[1]}.test.n5.jsonl\"))\n",
    "}).map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d7f9a-4ddc-4a9b-b6cf-d31233fff94b",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06cfa045-016f-44e2-91c9-567ddd56ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "epoch = 4\n",
    "metric_name = \"f1\"\n",
    "warmup_ratio=0.1\n",
    "save_steps=200\n",
    "eval_steps=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666373d-06fa-476c-9aaf-ffb3daadbf4b",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967aecb0-5bc7-488c-aefa-caf4525216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shard the data if the dataset is large for hyperparameter tuning\n",
    "shard = data[\"train\"].num_rows > 50000\n",
    "hp_tune_train = data[\"train\"].shuffle(seed=SEED).shard(num_shards=5, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e23d248-bb05-43da-a056-4e1fcc2a17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy = \"no\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\",\n",
    "    tf32=True\n",
    ")\n",
    "\n",
    "_ = model.train()\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=hp_tune_train if shard else data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5445d33-24fc-4a6d-8ed2-a89b26aa65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-5, 3e-5, 2e-5]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "    }\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d4edde-28e8-4b30-a700-dc3f4a2a2ce9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 13:44:38,036] A new study created in memory with name: no-name-22429e36-ada3-4df8-bb9e-33d8cc219dda\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4580' max='4580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4580/4580 12:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.590838</td>\n",
       "      <td>0.653693</td>\n",
       "      <td>0.649690</td>\n",
       "      <td>0.824519</td>\n",
       "      <td>0.544652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.315366</td>\n",
       "      <td>0.892284</td>\n",
       "      <td>0.891552</td>\n",
       "      <td>0.900270</td>\n",
       "      <td>0.891459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.278977</td>\n",
       "      <td>0.899290</td>\n",
       "      <td>0.898646</td>\n",
       "      <td>0.905239</td>\n",
       "      <td>0.898105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.281363</td>\n",
       "      <td>0.903766</td>\n",
       "      <td>0.902989</td>\n",
       "      <td>0.914432</td>\n",
       "      <td>0.903070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.212600</td>\n",
       "      <td>0.318529</td>\n",
       "      <td>0.903960</td>\n",
       "      <td>0.903366</td>\n",
       "      <td>0.909012</td>\n",
       "      <td>0.903025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.212600</td>\n",
       "      <td>0.249680</td>\n",
       "      <td>0.916513</td>\n",
       "      <td>0.915994</td>\n",
       "      <td>0.921238</td>\n",
       "      <td>0.916243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.212600</td>\n",
       "      <td>0.271009</td>\n",
       "      <td>0.911647</td>\n",
       "      <td>0.910966</td>\n",
       "      <td>0.919599</td>\n",
       "      <td>0.911099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.246249</td>\n",
       "      <td>0.922156</td>\n",
       "      <td>0.921712</td>\n",
       "      <td>0.925279</td>\n",
       "      <td>0.921676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.220190</td>\n",
       "      <td>0.924394</td>\n",
       "      <td>0.924152</td>\n",
       "      <td>0.925721</td>\n",
       "      <td>0.924473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.314358</td>\n",
       "      <td>0.921378</td>\n",
       "      <td>0.920906</td>\n",
       "      <td>0.925193</td>\n",
       "      <td>0.921115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.311419</td>\n",
       "      <td>0.924394</td>\n",
       "      <td>0.924050</td>\n",
       "      <td>0.926957</td>\n",
       "      <td>0.924480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.336916</td>\n",
       "      <td>0.922837</td>\n",
       "      <td>0.922492</td>\n",
       "      <td>0.925623</td>\n",
       "      <td>0.922981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.307687</td>\n",
       "      <td>0.920599</td>\n",
       "      <td>0.920140</td>\n",
       "      <td>0.924771</td>\n",
       "      <td>0.920531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.351003</td>\n",
       "      <td>0.920307</td>\n",
       "      <td>0.919855</td>\n",
       "      <td>0.924767</td>\n",
       "      <td>0.920374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.357331</td>\n",
       "      <td>0.925854</td>\n",
       "      <td>0.925482</td>\n",
       "      <td>0.928441</td>\n",
       "      <td>0.925783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.353869</td>\n",
       "      <td>0.925465</td>\n",
       "      <td>0.925017</td>\n",
       "      <td>0.929255</td>\n",
       "      <td>0.925371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.371203</td>\n",
       "      <td>0.924978</td>\n",
       "      <td>0.924558</td>\n",
       "      <td>0.928650</td>\n",
       "      <td>0.925020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.331236</td>\n",
       "      <td>0.931984</td>\n",
       "      <td>0.931707</td>\n",
       "      <td>0.933539</td>\n",
       "      <td>0.931950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.390118</td>\n",
       "      <td>0.927508</td>\n",
       "      <td>0.927153</td>\n",
       "      <td>0.929894</td>\n",
       "      <td>0.927415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.410964</td>\n",
       "      <td>0.926146</td>\n",
       "      <td>0.925776</td>\n",
       "      <td>0.928991</td>\n",
       "      <td>0.926138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.436750</td>\n",
       "      <td>0.923713</td>\n",
       "      <td>0.923265</td>\n",
       "      <td>0.927410</td>\n",
       "      <td>0.923495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.410323</td>\n",
       "      <td>0.927508</td>\n",
       "      <td>0.927188</td>\n",
       "      <td>0.929542</td>\n",
       "      <td>0.927458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 13:57:21,457] Trial 0 finished with value: 3.7116962953643977 and parameters: {'learning_rate': 5e-05, 'num_train_epochs': 5}. Best is trial 0 with value: 3.7116962953643977.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4580' max='4580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4580/4580 12:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.667036</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.647167</td>\n",
       "      <td>0.486575</td>\n",
       "      <td>0.539921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.494824</td>\n",
       "      <td>0.794493</td>\n",
       "      <td>0.792380</td>\n",
       "      <td>0.838563</td>\n",
       "      <td>0.779226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.528100</td>\n",
       "      <td>0.267156</td>\n",
       "      <td>0.898900</td>\n",
       "      <td>0.898392</td>\n",
       "      <td>0.901689</td>\n",
       "      <td>0.898118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.528100</td>\n",
       "      <td>0.275070</td>\n",
       "      <td>0.905517</td>\n",
       "      <td>0.904841</td>\n",
       "      <td>0.913710</td>\n",
       "      <td>0.905153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.302457</td>\n",
       "      <td>0.906490</td>\n",
       "      <td>0.905840</td>\n",
       "      <td>0.911936</td>\n",
       "      <td>0.905389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.253134</td>\n",
       "      <td>0.918264</td>\n",
       "      <td>0.917737</td>\n",
       "      <td>0.923638</td>\n",
       "      <td>0.918249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.241218</td>\n",
       "      <td>0.918167</td>\n",
       "      <td>0.917681</td>\n",
       "      <td>0.922086</td>\n",
       "      <td>0.917947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.245108</td>\n",
       "      <td>0.917778</td>\n",
       "      <td>0.917259</td>\n",
       "      <td>0.922570</td>\n",
       "      <td>0.917654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.211130</td>\n",
       "      <td>0.928968</td>\n",
       "      <td>0.928583</td>\n",
       "      <td>0.932085</td>\n",
       "      <td>0.928981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.255262</td>\n",
       "      <td>0.928189</td>\n",
       "      <td>0.927842</td>\n",
       "      <td>0.930780</td>\n",
       "      <td>0.928281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.336678</td>\n",
       "      <td>0.918167</td>\n",
       "      <td>0.917620</td>\n",
       "      <td>0.924235</td>\n",
       "      <td>0.918236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.277704</td>\n",
       "      <td>0.929746</td>\n",
       "      <td>0.929497</td>\n",
       "      <td>0.931399</td>\n",
       "      <td>0.929898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.085800</td>\n",
       "      <td>0.268533</td>\n",
       "      <td>0.927411</td>\n",
       "      <td>0.927027</td>\n",
       "      <td>0.930274</td>\n",
       "      <td>0.927382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.085800</td>\n",
       "      <td>0.286390</td>\n",
       "      <td>0.928384</td>\n",
       "      <td>0.928008</td>\n",
       "      <td>0.931037</td>\n",
       "      <td>0.928270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.297194</td>\n",
       "      <td>0.931303</td>\n",
       "      <td>0.931074</td>\n",
       "      <td>0.932388</td>\n",
       "      <td>0.931310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.296751</td>\n",
       "      <td>0.928481</td>\n",
       "      <td>0.928185</td>\n",
       "      <td>0.930205</td>\n",
       "      <td>0.928488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.309556</td>\n",
       "      <td>0.928870</td>\n",
       "      <td>0.928639</td>\n",
       "      <td>0.930088</td>\n",
       "      <td>0.928968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.308423</td>\n",
       "      <td>0.929649</td>\n",
       "      <td>0.929337</td>\n",
       "      <td>0.931575</td>\n",
       "      <td>0.929652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.336124</td>\n",
       "      <td>0.926340</td>\n",
       "      <td>0.925965</td>\n",
       "      <td>0.929165</td>\n",
       "      <td>0.926353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.346748</td>\n",
       "      <td>0.927119</td>\n",
       "      <td>0.926739</td>\n",
       "      <td>0.930044</td>\n",
       "      <td>0.927141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.358227</td>\n",
       "      <td>0.927605</td>\n",
       "      <td>0.927228</td>\n",
       "      <td>0.930567</td>\n",
       "      <td>0.927656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.343160</td>\n",
       "      <td>0.928578</td>\n",
       "      <td>0.928226</td>\n",
       "      <td>0.931215</td>\n",
       "      <td>0.928636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-04 14:10:01,960] Trial 1 finished with value: 3.7166557780025076 and parameters: {'learning_rate': 2e-05, 'num_train_epochs': 5}. Best is trial 1 with value: 3.7166557780025076.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3664' max='3664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3664/3664 10:10, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.452018</td>\n",
       "      <td>0.817651</td>\n",
       "      <td>0.816626</td>\n",
       "      <td>0.824603</td>\n",
       "      <td>0.814178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.316104</td>\n",
       "      <td>0.891797</td>\n",
       "      <td>0.891027</td>\n",
       "      <td>0.899883</td>\n",
       "      <td>0.890459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.294901</td>\n",
       "      <td>0.899290</td>\n",
       "      <td>0.898623</td>\n",
       "      <td>0.905290</td>\n",
       "      <td>0.897676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.247189</td>\n",
       "      <td>0.914469</td>\n",
       "      <td>0.913968</td>\n",
       "      <td>0.919854</td>\n",
       "      <td>0.914515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.258151</td>\n",
       "      <td>0.921183</td>\n",
       "      <td>0.920801</td>\n",
       "      <td>0.922802</td>\n",
       "      <td>0.920617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.274122</td>\n",
       "      <td>0.914469</td>\n",
       "      <td>0.913979</td>\n",
       "      <td>0.919247</td>\n",
       "      <td>0.914412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.298828</td>\n",
       "      <td>0.905128</td>\n",
       "      <td>0.904423</td>\n",
       "      <td>0.915049</td>\n",
       "      <td>0.905041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.243450</td>\n",
       "      <td>0.917194</td>\n",
       "      <td>0.916691</td>\n",
       "      <td>0.922098</td>\n",
       "      <td>0.917094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.234448</td>\n",
       "      <td>0.923616</td>\n",
       "      <td>0.923175</td>\n",
       "      <td>0.927431</td>\n",
       "      <td>0.923464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.280603</td>\n",
       "      <td>0.928870</td>\n",
       "      <td>0.928662</td>\n",
       "      <td>0.930313</td>\n",
       "      <td>0.929099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.318229</td>\n",
       "      <td>0.922156</td>\n",
       "      <td>0.921684</td>\n",
       "      <td>0.926777</td>\n",
       "      <td>0.922199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.325654</td>\n",
       "      <td>0.928092</td>\n",
       "      <td>0.927795</td>\n",
       "      <td>0.930078</td>\n",
       "      <td>0.928186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.262921</td>\n",
       "      <td>0.932373</td>\n",
       "      <td>0.932119</td>\n",
       "      <td>0.933677</td>\n",
       "      <td>0.932358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.316725</td>\n",
       "      <td>0.927508</td>\n",
       "      <td>0.927128</td>\n",
       "      <td>0.930894</td>\n",
       "      <td>0.927659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.306256</td>\n",
       "      <td>0.931498</td>\n",
       "      <td>0.931192</td>\n",
       "      <td>0.933631</td>\n",
       "      <td>0.931568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.313434</td>\n",
       "      <td>0.930427</td>\n",
       "      <td>0.930069</td>\n",
       "      <td>0.933118</td>\n",
       "      <td>0.930439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.315822</td>\n",
       "      <td>0.930914</td>\n",
       "      <td>0.930539</td>\n",
       "      <td>0.933708</td>\n",
       "      <td>0.930856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.314134</td>\n",
       "      <td>0.932373</td>\n",
       "      <td>0.932050</td>\n",
       "      <td>0.934481</td>\n",
       "      <td>0.932337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:20:13,640] Trial 2 finished with value: 3.731241053209169 and parameters: {'learning_rate': 5e-05, 'num_train_epochs': 4}. Best is trial 2 with value: 3.731241053209169.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3664' max='3664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3664/3664 10:11, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.644587</td>\n",
       "      <td>0.651844</td>\n",
       "      <td>0.647833</td>\n",
       "      <td>0.489438</td>\n",
       "      <td>0.541158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.371948</td>\n",
       "      <td>0.870682</td>\n",
       "      <td>0.869553</td>\n",
       "      <td>0.888426</td>\n",
       "      <td>0.868090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.285426</td>\n",
       "      <td>0.899971</td>\n",
       "      <td>0.899369</td>\n",
       "      <td>0.905537</td>\n",
       "      <td>0.899269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.238083</td>\n",
       "      <td>0.914858</td>\n",
       "      <td>0.914318</td>\n",
       "      <td>0.920825</td>\n",
       "      <td>0.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.267234</td>\n",
       "      <td>0.918653</td>\n",
       "      <td>0.918295</td>\n",
       "      <td>0.920514</td>\n",
       "      <td>0.918297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.235929</td>\n",
       "      <td>0.921572</td>\n",
       "      <td>0.921115</td>\n",
       "      <td>0.925935</td>\n",
       "      <td>0.921536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.258967</td>\n",
       "      <td>0.917778</td>\n",
       "      <td>0.917269</td>\n",
       "      <td>0.923302</td>\n",
       "      <td>0.917894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.229437</td>\n",
       "      <td>0.927508</td>\n",
       "      <td>0.927156</td>\n",
       "      <td>0.929766</td>\n",
       "      <td>0.927388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>0.929260</td>\n",
       "      <td>0.928959</td>\n",
       "      <td>0.931092</td>\n",
       "      <td>0.929273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.263190</td>\n",
       "      <td>0.929260</td>\n",
       "      <td>0.928924</td>\n",
       "      <td>0.931748</td>\n",
       "      <td>0.929309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.297339</td>\n",
       "      <td>0.925075</td>\n",
       "      <td>0.924686</td>\n",
       "      <td>0.928322</td>\n",
       "      <td>0.925148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.296762</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.929741</td>\n",
       "      <td>0.932115</td>\n",
       "      <td>0.930130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.283585</td>\n",
       "      <td>0.925854</td>\n",
       "      <td>0.925447</td>\n",
       "      <td>0.928813</td>\n",
       "      <td>0.925680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.320911</td>\n",
       "      <td>0.924686</td>\n",
       "      <td>0.924263</td>\n",
       "      <td>0.928298</td>\n",
       "      <td>0.924663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.340977</td>\n",
       "      <td>0.928676</td>\n",
       "      <td>0.928355</td>\n",
       "      <td>0.930929</td>\n",
       "      <td>0.928714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.363783</td>\n",
       "      <td>0.922837</td>\n",
       "      <td>0.922360</td>\n",
       "      <td>0.927428</td>\n",
       "      <td>0.922799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.363118</td>\n",
       "      <td>0.925367</td>\n",
       "      <td>0.924946</td>\n",
       "      <td>0.928864</td>\n",
       "      <td>0.925306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.356317</td>\n",
       "      <td>0.926048</td>\n",
       "      <td>0.925653</td>\n",
       "      <td>0.929185</td>\n",
       "      <td>0.926011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-04 14:30:26,122] Trial 3 finished with value: 3.7068982070106444 and parameters: {'learning_rate': 3e-05, 'num_train_epochs': 4}. Best is trial 2 with value: 3.731241053209169.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3664' max='3664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3664/3664 10:11, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.652238</td>\n",
       "      <td>0.652622</td>\n",
       "      <td>0.648621</td>\n",
       "      <td>0.487727</td>\n",
       "      <td>0.541195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.372396</td>\n",
       "      <td>0.857254</td>\n",
       "      <td>0.856307</td>\n",
       "      <td>0.861457</td>\n",
       "      <td>0.852995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.257217</td>\n",
       "      <td>0.904447</td>\n",
       "      <td>0.903982</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.903662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.256499</td>\n",
       "      <td>0.907463</td>\n",
       "      <td>0.906826</td>\n",
       "      <td>0.913046</td>\n",
       "      <td>0.906548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>0.293596</td>\n",
       "      <td>0.912329</td>\n",
       "      <td>0.911712</td>\n",
       "      <td>0.917422</td>\n",
       "      <td>0.911355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>0.218867</td>\n",
       "      <td>0.925173</td>\n",
       "      <td>0.924915</td>\n",
       "      <td>0.926922</td>\n",
       "      <td>0.925356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>0.238204</td>\n",
       "      <td>0.919626</td>\n",
       "      <td>0.919095</td>\n",
       "      <td>0.924223</td>\n",
       "      <td>0.919273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.214328</td>\n",
       "      <td>0.928384</td>\n",
       "      <td>0.928083</td>\n",
       "      <td>0.929848</td>\n",
       "      <td>0.928232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.208498</td>\n",
       "      <td>0.929162</td>\n",
       "      <td>0.928876</td>\n",
       "      <td>0.930782</td>\n",
       "      <td>0.929167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.260393</td>\n",
       "      <td>0.927897</td>\n",
       "      <td>0.927513</td>\n",
       "      <td>0.930972</td>\n",
       "      <td>0.927967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.287060</td>\n",
       "      <td>0.923908</td>\n",
       "      <td>0.923463</td>\n",
       "      <td>0.927725</td>\n",
       "      <td>0.923917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.269869</td>\n",
       "      <td>0.929941</td>\n",
       "      <td>0.929601</td>\n",
       "      <td>0.932145</td>\n",
       "      <td>0.929905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.280992</td>\n",
       "      <td>0.926048</td>\n",
       "      <td>0.925609</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>0.925922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.284890</td>\n",
       "      <td>0.930719</td>\n",
       "      <td>0.930367</td>\n",
       "      <td>0.933242</td>\n",
       "      <td>0.930701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.288967</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.929703</td>\n",
       "      <td>0.932397</td>\n",
       "      <td>0.930090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.315468</td>\n",
       "      <td>0.926535</td>\n",
       "      <td>0.926139</td>\n",
       "      <td>0.929733</td>\n",
       "      <td>0.926592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.298323</td>\n",
       "      <td>0.928578</td>\n",
       "      <td>0.928254</td>\n",
       "      <td>0.930737</td>\n",
       "      <td>0.928611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.302379</td>\n",
       "      <td>0.928870</td>\n",
       "      <td>0.928512</td>\n",
       "      <td>0.931388</td>\n",
       "      <td>0.928870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-04 14:40:39,071] Trial 4 finished with value: 3.7176406297132525 and parameters: {'learning_rate': 2e-05, 'num_train_epochs': 4}. Best is trial 2 with value: 3.731241053209169.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1832 05:01 < 00:05, 5.96 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593463</td>\n",
       "      <td>0.653790</td>\n",
       "      <td>0.649777</td>\n",
       "      <td>0.824438</td>\n",
       "      <td>0.543648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.314252</td>\n",
       "      <td>0.884889</td>\n",
       "      <td>0.884076</td>\n",
       "      <td>0.893657</td>\n",
       "      <td>0.883675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.435200</td>\n",
       "      <td>0.272513</td>\n",
       "      <td>0.903863</td>\n",
       "      <td>0.903241</td>\n",
       "      <td>0.909605</td>\n",
       "      <td>0.903307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.435200</td>\n",
       "      <td>0.254756</td>\n",
       "      <td>0.909215</td>\n",
       "      <td>0.908598</td>\n",
       "      <td>0.915702</td>\n",
       "      <td>0.908891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.270581</td>\n",
       "      <td>0.914567</td>\n",
       "      <td>0.914007</td>\n",
       "      <td>0.919142</td>\n",
       "      <td>0.914063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.243753</td>\n",
       "      <td>0.923129</td>\n",
       "      <td>0.922721</td>\n",
       "      <td>0.926442</td>\n",
       "      <td>0.923166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.248324</td>\n",
       "      <td>0.920210</td>\n",
       "      <td>0.919693</td>\n",
       "      <td>0.924848</td>\n",
       "      <td>0.919974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>0.216215</td>\n",
       "      <td>0.927411</td>\n",
       "      <td>0.927121</td>\n",
       "      <td>0.929032</td>\n",
       "      <td>0.927401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>0.228470</td>\n",
       "      <td>0.926048</td>\n",
       "      <td>0.925661</td>\n",
       "      <td>0.928865</td>\n",
       "      <td>0.925984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:45:41,956] Trial 5 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='916' max='916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [916/916 02:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.344190</td>\n",
       "      <td>0.863773</td>\n",
       "      <td>0.863017</td>\n",
       "      <td>0.870363</td>\n",
       "      <td>0.862805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.289511</td>\n",
       "      <td>0.893354</td>\n",
       "      <td>0.892508</td>\n",
       "      <td>0.904893</td>\n",
       "      <td>0.892242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.361700</td>\n",
       "      <td>0.273547</td>\n",
       "      <td>0.900165</td>\n",
       "      <td>0.899377</td>\n",
       "      <td>0.910180</td>\n",
       "      <td>0.899214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.361700</td>\n",
       "      <td>0.230252</td>\n",
       "      <td>0.916999</td>\n",
       "      <td>0.916515</td>\n",
       "      <td>0.921631</td>\n",
       "      <td>0.916920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:48:10,859] Trial 6 finished with value: 3.67206596618286 and parameters: {'learning_rate': 3e-05, 'num_train_epochs': 1}. Best is trial 2 with value: 3.731241053209169.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='1832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 800/1832 02:12 < 02:51, 6.01 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593463</td>\n",
       "      <td>0.653790</td>\n",
       "      <td>0.649777</td>\n",
       "      <td>0.824438</td>\n",
       "      <td>0.543648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.314252</td>\n",
       "      <td>0.884889</td>\n",
       "      <td>0.884076</td>\n",
       "      <td>0.893657</td>\n",
       "      <td>0.883675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.435200</td>\n",
       "      <td>0.272513</td>\n",
       "      <td>0.903863</td>\n",
       "      <td>0.903241</td>\n",
       "      <td>0.909605</td>\n",
       "      <td>0.903307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.435200</td>\n",
       "      <td>0.254756</td>\n",
       "      <td>0.909215</td>\n",
       "      <td>0.908598</td>\n",
       "      <td>0.915702</td>\n",
       "      <td>0.908891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:50:24,871] Trial 7 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='2748' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/2748 00:32 < 07:02, 6.03 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.648853</td>\n",
       "      <td>0.652039</td>\n",
       "      <td>0.648028</td>\n",
       "      <td>0.489253</td>\n",
       "      <td>0.541232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-04 14:50:58,875] Trial 8 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/916 01:39 < 00:52, 6.01 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.442202</td>\n",
       "      <td>0.799455</td>\n",
       "      <td>0.797848</td>\n",
       "      <td>0.821781</td>\n",
       "      <td>0.790572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.327025</td>\n",
       "      <td>0.886445</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>0.898133</td>\n",
       "      <td>0.884845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.396700</td>\n",
       "      <td>0.311826</td>\n",
       "      <td>0.885667</td>\n",
       "      <td>0.884742</td>\n",
       "      <td>0.899731</td>\n",
       "      <td>0.884841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:52:39,375] Trial 9 pruned. \n"
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    n_trials=10, \n",
    "    hp_space=optuna_hp_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a73ebc-2b56-45fd-93e8-c7f8caf940c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='2', objective=3.731241053209169, hyperparameters={'learning_rate': 5e-05, 'num_train_epochs': 4}, run_summary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94567a32-b8bc-47b6-a670-1235f2784399",
   "metadata": {},
   "source": [
    "## Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c5e4-bd9f-41d2-a073-223e41db8f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16001' max='18304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16001/18304 45:39 < 06:34, 5.84 it/s, Epoch 3.50/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.715272</td>\n",
       "      <td>0.644351</td>\n",
       "      <td>0.640364</td>\n",
       "      <td>0.483498</td>\n",
       "      <td>0.534485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.569554</td>\n",
       "      <td>0.662547</td>\n",
       "      <td>0.658635</td>\n",
       "      <td>0.810964</td>\n",
       "      <td>0.561236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.302422</td>\n",
       "      <td>0.883332</td>\n",
       "      <td>0.882668</td>\n",
       "      <td>0.889539</td>\n",
       "      <td>0.882769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.392634</td>\n",
       "      <td>0.863190</td>\n",
       "      <td>0.861843</td>\n",
       "      <td>0.893820</td>\n",
       "      <td>0.860694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.301653</td>\n",
       "      <td>0.894814</td>\n",
       "      <td>0.893829</td>\n",
       "      <td>0.910841</td>\n",
       "      <td>0.893418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.232056</td>\n",
       "      <td>0.920016</td>\n",
       "      <td>0.919696</td>\n",
       "      <td>0.921847</td>\n",
       "      <td>0.919801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.274955</td>\n",
       "      <td>0.909799</td>\n",
       "      <td>0.909217</td>\n",
       "      <td>0.915585</td>\n",
       "      <td>0.909532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.199800</td>\n",
       "      <td>0.278387</td>\n",
       "      <td>0.906101</td>\n",
       "      <td>0.905325</td>\n",
       "      <td>0.915928</td>\n",
       "      <td>0.905203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.199800</td>\n",
       "      <td>0.222763</td>\n",
       "      <td>0.924978</td>\n",
       "      <td>0.924687</td>\n",
       "      <td>0.926709</td>\n",
       "      <td>0.924990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.191300</td>\n",
       "      <td>0.234132</td>\n",
       "      <td>0.916026</td>\n",
       "      <td>0.915420</td>\n",
       "      <td>0.923361</td>\n",
       "      <td>0.915926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.191300</td>\n",
       "      <td>0.199877</td>\n",
       "      <td>0.929162</td>\n",
       "      <td>0.928841</td>\n",
       "      <td>0.931315</td>\n",
       "      <td>0.929146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.191300</td>\n",
       "      <td>0.205899</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.929762</td>\n",
       "      <td>0.931286</td>\n",
       "      <td>0.929873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.212715</td>\n",
       "      <td>0.923519</td>\n",
       "      <td>0.922985</td>\n",
       "      <td>0.928709</td>\n",
       "      <td>0.923255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.221306</td>\n",
       "      <td>0.928286</td>\n",
       "      <td>0.927963</td>\n",
       "      <td>0.930827</td>\n",
       "      <td>0.928470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.219705</td>\n",
       "      <td>0.927605</td>\n",
       "      <td>0.927326</td>\n",
       "      <td>0.929488</td>\n",
       "      <td>0.927680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.194407</td>\n",
       "      <td>0.930330</td>\n",
       "      <td>0.929928</td>\n",
       "      <td>0.933566</td>\n",
       "      <td>0.930311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.190403</td>\n",
       "      <td>0.933736</td>\n",
       "      <td>0.933422</td>\n",
       "      <td>0.935152</td>\n",
       "      <td>0.933411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.199578</td>\n",
       "      <td>0.931206</td>\n",
       "      <td>0.930842</td>\n",
       "      <td>0.933889</td>\n",
       "      <td>0.931192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.211022</td>\n",
       "      <td>0.925659</td>\n",
       "      <td>0.925214</td>\n",
       "      <td>0.929153</td>\n",
       "      <td>0.925398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.189203</td>\n",
       "      <td>0.937725</td>\n",
       "      <td>0.937462</td>\n",
       "      <td>0.939156</td>\n",
       "      <td>0.937703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.178799</td>\n",
       "      <td>0.934903</td>\n",
       "      <td>0.934584</td>\n",
       "      <td>0.936245</td>\n",
       "      <td>0.934493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.181886</td>\n",
       "      <td>0.935292</td>\n",
       "      <td>0.935142</td>\n",
       "      <td>0.935522</td>\n",
       "      <td>0.935181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.167400</td>\n",
       "      <td>0.209665</td>\n",
       "      <td>0.936752</td>\n",
       "      <td>0.936496</td>\n",
       "      <td>0.938633</td>\n",
       "      <td>0.936914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.167400</td>\n",
       "      <td>0.188630</td>\n",
       "      <td>0.937238</td>\n",
       "      <td>0.937123</td>\n",
       "      <td>0.937780</td>\n",
       "      <td>0.937349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.217233</td>\n",
       "      <td>0.934222</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>0.937133</td>\n",
       "      <td>0.934164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.222862</td>\n",
       "      <td>0.933152</td>\n",
       "      <td>0.932800</td>\n",
       "      <td>0.935628</td>\n",
       "      <td>0.933119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.228765</td>\n",
       "      <td>0.926340</td>\n",
       "      <td>0.925854</td>\n",
       "      <td>0.931539</td>\n",
       "      <td>0.926386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.227630</td>\n",
       "      <td>0.930914</td>\n",
       "      <td>0.930588</td>\n",
       "      <td>0.932935</td>\n",
       "      <td>0.930926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.204993</td>\n",
       "      <td>0.930524</td>\n",
       "      <td>0.930431</td>\n",
       "      <td>0.931563</td>\n",
       "      <td>0.930852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.243360</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.929763</td>\n",
       "      <td>0.934358</td>\n",
       "      <td>0.929986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.210742</td>\n",
       "      <td>0.933249</td>\n",
       "      <td>0.932943</td>\n",
       "      <td>0.935546</td>\n",
       "      <td>0.933400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.212497</td>\n",
       "      <td>0.937725</td>\n",
       "      <td>0.937517</td>\n",
       "      <td>0.938672</td>\n",
       "      <td>0.937740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.248499</td>\n",
       "      <td>0.935974</td>\n",
       "      <td>0.935777</td>\n",
       "      <td>0.937260</td>\n",
       "      <td>0.936178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.932860</td>\n",
       "      <td>0.932569</td>\n",
       "      <td>0.935086</td>\n",
       "      <td>0.933029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.226948</td>\n",
       "      <td>0.930816</td>\n",
       "      <td>0.930314</td>\n",
       "      <td>0.934991</td>\n",
       "      <td>0.930431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.209682</td>\n",
       "      <td>0.938309</td>\n",
       "      <td>0.938168</td>\n",
       "      <td>0.938816</td>\n",
       "      <td>0.938355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.217792</td>\n",
       "      <td>0.936849</td>\n",
       "      <td>0.936566</td>\n",
       "      <td>0.938068</td>\n",
       "      <td>0.936613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>0.212454</td>\n",
       "      <td>0.937628</td>\n",
       "      <td>0.937261</td>\n",
       "      <td>0.939796</td>\n",
       "      <td>0.937329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>0.198190</td>\n",
       "      <td>0.938114</td>\n",
       "      <td>0.937761</td>\n",
       "      <td>0.940182</td>\n",
       "      <td>0.937788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.178790</td>\n",
       "      <td>0.940158</td>\n",
       "      <td>0.939876</td>\n",
       "      <td>0.942087</td>\n",
       "      <td>0.940218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.213163</td>\n",
       "      <td>0.935974</td>\n",
       "      <td>0.935646</td>\n",
       "      <td>0.938453</td>\n",
       "      <td>0.936077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.201934</td>\n",
       "      <td>0.943271</td>\n",
       "      <td>0.943025</td>\n",
       "      <td>0.944284</td>\n",
       "      <td>0.943156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.190335</td>\n",
       "      <td>0.939087</td>\n",
       "      <td>0.938790</td>\n",
       "      <td>0.940967</td>\n",
       "      <td>0.939078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.224312</td>\n",
       "      <td>0.937336</td>\n",
       "      <td>0.937005</td>\n",
       "      <td>0.940018</td>\n",
       "      <td>0.937462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.180249</td>\n",
       "      <td>0.941715</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>0.943605</td>\n",
       "      <td>0.941628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.239166</td>\n",
       "      <td>0.937336</td>\n",
       "      <td>0.937057</td>\n",
       "      <td>0.938971</td>\n",
       "      <td>0.937299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.211920</td>\n",
       "      <td>0.942979</td>\n",
       "      <td>0.942783</td>\n",
       "      <td>0.943662</td>\n",
       "      <td>0.942892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.232404</td>\n",
       "      <td>0.940936</td>\n",
       "      <td>0.940600</td>\n",
       "      <td>0.943004</td>\n",
       "      <td>0.940756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.212355</td>\n",
       "      <td>0.939866</td>\n",
       "      <td>0.939562</td>\n",
       "      <td>0.941635</td>\n",
       "      <td>0.939782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.213376</td>\n",
       "      <td>0.941520</td>\n",
       "      <td>0.941280</td>\n",
       "      <td>0.942508</td>\n",
       "      <td>0.941402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.234565</td>\n",
       "      <td>0.934514</td>\n",
       "      <td>0.934130</td>\n",
       "      <td>0.937342</td>\n",
       "      <td>0.934447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.245782</td>\n",
       "      <td>0.935390</td>\n",
       "      <td>0.935012</td>\n",
       "      <td>0.938061</td>\n",
       "      <td>0.935259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.086300</td>\n",
       "      <td>0.234224</td>\n",
       "      <td>0.937238</td>\n",
       "      <td>0.936936</td>\n",
       "      <td>0.939386</td>\n",
       "      <td>0.937332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.086300</td>\n",
       "      <td>0.252060</td>\n",
       "      <td>0.936557</td>\n",
       "      <td>0.936172</td>\n",
       "      <td>0.939465</td>\n",
       "      <td>0.936456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.231112</td>\n",
       "      <td>0.940060</td>\n",
       "      <td>0.939822</td>\n",
       "      <td>0.941543</td>\n",
       "      <td>0.940143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.179771</td>\n",
       "      <td>0.943466</td>\n",
       "      <td>0.943256</td>\n",
       "      <td>0.944076</td>\n",
       "      <td>0.943322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.239341</td>\n",
       "      <td>0.940060</td>\n",
       "      <td>0.939741</td>\n",
       "      <td>0.941750</td>\n",
       "      <td>0.939838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>0.251352</td>\n",
       "      <td>0.937725</td>\n",
       "      <td>0.937425</td>\n",
       "      <td>0.939222</td>\n",
       "      <td>0.937565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>0.232927</td>\n",
       "      <td>0.941131</td>\n",
       "      <td>0.940902</td>\n",
       "      <td>0.942622</td>\n",
       "      <td>0.941270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.220828</td>\n",
       "      <td>0.943369</td>\n",
       "      <td>0.943130</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>0.943379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.233261</td>\n",
       "      <td>0.940547</td>\n",
       "      <td>0.940260</td>\n",
       "      <td>0.942259</td>\n",
       "      <td>0.940523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.255712</td>\n",
       "      <td>0.936947</td>\n",
       "      <td>0.936611</td>\n",
       "      <td>0.939771</td>\n",
       "      <td>0.937076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.248409</td>\n",
       "      <td>0.938212</td>\n",
       "      <td>0.937869</td>\n",
       "      <td>0.940767</td>\n",
       "      <td>0.938192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.232492</td>\n",
       "      <td>0.941033</td>\n",
       "      <td>0.940756</td>\n",
       "      <td>0.942969</td>\n",
       "      <td>0.941114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.231532</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>0.937647</td>\n",
       "      <td>0.940893</td>\n",
       "      <td>0.937981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.232394</td>\n",
       "      <td>0.937725</td>\n",
       "      <td>0.937400</td>\n",
       "      <td>0.939724</td>\n",
       "      <td>0.937642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.231108</td>\n",
       "      <td>0.938698</td>\n",
       "      <td>0.938368</td>\n",
       "      <td>0.941118</td>\n",
       "      <td>0.938733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.217389</td>\n",
       "      <td>0.942590</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.944352</td>\n",
       "      <td>0.942603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.242108</td>\n",
       "      <td>0.943661</td>\n",
       "      <td>0.943404</td>\n",
       "      <td>0.944886</td>\n",
       "      <td>0.943586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.264369</td>\n",
       "      <td>0.941617</td>\n",
       "      <td>0.941431</td>\n",
       "      <td>0.942296</td>\n",
       "      <td>0.941601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.258460</td>\n",
       "      <td>0.942882</td>\n",
       "      <td>0.942645</td>\n",
       "      <td>0.944041</td>\n",
       "      <td>0.942867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.273646</td>\n",
       "      <td>0.941617</td>\n",
       "      <td>0.941371</td>\n",
       "      <td>0.943248</td>\n",
       "      <td>0.941731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.257309</td>\n",
       "      <td>0.942882</td>\n",
       "      <td>0.942632</td>\n",
       "      <td>0.944017</td>\n",
       "      <td>0.942803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.263494</td>\n",
       "      <td>0.941423</td>\n",
       "      <td>0.941182</td>\n",
       "      <td>0.942619</td>\n",
       "      <td>0.941417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.257130</td>\n",
       "      <td>0.943855</td>\n",
       "      <td>0.943577</td>\n",
       "      <td>0.945259</td>\n",
       "      <td>0.943737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.260387</td>\n",
       "      <td>0.943369</td>\n",
       "      <td>0.943091</td>\n",
       "      <td>0.944777</td>\n",
       "      <td>0.943254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.262311</td>\n",
       "      <td>0.942104</td>\n",
       "      <td>0.941845</td>\n",
       "      <td>0.943510</td>\n",
       "      <td>0.942081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.942493</td>\n",
       "      <td>0.942209</td>\n",
       "      <td>0.944055</td>\n",
       "      <td>0.942410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.262959</td>\n",
       "      <td>0.944244</td>\n",
       "      <td>0.943979</td>\n",
       "      <td>0.945630</td>\n",
       "      <td>0.944177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 45/322 00:01 < 00:07, 36.48 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\",\n",
    "    tf32=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c033e6c-1341-4ddd-9c81-10f456146e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_store_path.parent / (model_store_path.stem + \".out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fc1ce-86d3-4168-a4a1-71f521fe37a3",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699f7bb-e4c9-45ad-9fd1-d7425dd6f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7590-31cc-40ad-b212-d0c47500277f",
   "metadata": {},
   "source": [
    "## Test on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfafa9a-6265-4792-8513-149ab1b0545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(data[\"validation\"])\n",
    "val = generate_doc_df(data[\"validation\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16081ff-6aa6-4620-8cdf-50f3e07b73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=val[\"actual\"], y_pred=val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d4cf6-3cfe-458f-9ad7-e58994554cbc",
   "metadata": {},
   "source": [
    "## Test on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9714a94-7d27-4b35-95ce-d53a47fc15dd",
   "metadata": {},
   "source": [
    "### FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b87e1-0054-4bbe-a6e8-b76867f03b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(data[\"fever_test\"])\n",
    "fval = generate_doc_df(data[\"fever_test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548fc2f-a81f-4e65-b697-0b9e80ba9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=fval[\"actual\"], y_pred=fval[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d884051-0c43-49e8-bbd7-5db17f9db858",
   "metadata": {},
   "source": [
    "### Climate-FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a4cf4-2ef4-482a-bb2f-dd525be6ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(data[\"climatefever_test\"])\n",
    "cfval = generate_doc_df(data[\"climatefever_test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e65f6-690a-4506-b890-d268cb69d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=cfval[\"actual\"], y_pred=cfval[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae0ef8-63c5-4092-b9a3-1528fe23a2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
