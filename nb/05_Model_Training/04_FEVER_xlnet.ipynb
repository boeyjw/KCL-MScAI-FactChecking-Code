{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402ecfd6-f01c-43ef-aaa4-018f9dc63dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/gen\")\n",
    "sys.path.insert(1, \"../src/rte\")\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from util import read_data, write_jsonl\n",
    "from aggregate import generate_micro_macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1929c84-28f4-4949-b880-a85b8f8b8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "LOOKUP = {\n",
    "    \"verifiable\": {\"no\": \"NOT VERIFIABLE\", \"yes\": \"VERIFIABLE\"},\n",
    "    \"label\": {\"nei\": \"NOT ENOUGH INFO\", \"r\": \"REFUTES\", \"s\": \"SUPPORTS\"}\n",
    "}\n",
    "\n",
    "SEED = 123456789\n",
    "\n",
    "LABEL2ID = {\"SUPPORTS\": 0, \"NOT ENOUGH INFO\": 1, \"REFUTES\": 2}\n",
    "ID2LABEL = {0: \"SUPPORTS\", 1: \"NOT ENOUGH INFO\", 2: \"REFUTES\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05b8ab-6775-4eed-98be-10904943ded1",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aacd87-5b38-45b8-8be3-f7046bfa06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    TextClassificationPipeline,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd986a-1e45-4f38-8487-dddf2ee3ef24",
   "metadata": {},
   "source": [
    "# Huggingface Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b42b8e-2eb3-444b-9275-192519540439",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e5281d-7e7d-410e-b1ba-730e334247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2c86dd-cb1b-4b77-836d-1e5619f4bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"xlnet-base-cased\"\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        num_labels=3, \n",
    "        id2label=ID2LABEL, \n",
    "        label2id=LABEL2ID\n",
    "    )\n",
    "\n",
    "model = model_init()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, do_lower_case=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"evidence\"], examples[\"claim\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4834e0-770d-49dc-8041-f4d9d2729053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"fever\", \"climatefever\", \"fever-climatefever\"]\n",
    "task = [\"hp_tune\", \"output\"]\n",
    "\n",
    "di = 0\n",
    "ti = 0\n",
    "\n",
    "model_store_path = Path(\"/users/k21190024/study/fact-check-transfer-learning/scratch/thesis/models\").joinpath(model_checkpoint)\n",
    "model_store_path.mkdir(exist_ok=True)\n",
    "model_store_path = model_store_path / f\"{dataset[di]}-{model_checkpoint}-{task[ti]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940210d7-1da5-4e6f-b4f6-95304ceb496b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a618a5-0c63-4a99-b7cb-89deea2416f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/223460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datap = Path(\"/users/k21190024/study/fact-check-transfer-learning/scratch/dumps/bert-data-sent-evidence\")\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.train.n5.jsonl\")),\n",
    "    \"validation\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.dev.n5.jsonl\")),\n",
    "    \"test\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.test.n5.jsonl\"))\n",
    "}).map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d7f9a-4ddc-4a9b-b6cf-d31233fff94b",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06cfa045-016f-44e2-91c9-567ddd56ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 4e-4\n",
    "epoch = 4\n",
    "metric_name = \"f1\"\n",
    "warmup_ratio=0.1\n",
    "save_steps=200\n",
    "eval_steps=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666373d-06fa-476c-9aaf-ffb3daadbf4b",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967aecb0-5bc7-488c-aefa-caf4525216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shard the data if the dataset is large for hyperparameter tuning\n",
    "shard = data[\"train\"].num_rows > 50000\n",
    "hp_tune_train = data[\"train\"].shuffle(seed=SEED).shard(num_shards=5, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e23d248-bb05-43da-a056-4e1fcc2a17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy = \"no\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "_ = model.train()\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=hp_tune_train if shard else data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5445d33-24fc-4a6d-8ed2-a89b26aa65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [1e-5, 3e-5, 2e-5]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "    }\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d4edde-28e8-4b30-a700-dc3f4a2a2ce9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 12:57:58,567] A new study created in memory with name: no-name-2bef115b-c1dc-4b25-97b1-f3755592b842\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a XLNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13970' max='13970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13970/13970 1:02:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.104038</td>\n",
       "      <td>0.425536</td>\n",
       "      <td>0.439966</td>\n",
       "      <td>0.367711</td>\n",
       "      <td>0.336744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.873155</td>\n",
       "      <td>0.573349</td>\n",
       "      <td>0.563443</td>\n",
       "      <td>0.604527</td>\n",
       "      <td>0.463396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.927000</td>\n",
       "      <td>0.573731</td>\n",
       "      <td>0.729216</td>\n",
       "      <td>0.708241</td>\n",
       "      <td>0.795350</td>\n",
       "      <td>0.659866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.927000</td>\n",
       "      <td>0.626781</td>\n",
       "      <td>0.793644</td>\n",
       "      <td>0.785391</td>\n",
       "      <td>0.834284</td>\n",
       "      <td>0.770983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.340676</td>\n",
       "      <td>0.878082</td>\n",
       "      <td>0.868632</td>\n",
       "      <td>0.877650</td>\n",
       "      <td>0.866576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.298316</td>\n",
       "      <td>0.898464</td>\n",
       "      <td>0.890141</td>\n",
       "      <td>0.893090</td>\n",
       "      <td>0.890772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.472975</td>\n",
       "      <td>0.860055</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.873206</td>\n",
       "      <td>0.846291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.300246</td>\n",
       "      <td>0.903544</td>\n",
       "      <td>0.899288</td>\n",
       "      <td>0.897161</td>\n",
       "      <td>0.897288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.318785</td>\n",
       "      <td>0.901004</td>\n",
       "      <td>0.892937</td>\n",
       "      <td>0.901905</td>\n",
       "      <td>0.891543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.320226</td>\n",
       "      <td>0.895986</td>\n",
       "      <td>0.897514</td>\n",
       "      <td>0.891704</td>\n",
       "      <td>0.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.916119</td>\n",
       "      <td>0.911207</td>\n",
       "      <td>0.913899</td>\n",
       "      <td>0.909813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.275266</td>\n",
       "      <td>0.917482</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.911036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.237149</td>\n",
       "      <td>0.926279</td>\n",
       "      <td>0.921493</td>\n",
       "      <td>0.921017</td>\n",
       "      <td>0.921184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.252502</td>\n",
       "      <td>0.919589</td>\n",
       "      <td>0.915225</td>\n",
       "      <td>0.916433</td>\n",
       "      <td>0.913316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>0.307527</td>\n",
       "      <td>0.918907</td>\n",
       "      <td>0.913670</td>\n",
       "      <td>0.917479</td>\n",
       "      <td>0.912314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>0.286830</td>\n",
       "      <td>0.925412</td>\n",
       "      <td>0.921310</td>\n",
       "      <td>0.920337</td>\n",
       "      <td>0.919986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>0.295843</td>\n",
       "      <td>0.921261</td>\n",
       "      <td>0.918376</td>\n",
       "      <td>0.916998</td>\n",
       "      <td>0.915798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.206300</td>\n",
       "      <td>0.355929</td>\n",
       "      <td>0.904225</td>\n",
       "      <td>0.907062</td>\n",
       "      <td>0.899176</td>\n",
       "      <td>0.901293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.206300</td>\n",
       "      <td>0.285794</td>\n",
       "      <td>0.929005</td>\n",
       "      <td>0.923331</td>\n",
       "      <td>0.925946</td>\n",
       "      <td>0.923299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.307206</td>\n",
       "      <td>0.919527</td>\n",
       "      <td>0.917680</td>\n",
       "      <td>0.915581</td>\n",
       "      <td>0.914585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.414995</td>\n",
       "      <td>0.902552</td>\n",
       "      <td>0.901982</td>\n",
       "      <td>0.902746</td>\n",
       "      <td>0.897467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>0.920146</td>\n",
       "      <td>0.918318</td>\n",
       "      <td>0.916022</td>\n",
       "      <td>0.915275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.246593</td>\n",
       "      <td>0.931607</td>\n",
       "      <td>0.927241</td>\n",
       "      <td>0.926082</td>\n",
       "      <td>0.926548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.279292</td>\n",
       "      <td>0.922996</td>\n",
       "      <td>0.922854</td>\n",
       "      <td>0.917969</td>\n",
       "      <td>0.919780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>0.328493</td>\n",
       "      <td>0.920704</td>\n",
       "      <td>0.918686</td>\n",
       "      <td>0.917211</td>\n",
       "      <td>0.915473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>0.290840</td>\n",
       "      <td>0.928509</td>\n",
       "      <td>0.927171</td>\n",
       "      <td>0.923051</td>\n",
       "      <td>0.924598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>0.316627</td>\n",
       "      <td>0.923306</td>\n",
       "      <td>0.923418</td>\n",
       "      <td>0.917695</td>\n",
       "      <td>0.919662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.337254</td>\n",
       "      <td>0.921633</td>\n",
       "      <td>0.922122</td>\n",
       "      <td>0.916556</td>\n",
       "      <td>0.917956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.391116</td>\n",
       "      <td>0.913827</td>\n",
       "      <td>0.915785</td>\n",
       "      <td>0.909351</td>\n",
       "      <td>0.910553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.314144</td>\n",
       "      <td>0.932165</td>\n",
       "      <td>0.930053</td>\n",
       "      <td>0.927118</td>\n",
       "      <td>0.927846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.313139</td>\n",
       "      <td>0.932660</td>\n",
       "      <td>0.930474</td>\n",
       "      <td>0.927613</td>\n",
       "      <td>0.928516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.382518</td>\n",
       "      <td>0.919775</td>\n",
       "      <td>0.919250</td>\n",
       "      <td>0.916191</td>\n",
       "      <td>0.915378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.289801</td>\n",
       "      <td>0.932165</td>\n",
       "      <td>0.929225</td>\n",
       "      <td>0.928557</td>\n",
       "      <td>0.927206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.388856</td>\n",
       "      <td>0.916925</td>\n",
       "      <td>0.916296</td>\n",
       "      <td>0.913452</td>\n",
       "      <td>0.912336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.324756</td>\n",
       "      <td>0.927704</td>\n",
       "      <td>0.926413</td>\n",
       "      <td>0.923158</td>\n",
       "      <td>0.923386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.350605</td>\n",
       "      <td>0.928138</td>\n",
       "      <td>0.926139</td>\n",
       "      <td>0.924019</td>\n",
       "      <td>0.923488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.335696</td>\n",
       "      <td>0.928819</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.923690</td>\n",
       "      <td>0.924735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.333765</td>\n",
       "      <td>0.928076</td>\n",
       "      <td>0.925577</td>\n",
       "      <td>0.925145</td>\n",
       "      <td>0.923099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.351787</td>\n",
       "      <td>0.927890</td>\n",
       "      <td>0.926768</td>\n",
       "      <td>0.923234</td>\n",
       "      <td>0.923714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.337071</td>\n",
       "      <td>0.927332</td>\n",
       "      <td>0.926659</td>\n",
       "      <td>0.921873</td>\n",
       "      <td>0.923538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.302468</td>\n",
       "      <td>0.934271</td>\n",
       "      <td>0.931570</td>\n",
       "      <td>0.929620</td>\n",
       "      <td>0.929693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.310642</td>\n",
       "      <td>0.936377</td>\n",
       "      <td>0.933079</td>\n",
       "      <td>0.931982</td>\n",
       "      <td>0.931651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.331273</td>\n",
       "      <td>0.931235</td>\n",
       "      <td>0.928894</td>\n",
       "      <td>0.926384</td>\n",
       "      <td>0.926767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.356604</td>\n",
       "      <td>0.927270</td>\n",
       "      <td>0.925219</td>\n",
       "      <td>0.923521</td>\n",
       "      <td>0.922421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.324578</td>\n",
       "      <td>0.935882</td>\n",
       "      <td>0.932913</td>\n",
       "      <td>0.930597</td>\n",
       "      <td>0.931488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.356783</td>\n",
       "      <td>0.928819</td>\n",
       "      <td>0.927739</td>\n",
       "      <td>0.923438</td>\n",
       "      <td>0.924847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.380054</td>\n",
       "      <td>0.931669</td>\n",
       "      <td>0.929391</td>\n",
       "      <td>0.927112</td>\n",
       "      <td>0.927097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.367087</td>\n",
       "      <td>0.932350</td>\n",
       "      <td>0.930136</td>\n",
       "      <td>0.927956</td>\n",
       "      <td>0.927809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.321811</td>\n",
       "      <td>0.934333</td>\n",
       "      <td>0.932279</td>\n",
       "      <td>0.929534</td>\n",
       "      <td>0.930125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.353844</td>\n",
       "      <td>0.932660</td>\n",
       "      <td>0.930478</td>\n",
       "      <td>0.928067</td>\n",
       "      <td>0.928253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.316862</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.933801</td>\n",
       "      <td>0.931118</td>\n",
       "      <td>0.932071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.369854</td>\n",
       "      <td>0.929315</td>\n",
       "      <td>0.928938</td>\n",
       "      <td>0.923997</td>\n",
       "      <td>0.925600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.111400</td>\n",
       "      <td>0.346485</td>\n",
       "      <td>0.932598</td>\n",
       "      <td>0.930424</td>\n",
       "      <td>0.928146</td>\n",
       "      <td>0.928178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.111400</td>\n",
       "      <td>0.347940</td>\n",
       "      <td>0.931111</td>\n",
       "      <td>0.929128</td>\n",
       "      <td>0.926633</td>\n",
       "      <td>0.926662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.332212</td>\n",
       "      <td>0.934457</td>\n",
       "      <td>0.931952</td>\n",
       "      <td>0.929804</td>\n",
       "      <td>0.929985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.348561</td>\n",
       "      <td>0.932598</td>\n",
       "      <td>0.930310</td>\n",
       "      <td>0.928279</td>\n",
       "      <td>0.928095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.338627</td>\n",
       "      <td>0.933713</td>\n",
       "      <td>0.931844</td>\n",
       "      <td>0.928688</td>\n",
       "      <td>0.929541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.383085</td>\n",
       "      <td>0.931111</td>\n",
       "      <td>0.929504</td>\n",
       "      <td>0.926685</td>\n",
       "      <td>0.926775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.346683</td>\n",
       "      <td>0.936749</td>\n",
       "      <td>0.934439</td>\n",
       "      <td>0.931878</td>\n",
       "      <td>0.932508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.378154</td>\n",
       "      <td>0.932226</td>\n",
       "      <td>0.930404</td>\n",
       "      <td>0.927758</td>\n",
       "      <td>0.927869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.369084</td>\n",
       "      <td>0.934581</td>\n",
       "      <td>0.931854</td>\n",
       "      <td>0.930623</td>\n",
       "      <td>0.929889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.357946</td>\n",
       "      <td>0.937802</td>\n",
       "      <td>0.934971</td>\n",
       "      <td>0.933287</td>\n",
       "      <td>0.933396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.375104</td>\n",
       "      <td>0.933899</td>\n",
       "      <td>0.932098</td>\n",
       "      <td>0.928882</td>\n",
       "      <td>0.929747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.366455</td>\n",
       "      <td>0.935076</td>\n",
       "      <td>0.932734</td>\n",
       "      <td>0.930272</td>\n",
       "      <td>0.930707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.376068</td>\n",
       "      <td>0.933961</td>\n",
       "      <td>0.931778</td>\n",
       "      <td>0.929418</td>\n",
       "      <td>0.929539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.370705</td>\n",
       "      <td>0.933837</td>\n",
       "      <td>0.932152</td>\n",
       "      <td>0.928962</td>\n",
       "      <td>0.929705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.368412</td>\n",
       "      <td>0.934395</td>\n",
       "      <td>0.932411</td>\n",
       "      <td>0.929658</td>\n",
       "      <td>0.930137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.363781</td>\n",
       "      <td>0.934890</td>\n",
       "      <td>0.932669</td>\n",
       "      <td>0.930313</td>\n",
       "      <td>0.930540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.376483</td>\n",
       "      <td>0.932660</td>\n",
       "      <td>0.931069</td>\n",
       "      <td>0.927864</td>\n",
       "      <td>0.928481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 14:00:36,014] Trial 0 finished with value: 3.7200750259754667 and parameters: {'learning_rate': 1e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5}. Best is trial 0 with value: 3.7200750259754667.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4191' max='4191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4191/4191 26:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.628372</td>\n",
       "      <td>0.700161</td>\n",
       "      <td>0.682135</td>\n",
       "      <td>0.730822</td>\n",
       "      <td>0.628490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412479</td>\n",
       "      <td>0.851134</td>\n",
       "      <td>0.850169</td>\n",
       "      <td>0.858349</td>\n",
       "      <td>0.843399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.291437</td>\n",
       "      <td>0.896419</td>\n",
       "      <td>0.892769</td>\n",
       "      <td>0.892628</td>\n",
       "      <td>0.889661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.358873</td>\n",
       "      <td>0.886136</td>\n",
       "      <td>0.886796</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.880672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.297880</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.901587</td>\n",
       "      <td>0.898697</td>\n",
       "      <td>0.896829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.256079</td>\n",
       "      <td>0.918597</td>\n",
       "      <td>0.916510</td>\n",
       "      <td>0.912997</td>\n",
       "      <td>0.913521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.212030</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.924447</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.923925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.275353</td>\n",
       "      <td>0.928695</td>\n",
       "      <td>0.925179</td>\n",
       "      <td>0.923107</td>\n",
       "      <td>0.923539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.261209</td>\n",
       "      <td>0.924545</td>\n",
       "      <td>0.922519</td>\n",
       "      <td>0.919569</td>\n",
       "      <td>0.919763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.265897</td>\n",
       "      <td>0.928819</td>\n",
       "      <td>0.926188</td>\n",
       "      <td>0.924124</td>\n",
       "      <td>0.924029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.344662</td>\n",
       "      <td>0.908747</td>\n",
       "      <td>0.907437</td>\n",
       "      <td>0.907728</td>\n",
       "      <td>0.903349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.919775</td>\n",
       "      <td>0.920227</td>\n",
       "      <td>0.914073</td>\n",
       "      <td>0.916201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.256783</td>\n",
       "      <td>0.931979</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.926777</td>\n",
       "      <td>0.928003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.258609</td>\n",
       "      <td>0.933899</td>\n",
       "      <td>0.930672</td>\n",
       "      <td>0.929948</td>\n",
       "      <td>0.929004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.308286</td>\n",
       "      <td>0.927456</td>\n",
       "      <td>0.927462</td>\n",
       "      <td>0.922179</td>\n",
       "      <td>0.923963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.270852</td>\n",
       "      <td>0.935696</td>\n",
       "      <td>0.933557</td>\n",
       "      <td>0.930955</td>\n",
       "      <td>0.931548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.307238</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.927053</td>\n",
       "      <td>0.925454</td>\n",
       "      <td>0.924682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.294630</td>\n",
       "      <td>0.932226</td>\n",
       "      <td>0.930432</td>\n",
       "      <td>0.927603</td>\n",
       "      <td>0.927982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.268856</td>\n",
       "      <td>0.935572</td>\n",
       "      <td>0.933688</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>0.931568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.265248</td>\n",
       "      <td>0.938050</td>\n",
       "      <td>0.935620</td>\n",
       "      <td>0.933598</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 14:27:03,281] Trial 1 finished with value: 3.741136595545795 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2794' max='2794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2794/2794 12:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.936659</td>\n",
       "      <td>0.578615</td>\n",
       "      <td>0.568511</td>\n",
       "      <td>0.443802</td>\n",
       "      <td>0.464420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.505874</td>\n",
       "      <td>0.833664</td>\n",
       "      <td>0.822162</td>\n",
       "      <td>0.847289</td>\n",
       "      <td>0.813687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.320832</td>\n",
       "      <td>0.888118</td>\n",
       "      <td>0.880775</td>\n",
       "      <td>0.882481</td>\n",
       "      <td>0.879690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.429181</td>\n",
       "      <td>0.867365</td>\n",
       "      <td>0.863590</td>\n",
       "      <td>0.871978</td>\n",
       "      <td>0.858462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.279572</td>\n",
       "      <td>0.904906</td>\n",
       "      <td>0.898293</td>\n",
       "      <td>0.900055</td>\n",
       "      <td>0.897243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.304974</td>\n",
       "      <td>0.908623</td>\n",
       "      <td>0.900092</td>\n",
       "      <td>0.904457</td>\n",
       "      <td>0.901564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.308111</td>\n",
       "      <td>0.900260</td>\n",
       "      <td>0.894686</td>\n",
       "      <td>0.899326</td>\n",
       "      <td>0.892204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.306310</td>\n",
       "      <td>0.906641</td>\n",
       "      <td>0.903065</td>\n",
       "      <td>0.901997</td>\n",
       "      <td>0.900367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.334340</td>\n",
       "      <td>0.894313</td>\n",
       "      <td>0.892072</td>\n",
       "      <td>0.894141</td>\n",
       "      <td>0.887468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.280942</td>\n",
       "      <td>0.912526</td>\n",
       "      <td>0.911056</td>\n",
       "      <td>0.907341</td>\n",
       "      <td>0.907681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.255359</td>\n",
       "      <td>0.919527</td>\n",
       "      <td>0.917254</td>\n",
       "      <td>0.913802</td>\n",
       "      <td>0.915020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.264162</td>\n",
       "      <td>0.919713</td>\n",
       "      <td>0.916069</td>\n",
       "      <td>0.915395</td>\n",
       "      <td>0.914041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>0.266271</td>\n",
       "      <td>0.920394</td>\n",
       "      <td>0.917051</td>\n",
       "      <td>0.916032</td>\n",
       "      <td>0.914927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 14:39:24,170] Trial 2 finished with value: 3.6684040371613156 and parameters: {'learning_rate': 1e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 1}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5588' max='5588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5588/5588 35:40, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.849029</td>\n",
       "      <td>0.616157</td>\n",
       "      <td>0.593408</td>\n",
       "      <td>0.684299</td>\n",
       "      <td>0.488341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.455377</td>\n",
       "      <td>0.816380</td>\n",
       "      <td>0.805563</td>\n",
       "      <td>0.834531</td>\n",
       "      <td>0.794730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.704900</td>\n",
       "      <td>0.279086</td>\n",
       "      <td>0.898773</td>\n",
       "      <td>0.890799</td>\n",
       "      <td>0.893209</td>\n",
       "      <td>0.891043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.704900</td>\n",
       "      <td>0.343158</td>\n",
       "      <td>0.881799</td>\n",
       "      <td>0.883181</td>\n",
       "      <td>0.876809</td>\n",
       "      <td>0.877259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>0.298228</td>\n",
       "      <td>0.900942</td>\n",
       "      <td>0.901366</td>\n",
       "      <td>0.895607</td>\n",
       "      <td>0.896612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>0.283095</td>\n",
       "      <td>0.906022</td>\n",
       "      <td>0.903629</td>\n",
       "      <td>0.901827</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>0.220451</td>\n",
       "      <td>0.925970</td>\n",
       "      <td>0.920422</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.920262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.223800</td>\n",
       "      <td>0.282476</td>\n",
       "      <td>0.917111</td>\n",
       "      <td>0.914276</td>\n",
       "      <td>0.912513</td>\n",
       "      <td>0.911470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.223800</td>\n",
       "      <td>0.244075</td>\n",
       "      <td>0.921633</td>\n",
       "      <td>0.919223</td>\n",
       "      <td>0.916617</td>\n",
       "      <td>0.916486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.190400</td>\n",
       "      <td>0.261310</td>\n",
       "      <td>0.921509</td>\n",
       "      <td>0.918294</td>\n",
       "      <td>0.917931</td>\n",
       "      <td>0.915967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.190400</td>\n",
       "      <td>0.324312</td>\n",
       "      <td>0.910172</td>\n",
       "      <td>0.908240</td>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.904745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.190400</td>\n",
       "      <td>0.236196</td>\n",
       "      <td>0.925660</td>\n",
       "      <td>0.924989</td>\n",
       "      <td>0.919962</td>\n",
       "      <td>0.921965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.224515</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>0.931153</td>\n",
       "      <td>0.928511</td>\n",
       "      <td>0.929488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.241761</td>\n",
       "      <td>0.932103</td>\n",
       "      <td>0.929670</td>\n",
       "      <td>0.927494</td>\n",
       "      <td>0.927653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>0.264484</td>\n",
       "      <td>0.926775</td>\n",
       "      <td>0.925801</td>\n",
       "      <td>0.921617</td>\n",
       "      <td>0.922683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>0.929129</td>\n",
       "      <td>0.926516</td>\n",
       "      <td>0.925192</td>\n",
       "      <td>0.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>0.277736</td>\n",
       "      <td>0.927952</td>\n",
       "      <td>0.925980</td>\n",
       "      <td>0.924122</td>\n",
       "      <td>0.923410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.251971</td>\n",
       "      <td>0.930430</td>\n",
       "      <td>0.928370</td>\n",
       "      <td>0.925806</td>\n",
       "      <td>0.926080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.269072</td>\n",
       "      <td>0.925846</td>\n",
       "      <td>0.923783</td>\n",
       "      <td>0.922499</td>\n",
       "      <td>0.921030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>0.238980</td>\n",
       "      <td>0.935076</td>\n",
       "      <td>0.933171</td>\n",
       "      <td>0.929824</td>\n",
       "      <td>0.931184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>0.260250</td>\n",
       "      <td>0.932474</td>\n",
       "      <td>0.930603</td>\n",
       "      <td>0.927387</td>\n",
       "      <td>0.928262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>0.301238</td>\n",
       "      <td>0.926031</td>\n",
       "      <td>0.925306</td>\n",
       "      <td>0.920915</td>\n",
       "      <td>0.921933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.250455</td>\n",
       "      <td>0.933280</td>\n",
       "      <td>0.930500</td>\n",
       "      <td>0.928160</td>\n",
       "      <td>0.928768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.290996</td>\n",
       "      <td>0.930864</td>\n",
       "      <td>0.929009</td>\n",
       "      <td>0.926384</td>\n",
       "      <td>0.926430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.276444</td>\n",
       "      <td>0.931607</td>\n",
       "      <td>0.930067</td>\n",
       "      <td>0.926576</td>\n",
       "      <td>0.927512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.282371</td>\n",
       "      <td>0.930616</td>\n",
       "      <td>0.929729</td>\n",
       "      <td>0.925554</td>\n",
       "      <td>0.926726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.269419</td>\n",
       "      <td>0.933961</td>\n",
       "      <td>0.931997</td>\n",
       "      <td>0.929341</td>\n",
       "      <td>0.929733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:15:06,179] Trial 3 finished with value: 3.7250316930630176 and parameters: {'learning_rate': 1e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4191' max='4191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4191/4191 26:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.811900</td>\n",
       "      <td>0.619626</td>\n",
       "      <td>0.600362</td>\n",
       "      <td>0.673474</td>\n",
       "      <td>0.495369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412550</td>\n",
       "      <td>0.837567</td>\n",
       "      <td>0.828779</td>\n",
       "      <td>0.850457</td>\n",
       "      <td>0.821045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.666400</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.897658</td>\n",
       "      <td>0.891108</td>\n",
       "      <td>0.892656</td>\n",
       "      <td>0.889826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.666400</td>\n",
       "      <td>0.379457</td>\n",
       "      <td>0.873807</td>\n",
       "      <td>0.875853</td>\n",
       "      <td>0.870841</td>\n",
       "      <td>0.868766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275900</td>\n",
       "      <td>0.299563</td>\n",
       "      <td>0.899455</td>\n",
       "      <td>0.900187</td>\n",
       "      <td>0.894671</td>\n",
       "      <td>0.895033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.275900</td>\n",
       "      <td>0.262573</td>\n",
       "      <td>0.910234</td>\n",
       "      <td>0.907003</td>\n",
       "      <td>0.906133</td>\n",
       "      <td>0.904078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.275900</td>\n",
       "      <td>0.222868</td>\n",
       "      <td>0.925970</td>\n",
       "      <td>0.920313</td>\n",
       "      <td>0.921718</td>\n",
       "      <td>0.920135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>0.255978</td>\n",
       "      <td>0.923368</td>\n",
       "      <td>0.919964</td>\n",
       "      <td>0.917704</td>\n",
       "      <td>0.918036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>0.241767</td>\n",
       "      <td>0.923987</td>\n",
       "      <td>0.920996</td>\n",
       "      <td>0.919046</td>\n",
       "      <td>0.918695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.254324</td>\n",
       "      <td>0.924792</td>\n",
       "      <td>0.921481</td>\n",
       "      <td>0.920622</td>\n",
       "      <td>0.919380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.374006</td>\n",
       "      <td>0.897596</td>\n",
       "      <td>0.897418</td>\n",
       "      <td>0.898087</td>\n",
       "      <td>0.892425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.235473</td>\n",
       "      <td>0.925412</td>\n",
       "      <td>0.924702</td>\n",
       "      <td>0.919609</td>\n",
       "      <td>0.921547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.216611</td>\n",
       "      <td>0.933713</td>\n",
       "      <td>0.930430</td>\n",
       "      <td>0.928608</td>\n",
       "      <td>0.929134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.236689</td>\n",
       "      <td>0.931545</td>\n",
       "      <td>0.928680</td>\n",
       "      <td>0.926998</td>\n",
       "      <td>0.926871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.257754</td>\n",
       "      <td>0.928386</td>\n",
       "      <td>0.926964</td>\n",
       "      <td>0.923555</td>\n",
       "      <td>0.924100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.257459</td>\n",
       "      <td>0.930430</td>\n",
       "      <td>0.928108</td>\n",
       "      <td>0.926400</td>\n",
       "      <td>0.925829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.269161</td>\n",
       "      <td>0.926403</td>\n",
       "      <td>0.924129</td>\n",
       "      <td>0.922933</td>\n",
       "      <td>0.921492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.257795</td>\n",
       "      <td>0.929501</td>\n",
       "      <td>0.927673</td>\n",
       "      <td>0.925054</td>\n",
       "      <td>0.925071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.247612</td>\n",
       "      <td>0.930678</td>\n",
       "      <td>0.928791</td>\n",
       "      <td>0.926331</td>\n",
       "      <td>0.926332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>0.231624</td>\n",
       "      <td>0.935014</td>\n",
       "      <td>0.932385</td>\n",
       "      <td>0.930399</td>\n",
       "      <td>0.930645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:41:46,366] Trial 4 finished with value: 3.728442949962969 and parameters: {'learning_rate': 1e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='6985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/6985 06:31 < 39:07, 2.55 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.762799</td>\n",
       "      <td>0.631954</td>\n",
       "      <td>0.610797</td>\n",
       "      <td>0.664168</td>\n",
       "      <td>0.503767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.368564</td>\n",
       "      <td>0.860302</td>\n",
       "      <td>0.849357</td>\n",
       "      <td>0.862799</td>\n",
       "      <td>0.845825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.642200</td>\n",
       "      <td>0.289155</td>\n",
       "      <td>0.898588</td>\n",
       "      <td>0.890934</td>\n",
       "      <td>0.896129</td>\n",
       "      <td>0.889741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.642200</td>\n",
       "      <td>0.463325</td>\n",
       "      <td>0.854975</td>\n",
       "      <td>0.855867</td>\n",
       "      <td>0.864104</td>\n",
       "      <td>0.847951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.424820</td>\n",
       "      <td>0.872878</td>\n",
       "      <td>0.877307</td>\n",
       "      <td>0.874743</td>\n",
       "      <td>0.869345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:48:18,962] Trial 5 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='6985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/6985 01:17 < 44:16, 2.55 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909115</td>\n",
       "      <td>0.593421</td>\n",
       "      <td>0.573954</td>\n",
       "      <td>0.619689</td>\n",
       "      <td>0.470166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:49:37,652] Trial 6 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='2794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/2794 06:27 < 06:26, 3.61 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.463287</td>\n",
       "      <td>0.812725</td>\n",
       "      <td>0.795276</td>\n",
       "      <td>0.811373</td>\n",
       "      <td>0.789437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.441193</td>\n",
       "      <td>0.850452</td>\n",
       "      <td>0.839572</td>\n",
       "      <td>0.863735</td>\n",
       "      <td>0.831883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.325288</td>\n",
       "      <td>0.899393</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.893789</td>\n",
       "      <td>0.893016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.551261</td>\n",
       "      <td>0.823628</td>\n",
       "      <td>0.819681</td>\n",
       "      <td>0.853737</td>\n",
       "      <td>0.810328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.258313</td>\n",
       "      <td>0.906827</td>\n",
       "      <td>0.897895</td>\n",
       "      <td>0.903244</td>\n",
       "      <td>0.898802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.282593</td>\n",
       "      <td>0.911040</td>\n",
       "      <td>0.905110</td>\n",
       "      <td>0.905627</td>\n",
       "      <td>0.904318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.392520</td>\n",
       "      <td>0.891215</td>\n",
       "      <td>0.886265</td>\n",
       "      <td>0.895372</td>\n",
       "      <td>0.882451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:56:05,798] Trial 7 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='11176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/11176 00:54 < 50:41, 3.61 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.082600</td>\n",
       "      <td>0.458369</td>\n",
       "      <td>0.466106</td>\n",
       "      <td>0.377943</td>\n",
       "      <td>0.366113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 15:57:01,746] Trial 8 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='5588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/5588 00:54 < 24:52, 3.61 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.906309</td>\n",
       "      <td>0.598563</td>\n",
       "      <td>0.581714</td>\n",
       "      <td>0.434990</td>\n",
       "      <td>0.475703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 15:57:57,651] Trial 9 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4191' max='4191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4191/4191 26:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.628372</td>\n",
       "      <td>0.700161</td>\n",
       "      <td>0.682135</td>\n",
       "      <td>0.730822</td>\n",
       "      <td>0.628490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412479</td>\n",
       "      <td>0.851134</td>\n",
       "      <td>0.850169</td>\n",
       "      <td>0.858349</td>\n",
       "      <td>0.843399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.291437</td>\n",
       "      <td>0.896419</td>\n",
       "      <td>0.892769</td>\n",
       "      <td>0.892628</td>\n",
       "      <td>0.889661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.358873</td>\n",
       "      <td>0.886136</td>\n",
       "      <td>0.886796</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.880672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.297880</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.901587</td>\n",
       "      <td>0.898697</td>\n",
       "      <td>0.896829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.256079</td>\n",
       "      <td>0.918597</td>\n",
       "      <td>0.916510</td>\n",
       "      <td>0.912997</td>\n",
       "      <td>0.913521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.212030</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.924447</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.923925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.275353</td>\n",
       "      <td>0.928695</td>\n",
       "      <td>0.925179</td>\n",
       "      <td>0.923107</td>\n",
       "      <td>0.923539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.261209</td>\n",
       "      <td>0.924545</td>\n",
       "      <td>0.922519</td>\n",
       "      <td>0.919569</td>\n",
       "      <td>0.919763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.265897</td>\n",
       "      <td>0.928819</td>\n",
       "      <td>0.926188</td>\n",
       "      <td>0.924124</td>\n",
       "      <td>0.924029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.344662</td>\n",
       "      <td>0.908747</td>\n",
       "      <td>0.907437</td>\n",
       "      <td>0.907728</td>\n",
       "      <td>0.903349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.919775</td>\n",
       "      <td>0.920227</td>\n",
       "      <td>0.914073</td>\n",
       "      <td>0.916201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.256783</td>\n",
       "      <td>0.931979</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.926777</td>\n",
       "      <td>0.928003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.258609</td>\n",
       "      <td>0.933899</td>\n",
       "      <td>0.930672</td>\n",
       "      <td>0.929948</td>\n",
       "      <td>0.929004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.308286</td>\n",
       "      <td>0.927456</td>\n",
       "      <td>0.927462</td>\n",
       "      <td>0.922179</td>\n",
       "      <td>0.923963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.270852</td>\n",
       "      <td>0.935696</td>\n",
       "      <td>0.933557</td>\n",
       "      <td>0.930955</td>\n",
       "      <td>0.931548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.307238</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.927053</td>\n",
       "      <td>0.925454</td>\n",
       "      <td>0.924682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.294630</td>\n",
       "      <td>0.932226</td>\n",
       "      <td>0.930432</td>\n",
       "      <td>0.927603</td>\n",
       "      <td>0.927982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.268856</td>\n",
       "      <td>0.935572</td>\n",
       "      <td>0.933688</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>0.931568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.265248</td>\n",
       "      <td>0.938050</td>\n",
       "      <td>0.935620</td>\n",
       "      <td>0.933598</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 16:24:38,140] Trial 10 finished with value: 3.741136595545795 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4191' max='4191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4191/4191 26:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.628372</td>\n",
       "      <td>0.700161</td>\n",
       "      <td>0.682135</td>\n",
       "      <td>0.730822</td>\n",
       "      <td>0.628490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412479</td>\n",
       "      <td>0.851134</td>\n",
       "      <td>0.850169</td>\n",
       "      <td>0.858349</td>\n",
       "      <td>0.843399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.291437</td>\n",
       "      <td>0.896419</td>\n",
       "      <td>0.892769</td>\n",
       "      <td>0.892628</td>\n",
       "      <td>0.889661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.358873</td>\n",
       "      <td>0.886136</td>\n",
       "      <td>0.886796</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.880672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.297880</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.901587</td>\n",
       "      <td>0.898697</td>\n",
       "      <td>0.896829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.256079</td>\n",
       "      <td>0.918597</td>\n",
       "      <td>0.916510</td>\n",
       "      <td>0.912997</td>\n",
       "      <td>0.913521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.212030</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.924447</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.923925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.275353</td>\n",
       "      <td>0.928695</td>\n",
       "      <td>0.925179</td>\n",
       "      <td>0.923107</td>\n",
       "      <td>0.923539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.261209</td>\n",
       "      <td>0.924545</td>\n",
       "      <td>0.922519</td>\n",
       "      <td>0.919569</td>\n",
       "      <td>0.919763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.265897</td>\n",
       "      <td>0.928819</td>\n",
       "      <td>0.926188</td>\n",
       "      <td>0.924124</td>\n",
       "      <td>0.924029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.344662</td>\n",
       "      <td>0.908747</td>\n",
       "      <td>0.907437</td>\n",
       "      <td>0.907728</td>\n",
       "      <td>0.903349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.919775</td>\n",
       "      <td>0.920227</td>\n",
       "      <td>0.914073</td>\n",
       "      <td>0.916201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.256783</td>\n",
       "      <td>0.931979</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.926777</td>\n",
       "      <td>0.928003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.258609</td>\n",
       "      <td>0.933899</td>\n",
       "      <td>0.930672</td>\n",
       "      <td>0.929948</td>\n",
       "      <td>0.929004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.308286</td>\n",
       "      <td>0.927456</td>\n",
       "      <td>0.927462</td>\n",
       "      <td>0.922179</td>\n",
       "      <td>0.923963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.270852</td>\n",
       "      <td>0.935696</td>\n",
       "      <td>0.933557</td>\n",
       "      <td>0.930955</td>\n",
       "      <td>0.931548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.307238</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.927053</td>\n",
       "      <td>0.925454</td>\n",
       "      <td>0.924682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.294630</td>\n",
       "      <td>0.932226</td>\n",
       "      <td>0.930432</td>\n",
       "      <td>0.927603</td>\n",
       "      <td>0.927982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.268856</td>\n",
       "      <td>0.935572</td>\n",
       "      <td>0.933688</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>0.931568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.265248</td>\n",
       "      <td>0.938050</td>\n",
       "      <td>0.935620</td>\n",
       "      <td>0.933598</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 16:51:18,172] Trial 11 finished with value: 3.741136595545795 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2794' max='2794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2794/2794 17:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.495049</td>\n",
       "      <td>0.804919</td>\n",
       "      <td>0.798151</td>\n",
       "      <td>0.814046</td>\n",
       "      <td>0.786176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.325774</td>\n",
       "      <td>0.886321</td>\n",
       "      <td>0.878810</td>\n",
       "      <td>0.886626</td>\n",
       "      <td>0.876078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.248774</td>\n",
       "      <td>0.910978</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.906365</td>\n",
       "      <td>0.904405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.336135</td>\n",
       "      <td>0.895366</td>\n",
       "      <td>0.896615</td>\n",
       "      <td>0.891996</td>\n",
       "      <td>0.890978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.285281</td>\n",
       "      <td>0.906517</td>\n",
       "      <td>0.904857</td>\n",
       "      <td>0.904625</td>\n",
       "      <td>0.900715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.243058</td>\n",
       "      <td>0.922376</td>\n",
       "      <td>0.917983</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.916314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.215706</td>\n",
       "      <td>0.926713</td>\n",
       "      <td>0.922766</td>\n",
       "      <td>0.922603</td>\n",
       "      <td>0.921320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.257392</td>\n",
       "      <td>0.929563</td>\n",
       "      <td>0.926314</td>\n",
       "      <td>0.924202</td>\n",
       "      <td>0.924685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.242142</td>\n",
       "      <td>0.928448</td>\n",
       "      <td>0.925871</td>\n",
       "      <td>0.923413</td>\n",
       "      <td>0.923681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.240745</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>0.930925</td>\n",
       "      <td>0.928690</td>\n",
       "      <td>0.929324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.317711</td>\n",
       "      <td>0.915004</td>\n",
       "      <td>0.913066</td>\n",
       "      <td>0.912993</td>\n",
       "      <td>0.909537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.241085</td>\n",
       "      <td>0.930926</td>\n",
       "      <td>0.929038</td>\n",
       "      <td>0.926003</td>\n",
       "      <td>0.926624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>0.234821</td>\n",
       "      <td>0.934395</td>\n",
       "      <td>0.931921</td>\n",
       "      <td>0.929644</td>\n",
       "      <td>0.930071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 17:08:54,755] Trial 12 finished with value: 3.7260306427665495 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2794' max='2794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2794/2794 17:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.495049</td>\n",
       "      <td>0.804919</td>\n",
       "      <td>0.798151</td>\n",
       "      <td>0.814046</td>\n",
       "      <td>0.786176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.325774</td>\n",
       "      <td>0.886321</td>\n",
       "      <td>0.878810</td>\n",
       "      <td>0.886626</td>\n",
       "      <td>0.876078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.248774</td>\n",
       "      <td>0.910978</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.906365</td>\n",
       "      <td>0.904405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.336135</td>\n",
       "      <td>0.895366</td>\n",
       "      <td>0.896615</td>\n",
       "      <td>0.891996</td>\n",
       "      <td>0.890978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.285281</td>\n",
       "      <td>0.906517</td>\n",
       "      <td>0.904857</td>\n",
       "      <td>0.904625</td>\n",
       "      <td>0.900715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.243058</td>\n",
       "      <td>0.922376</td>\n",
       "      <td>0.917983</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.916314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.215706</td>\n",
       "      <td>0.926713</td>\n",
       "      <td>0.922766</td>\n",
       "      <td>0.922603</td>\n",
       "      <td>0.921320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.257392</td>\n",
       "      <td>0.929563</td>\n",
       "      <td>0.926314</td>\n",
       "      <td>0.924202</td>\n",
       "      <td>0.924685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.242142</td>\n",
       "      <td>0.928448</td>\n",
       "      <td>0.925871</td>\n",
       "      <td>0.923413</td>\n",
       "      <td>0.923681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.240745</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>0.930925</td>\n",
       "      <td>0.928690</td>\n",
       "      <td>0.929324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.317711</td>\n",
       "      <td>0.915004</td>\n",
       "      <td>0.913066</td>\n",
       "      <td>0.912993</td>\n",
       "      <td>0.909537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.241085</td>\n",
       "      <td>0.930926</td>\n",
       "      <td>0.929038</td>\n",
       "      <td>0.926003</td>\n",
       "      <td>0.926624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>0.234821</td>\n",
       "      <td>0.934395</td>\n",
       "      <td>0.931921</td>\n",
       "      <td>0.929644</td>\n",
       "      <td>0.930071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 17:26:32,412] Trial 13 finished with value: 3.7260306427665495 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='5588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 600/5588 03:55 < 32:45, 2.54 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.524372</td>\n",
       "      <td>0.761925</td>\n",
       "      <td>0.745953</td>\n",
       "      <td>0.787374</td>\n",
       "      <td>0.718256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.372496</td>\n",
       "      <td>0.874303</td>\n",
       "      <td>0.869110</td>\n",
       "      <td>0.876131</td>\n",
       "      <td>0.864705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.563800</td>\n",
       "      <td>0.309601</td>\n",
       "      <td>0.891092</td>\n",
       "      <td>0.888160</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.884320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 17:30:29,239] Trial 14 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4191' max='4191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4191/4191 26:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.628372</td>\n",
       "      <td>0.700161</td>\n",
       "      <td>0.682135</td>\n",
       "      <td>0.730822</td>\n",
       "      <td>0.628490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412479</td>\n",
       "      <td>0.851134</td>\n",
       "      <td>0.850169</td>\n",
       "      <td>0.858349</td>\n",
       "      <td>0.843399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.291437</td>\n",
       "      <td>0.896419</td>\n",
       "      <td>0.892769</td>\n",
       "      <td>0.892628</td>\n",
       "      <td>0.889661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.358873</td>\n",
       "      <td>0.886136</td>\n",
       "      <td>0.886796</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.880672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.297880</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.901587</td>\n",
       "      <td>0.898697</td>\n",
       "      <td>0.896829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.256079</td>\n",
       "      <td>0.918597</td>\n",
       "      <td>0.916510</td>\n",
       "      <td>0.912997</td>\n",
       "      <td>0.913521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.212030</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.924447</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.923925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.275353</td>\n",
       "      <td>0.928695</td>\n",
       "      <td>0.925179</td>\n",
       "      <td>0.923107</td>\n",
       "      <td>0.923539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.261209</td>\n",
       "      <td>0.924545</td>\n",
       "      <td>0.922519</td>\n",
       "      <td>0.919569</td>\n",
       "      <td>0.919763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.265897</td>\n",
       "      <td>0.928819</td>\n",
       "      <td>0.926188</td>\n",
       "      <td>0.924124</td>\n",
       "      <td>0.924029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.344662</td>\n",
       "      <td>0.908747</td>\n",
       "      <td>0.907437</td>\n",
       "      <td>0.907728</td>\n",
       "      <td>0.903349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.919775</td>\n",
       "      <td>0.920227</td>\n",
       "      <td>0.914073</td>\n",
       "      <td>0.916201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.256783</td>\n",
       "      <td>0.931979</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.926777</td>\n",
       "      <td>0.928003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.258609</td>\n",
       "      <td>0.933899</td>\n",
       "      <td>0.930672</td>\n",
       "      <td>0.929948</td>\n",
       "      <td>0.929004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.308286</td>\n",
       "      <td>0.927456</td>\n",
       "      <td>0.927462</td>\n",
       "      <td>0.922179</td>\n",
       "      <td>0.923963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.270852</td>\n",
       "      <td>0.935696</td>\n",
       "      <td>0.933557</td>\n",
       "      <td>0.930955</td>\n",
       "      <td>0.931548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.307238</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.927053</td>\n",
       "      <td>0.925454</td>\n",
       "      <td>0.924682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.294630</td>\n",
       "      <td>0.932226</td>\n",
       "      <td>0.930432</td>\n",
       "      <td>0.927603</td>\n",
       "      <td>0.927982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.268856</td>\n",
       "      <td>0.935572</td>\n",
       "      <td>0.933688</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>0.931568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.265248</td>\n",
       "      <td>0.938050</td>\n",
       "      <td>0.935620</td>\n",
       "      <td>0.933598</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 17:57:09,461] Trial 15 finished with value: 3.741136595545795 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2794' max='2794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2794/2794 17:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.495049</td>\n",
       "      <td>0.804919</td>\n",
       "      <td>0.798151</td>\n",
       "      <td>0.814046</td>\n",
       "      <td>0.786176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.325774</td>\n",
       "      <td>0.886321</td>\n",
       "      <td>0.878810</td>\n",
       "      <td>0.886626</td>\n",
       "      <td>0.876078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.248774</td>\n",
       "      <td>0.910978</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.906365</td>\n",
       "      <td>0.904405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.336135</td>\n",
       "      <td>0.895366</td>\n",
       "      <td>0.896615</td>\n",
       "      <td>0.891996</td>\n",
       "      <td>0.890978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.285281</td>\n",
       "      <td>0.906517</td>\n",
       "      <td>0.904857</td>\n",
       "      <td>0.904625</td>\n",
       "      <td>0.900715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.243058</td>\n",
       "      <td>0.922376</td>\n",
       "      <td>0.917983</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.916314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.215706</td>\n",
       "      <td>0.926713</td>\n",
       "      <td>0.922766</td>\n",
       "      <td>0.922603</td>\n",
       "      <td>0.921320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.257392</td>\n",
       "      <td>0.929563</td>\n",
       "      <td>0.926314</td>\n",
       "      <td>0.924202</td>\n",
       "      <td>0.924685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.242142</td>\n",
       "      <td>0.928448</td>\n",
       "      <td>0.925871</td>\n",
       "      <td>0.923413</td>\n",
       "      <td>0.923681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.240745</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>0.930925</td>\n",
       "      <td>0.928690</td>\n",
       "      <td>0.929324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.317711</td>\n",
       "      <td>0.915004</td>\n",
       "      <td>0.913066</td>\n",
       "      <td>0.912993</td>\n",
       "      <td>0.909537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.241085</td>\n",
       "      <td>0.930926</td>\n",
       "      <td>0.929038</td>\n",
       "      <td>0.926003</td>\n",
       "      <td>0.926624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>0.234821</td>\n",
       "      <td>0.934395</td>\n",
       "      <td>0.931921</td>\n",
       "      <td>0.929644</td>\n",
       "      <td>0.930071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 18:14:46,875] Trial 16 finished with value: 3.7260306427665495 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='5588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 600/5588 03:55 < 32:46, 2.54 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.524372</td>\n",
       "      <td>0.761925</td>\n",
       "      <td>0.745953</td>\n",
       "      <td>0.787374</td>\n",
       "      <td>0.718256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.372496</td>\n",
       "      <td>0.874303</td>\n",
       "      <td>0.869110</td>\n",
       "      <td>0.876131</td>\n",
       "      <td>0.864705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.563800</td>\n",
       "      <td>0.309601</td>\n",
       "      <td>0.891092</td>\n",
       "      <td>0.888160</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.884320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 18:18:43,816] Trial 17 pruned. \n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4191' max='4191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4191/4191 26:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.628372</td>\n",
       "      <td>0.700161</td>\n",
       "      <td>0.682135</td>\n",
       "      <td>0.730822</td>\n",
       "      <td>0.628490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412479</td>\n",
       "      <td>0.851134</td>\n",
       "      <td>0.850169</td>\n",
       "      <td>0.858349</td>\n",
       "      <td>0.843399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.291437</td>\n",
       "      <td>0.896419</td>\n",
       "      <td>0.892769</td>\n",
       "      <td>0.892628</td>\n",
       "      <td>0.889661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.358873</td>\n",
       "      <td>0.886136</td>\n",
       "      <td>0.886796</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.880672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.297880</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.901587</td>\n",
       "      <td>0.898697</td>\n",
       "      <td>0.896829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.256079</td>\n",
       "      <td>0.918597</td>\n",
       "      <td>0.916510</td>\n",
       "      <td>0.912997</td>\n",
       "      <td>0.913521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.212030</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.924447</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.923925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.275353</td>\n",
       "      <td>0.928695</td>\n",
       "      <td>0.925179</td>\n",
       "      <td>0.923107</td>\n",
       "      <td>0.923539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.261209</td>\n",
       "      <td>0.924545</td>\n",
       "      <td>0.922519</td>\n",
       "      <td>0.919569</td>\n",
       "      <td>0.919763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.265897</td>\n",
       "      <td>0.928819</td>\n",
       "      <td>0.926188</td>\n",
       "      <td>0.924124</td>\n",
       "      <td>0.924029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.344662</td>\n",
       "      <td>0.908747</td>\n",
       "      <td>0.907437</td>\n",
       "      <td>0.907728</td>\n",
       "      <td>0.903349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.919775</td>\n",
       "      <td>0.920227</td>\n",
       "      <td>0.914073</td>\n",
       "      <td>0.916201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.256783</td>\n",
       "      <td>0.931979</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.926777</td>\n",
       "      <td>0.928003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.258609</td>\n",
       "      <td>0.933899</td>\n",
       "      <td>0.930672</td>\n",
       "      <td>0.929948</td>\n",
       "      <td>0.929004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.308286</td>\n",
       "      <td>0.927456</td>\n",
       "      <td>0.927462</td>\n",
       "      <td>0.922179</td>\n",
       "      <td>0.923963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.270852</td>\n",
       "      <td>0.935696</td>\n",
       "      <td>0.933557</td>\n",
       "      <td>0.930955</td>\n",
       "      <td>0.931548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.307238</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.927053</td>\n",
       "      <td>0.925454</td>\n",
       "      <td>0.924682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.294630</td>\n",
       "      <td>0.932226</td>\n",
       "      <td>0.930432</td>\n",
       "      <td>0.927603</td>\n",
       "      <td>0.927982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.268856</td>\n",
       "      <td>0.935572</td>\n",
       "      <td>0.933688</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>0.931568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.265248</td>\n",
       "      <td>0.938050</td>\n",
       "      <td>0.935620</td>\n",
       "      <td>0.933598</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 18:45:24,063] Trial 18 finished with value: 3.741136595545795 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 1 with value: 3.741136595545795.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4191' max='4191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4191/4191 26:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.628372</td>\n",
       "      <td>0.700161</td>\n",
       "      <td>0.682135</td>\n",
       "      <td>0.730822</td>\n",
       "      <td>0.628490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412479</td>\n",
       "      <td>0.851134</td>\n",
       "      <td>0.850169</td>\n",
       "      <td>0.858349</td>\n",
       "      <td>0.843399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.291437</td>\n",
       "      <td>0.896419</td>\n",
       "      <td>0.892769</td>\n",
       "      <td>0.892628</td>\n",
       "      <td>0.889661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.358873</td>\n",
       "      <td>0.886136</td>\n",
       "      <td>0.886796</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.880672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.297880</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.901587</td>\n",
       "      <td>0.898697</td>\n",
       "      <td>0.896829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.256079</td>\n",
       "      <td>0.918597</td>\n",
       "      <td>0.916510</td>\n",
       "      <td>0.912997</td>\n",
       "      <td>0.913521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.212030</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.924447</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.923925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.275353</td>\n",
       "      <td>0.928695</td>\n",
       "      <td>0.925179</td>\n",
       "      <td>0.923107</td>\n",
       "      <td>0.923539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.261209</td>\n",
       "      <td>0.924545</td>\n",
       "      <td>0.922519</td>\n",
       "      <td>0.919569</td>\n",
       "      <td>0.919763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.265897</td>\n",
       "      <td>0.928819</td>\n",
       "      <td>0.926188</td>\n",
       "      <td>0.924124</td>\n",
       "      <td>0.924029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.344662</td>\n",
       "      <td>0.908747</td>\n",
       "      <td>0.907437</td>\n",
       "      <td>0.907728</td>\n",
       "      <td>0.903349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.919775</td>\n",
       "      <td>0.920227</td>\n",
       "      <td>0.914073</td>\n",
       "      <td>0.916201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.256783</td>\n",
       "      <td>0.931979</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.926777</td>\n",
       "      <td>0.928003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.258609</td>\n",
       "      <td>0.933899</td>\n",
       "      <td>0.930672</td>\n",
       "      <td>0.929948</td>\n",
       "      <td>0.929004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.308286</td>\n",
       "      <td>0.927456</td>\n",
       "      <td>0.927462</td>\n",
       "      <td>0.922179</td>\n",
       "      <td>0.923963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.270852</td>\n",
       "      <td>0.935696</td>\n",
       "      <td>0.933557</td>\n",
       "      <td>0.930955</td>\n",
       "      <td>0.931548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.307238</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>0.927053</td>\n",
       "      <td>0.925454</td>\n",
       "      <td>0.924682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.294630</td>\n",
       "      <td>0.932226</td>\n",
       "      <td>0.930432</td>\n",
       "      <td>0.927603</td>\n",
       "      <td>0.927982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.268856</td>\n",
       "      <td>0.935572</td>\n",
       "      <td>0.933688</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>0.931568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.265248</td>\n",
       "      <td>0.938050</td>\n",
       "      <td>0.935620</td>\n",
       "      <td>0.933598</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 19:12:04,617] Trial 19 finished with value: 3.741136595545795 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 1 with value: 3.741136595545795.\n"
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    n_trials=20, \n",
    "    hp_space=optuna_hp_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a73ebc-2b56-45fd-93e8-c7f8caf940c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='1', objective=3.741136595545795, hyperparameters={'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}, run_summary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94567a32-b8bc-47b6-a670-1235f2784399",
   "metadata": {},
   "source": [
    "## Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c5e4-bd9f-41d2-a073-223e41db8f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a XLNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18801' max='20952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18801/20952 2:00:50 < 13:49, 2.59 it/s, Epoch 2.69/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.979810</td>\n",
       "      <td>0.538099</td>\n",
       "      <td>0.527977</td>\n",
       "      <td>0.654790</td>\n",
       "      <td>0.429720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.583995</td>\n",
       "      <td>0.728163</td>\n",
       "      <td>0.703433</td>\n",
       "      <td>0.799590</td>\n",
       "      <td>0.649084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.792800</td>\n",
       "      <td>0.364707</td>\n",
       "      <td>0.868789</td>\n",
       "      <td>0.866238</td>\n",
       "      <td>0.867951</td>\n",
       "      <td>0.860992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.792800</td>\n",
       "      <td>0.272830</td>\n",
       "      <td>0.902181</td>\n",
       "      <td>0.894584</td>\n",
       "      <td>0.898441</td>\n",
       "      <td>0.894338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.259474</td>\n",
       "      <td>0.905154</td>\n",
       "      <td>0.900147</td>\n",
       "      <td>0.901022</td>\n",
       "      <td>0.898284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.242053</td>\n",
       "      <td>0.916429</td>\n",
       "      <td>0.910043</td>\n",
       "      <td>0.912242</td>\n",
       "      <td>0.909798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.243242</td>\n",
       "      <td>0.918040</td>\n",
       "      <td>0.913659</td>\n",
       "      <td>0.912796</td>\n",
       "      <td>0.912668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.235800</td>\n",
       "      <td>0.265941</td>\n",
       "      <td>0.914695</td>\n",
       "      <td>0.913514</td>\n",
       "      <td>0.908828</td>\n",
       "      <td>0.910690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.235800</td>\n",
       "      <td>0.312867</td>\n",
       "      <td>0.902243</td>\n",
       "      <td>0.901494</td>\n",
       "      <td>0.899914</td>\n",
       "      <td>0.896866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.407848</td>\n",
       "      <td>0.873622</td>\n",
       "      <td>0.869789</td>\n",
       "      <td>0.883767</td>\n",
       "      <td>0.863788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.270980</td>\n",
       "      <td>0.920270</td>\n",
       "      <td>0.919233</td>\n",
       "      <td>0.915300</td>\n",
       "      <td>0.915803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.219397</td>\n",
       "      <td>0.922624</td>\n",
       "      <td>0.920267</td>\n",
       "      <td>0.917403</td>\n",
       "      <td>0.917401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.213711</td>\n",
       "      <td>0.930368</td>\n",
       "      <td>0.927210</td>\n",
       "      <td>0.925196</td>\n",
       "      <td>0.925611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.201189</td>\n",
       "      <td>0.931483</td>\n",
       "      <td>0.927103</td>\n",
       "      <td>0.928169</td>\n",
       "      <td>0.925901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>0.208255</td>\n",
       "      <td>0.931917</td>\n",
       "      <td>0.927111</td>\n",
       "      <td>0.927184</td>\n",
       "      <td>0.926481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>0.225259</td>\n",
       "      <td>0.929934</td>\n",
       "      <td>0.926722</td>\n",
       "      <td>0.925726</td>\n",
       "      <td>0.924748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>0.221432</td>\n",
       "      <td>0.925722</td>\n",
       "      <td>0.923519</td>\n",
       "      <td>0.922695</td>\n",
       "      <td>0.920926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.244054</td>\n",
       "      <td>0.930120</td>\n",
       "      <td>0.926179</td>\n",
       "      <td>0.926887</td>\n",
       "      <td>0.924541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.227684</td>\n",
       "      <td>0.930802</td>\n",
       "      <td>0.928129</td>\n",
       "      <td>0.926760</td>\n",
       "      <td>0.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.251613</td>\n",
       "      <td>0.931359</td>\n",
       "      <td>0.926562</td>\n",
       "      <td>0.928315</td>\n",
       "      <td>0.925507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.231204</td>\n",
       "      <td>0.928262</td>\n",
       "      <td>0.926724</td>\n",
       "      <td>0.923910</td>\n",
       "      <td>0.923665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.207970</td>\n",
       "      <td>0.934704</td>\n",
       "      <td>0.932989</td>\n",
       "      <td>0.929345</td>\n",
       "      <td>0.930936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.210631</td>\n",
       "      <td>0.934828</td>\n",
       "      <td>0.930884</td>\n",
       "      <td>0.931025</td>\n",
       "      <td>0.929937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.222527</td>\n",
       "      <td>0.933218</td>\n",
       "      <td>0.930217</td>\n",
       "      <td>0.929446</td>\n",
       "      <td>0.928469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>0.174394</td>\n",
       "      <td>0.941085</td>\n",
       "      <td>0.936621</td>\n",
       "      <td>0.937745</td>\n",
       "      <td>0.936352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>0.204564</td>\n",
       "      <td>0.940280</td>\n",
       "      <td>0.936821</td>\n",
       "      <td>0.936165</td>\n",
       "      <td>0.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>0.225960</td>\n",
       "      <td>0.931297</td>\n",
       "      <td>0.929967</td>\n",
       "      <td>0.926412</td>\n",
       "      <td>0.927397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.200256</td>\n",
       "      <td>0.939413</td>\n",
       "      <td>0.935951</td>\n",
       "      <td>0.936131</td>\n",
       "      <td>0.935064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.184170</td>\n",
       "      <td>0.942510</td>\n",
       "      <td>0.939787</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.938558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.204203</td>\n",
       "      <td>0.938855</td>\n",
       "      <td>0.936047</td>\n",
       "      <td>0.934849</td>\n",
       "      <td>0.934666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.241120</td>\n",
       "      <td>0.927642</td>\n",
       "      <td>0.928604</td>\n",
       "      <td>0.922369</td>\n",
       "      <td>0.924676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.224155</td>\n",
       "      <td>0.931855</td>\n",
       "      <td>0.930065</td>\n",
       "      <td>0.927388</td>\n",
       "      <td>0.927275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.217887</td>\n",
       "      <td>0.939599</td>\n",
       "      <td>0.937185</td>\n",
       "      <td>0.934784</td>\n",
       "      <td>0.935470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.231422</td>\n",
       "      <td>0.940404</td>\n",
       "      <td>0.937897</td>\n",
       "      <td>0.935709</td>\n",
       "      <td>0.936150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.208497</td>\n",
       "      <td>0.940652</td>\n",
       "      <td>0.936477</td>\n",
       "      <td>0.937288</td>\n",
       "      <td>0.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.195881</td>\n",
       "      <td>0.945856</td>\n",
       "      <td>0.942690</td>\n",
       "      <td>0.941613</td>\n",
       "      <td>0.941996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.206734</td>\n",
       "      <td>0.938731</td>\n",
       "      <td>0.935923</td>\n",
       "      <td>0.934328</td>\n",
       "      <td>0.934208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.117800</td>\n",
       "      <td>0.199233</td>\n",
       "      <td>0.944740</td>\n",
       "      <td>0.940716</td>\n",
       "      <td>0.941319</td>\n",
       "      <td>0.940302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.117800</td>\n",
       "      <td>0.202938</td>\n",
       "      <td>0.942386</td>\n",
       "      <td>0.940159</td>\n",
       "      <td>0.937677</td>\n",
       "      <td>0.938554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.118300</td>\n",
       "      <td>0.242958</td>\n",
       "      <td>0.938607</td>\n",
       "      <td>0.937219</td>\n",
       "      <td>0.933902</td>\n",
       "      <td>0.934793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.118300</td>\n",
       "      <td>0.221669</td>\n",
       "      <td>0.941705</td>\n",
       "      <td>0.940069</td>\n",
       "      <td>0.936779</td>\n",
       "      <td>0.938080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.118300</td>\n",
       "      <td>0.225474</td>\n",
       "      <td>0.941085</td>\n",
       "      <td>0.936867</td>\n",
       "      <td>0.938205</td>\n",
       "      <td>0.936189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.252565</td>\n",
       "      <td>0.938236</td>\n",
       "      <td>0.936901</td>\n",
       "      <td>0.933412</td>\n",
       "      <td>0.934407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.283963</td>\n",
       "      <td>0.933404</td>\n",
       "      <td>0.930737</td>\n",
       "      <td>0.930744</td>\n",
       "      <td>0.928900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.199144</td>\n",
       "      <td>0.942820</td>\n",
       "      <td>0.939641</td>\n",
       "      <td>0.938895</td>\n",
       "      <td>0.938338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.261741</td>\n",
       "      <td>0.929129</td>\n",
       "      <td>0.930284</td>\n",
       "      <td>0.923940</td>\n",
       "      <td>0.926173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.233896</td>\n",
       "      <td>0.940404</td>\n",
       "      <td>0.938839</td>\n",
       "      <td>0.936100</td>\n",
       "      <td>0.936420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.212869</td>\n",
       "      <td>0.941953</td>\n",
       "      <td>0.939290</td>\n",
       "      <td>0.937804</td>\n",
       "      <td>0.937602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.219356</td>\n",
       "      <td>0.945979</td>\n",
       "      <td>0.942646</td>\n",
       "      <td>0.941787</td>\n",
       "      <td>0.941913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.214911</td>\n",
       "      <td>0.940032</td>\n",
       "      <td>0.939089</td>\n",
       "      <td>0.935268</td>\n",
       "      <td>0.936654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>0.945979</td>\n",
       "      <td>0.942983</td>\n",
       "      <td>0.942099</td>\n",
       "      <td>0.941939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.227622</td>\n",
       "      <td>0.943997</td>\n",
       "      <td>0.941549</td>\n",
       "      <td>0.940096</td>\n",
       "      <td>0.939855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.215118</td>\n",
       "      <td>0.943811</td>\n",
       "      <td>0.941093</td>\n",
       "      <td>0.939687</td>\n",
       "      <td>0.939785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.210816</td>\n",
       "      <td>0.945050</td>\n",
       "      <td>0.941546</td>\n",
       "      <td>0.940825</td>\n",
       "      <td>0.940850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.232737</td>\n",
       "      <td>0.942696</td>\n",
       "      <td>0.940437</td>\n",
       "      <td>0.938464</td>\n",
       "      <td>0.938663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.211334</td>\n",
       "      <td>0.948395</td>\n",
       "      <td>0.944555</td>\n",
       "      <td>0.945389</td>\n",
       "      <td>0.944166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.221365</td>\n",
       "      <td>0.944493</td>\n",
       "      <td>0.942473</td>\n",
       "      <td>0.939722</td>\n",
       "      <td>0.940788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.238804</td>\n",
       "      <td>0.941705</td>\n",
       "      <td>0.939951</td>\n",
       "      <td>0.937444</td>\n",
       "      <td>0.937952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.222954</td>\n",
       "      <td>0.945670</td>\n",
       "      <td>0.943002</td>\n",
       "      <td>0.941655</td>\n",
       "      <td>0.941753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.255915</td>\n",
       "      <td>0.934828</td>\n",
       "      <td>0.934260</td>\n",
       "      <td>0.930799</td>\n",
       "      <td>0.931037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.207483</td>\n",
       "      <td>0.944493</td>\n",
       "      <td>0.943674</td>\n",
       "      <td>0.939725</td>\n",
       "      <td>0.941443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.209921</td>\n",
       "      <td>0.940776</td>\n",
       "      <td>0.939472</td>\n",
       "      <td>0.936439</td>\n",
       "      <td>0.937022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.196314</td>\n",
       "      <td>0.948334</td>\n",
       "      <td>0.945468</td>\n",
       "      <td>0.944160</td>\n",
       "      <td>0.944570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.182079</td>\n",
       "      <td>0.950626</td>\n",
       "      <td>0.947795</td>\n",
       "      <td>0.946826</td>\n",
       "      <td>0.947044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.194605</td>\n",
       "      <td>0.947838</td>\n",
       "      <td>0.945620</td>\n",
       "      <td>0.943826</td>\n",
       "      <td>0.944269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.197237</td>\n",
       "      <td>0.949511</td>\n",
       "      <td>0.946193</td>\n",
       "      <td>0.945930</td>\n",
       "      <td>0.945562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.191605</td>\n",
       "      <td>0.947466</td>\n",
       "      <td>0.943968</td>\n",
       "      <td>0.944232</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.197458</td>\n",
       "      <td>0.946847</td>\n",
       "      <td>0.944383</td>\n",
       "      <td>0.942752</td>\n",
       "      <td>0.942962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.211370</td>\n",
       "      <td>0.944740</td>\n",
       "      <td>0.942818</td>\n",
       "      <td>0.940681</td>\n",
       "      <td>0.940995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.197304</td>\n",
       "      <td>0.949758</td>\n",
       "      <td>0.946699</td>\n",
       "      <td>0.945959</td>\n",
       "      <td>0.945988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.227369</td>\n",
       "      <td>0.947342</td>\n",
       "      <td>0.945190</td>\n",
       "      <td>0.943092</td>\n",
       "      <td>0.943764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.258642</td>\n",
       "      <td>0.943378</td>\n",
       "      <td>0.941954</td>\n",
       "      <td>0.939089</td>\n",
       "      <td>0.939707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.212783</td>\n",
       "      <td>0.948334</td>\n",
       "      <td>0.945800</td>\n",
       "      <td>0.944116</td>\n",
       "      <td>0.944669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.221644</td>\n",
       "      <td>0.948024</td>\n",
       "      <td>0.945698</td>\n",
       "      <td>0.943964</td>\n",
       "      <td>0.944403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.238492</td>\n",
       "      <td>0.944678</td>\n",
       "      <td>0.942255</td>\n",
       "      <td>0.940599</td>\n",
       "      <td>0.940720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.238919</td>\n",
       "      <td>0.945979</td>\n",
       "      <td>0.943378</td>\n",
       "      <td>0.942127</td>\n",
       "      <td>0.942081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.219455</td>\n",
       "      <td>0.948767</td>\n",
       "      <td>0.945704</td>\n",
       "      <td>0.944914</td>\n",
       "      <td>0.944992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.252885</td>\n",
       "      <td>0.945608</td>\n",
       "      <td>0.943566</td>\n",
       "      <td>0.941169</td>\n",
       "      <td>0.941928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.223268</td>\n",
       "      <td>0.949139</td>\n",
       "      <td>0.946927</td>\n",
       "      <td>0.944880</td>\n",
       "      <td>0.945658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.223603</td>\n",
       "      <td>0.948767</td>\n",
       "      <td>0.946283</td>\n",
       "      <td>0.944623</td>\n",
       "      <td>0.945234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.230760</td>\n",
       "      <td>0.949139</td>\n",
       "      <td>0.946895</td>\n",
       "      <td>0.945013</td>\n",
       "      <td>0.945668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.230765</td>\n",
       "      <td>0.946413</td>\n",
       "      <td>0.944439</td>\n",
       "      <td>0.942234</td>\n",
       "      <td>0.942887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.213570</td>\n",
       "      <td>0.949696</td>\n",
       "      <td>0.946854</td>\n",
       "      <td>0.945871</td>\n",
       "      <td>0.946055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.211753</td>\n",
       "      <td>0.951431</td>\n",
       "      <td>0.948611</td>\n",
       "      <td>0.947555</td>\n",
       "      <td>0.947941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.225417</td>\n",
       "      <td>0.949325</td>\n",
       "      <td>0.946280</td>\n",
       "      <td>0.945574</td>\n",
       "      <td>0.945483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.226770</td>\n",
       "      <td>0.948581</td>\n",
       "      <td>0.945680</td>\n",
       "      <td>0.944747</td>\n",
       "      <td>0.944749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.227817</td>\n",
       "      <td>0.948334</td>\n",
       "      <td>0.945707</td>\n",
       "      <td>0.944360</td>\n",
       "      <td>0.944550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.233551</td>\n",
       "      <td>0.946785</td>\n",
       "      <td>0.944275</td>\n",
       "      <td>0.942792</td>\n",
       "      <td>0.942988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>0.948519</td>\n",
       "      <td>0.945871</td>\n",
       "      <td>0.944597</td>\n",
       "      <td>0.944800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.249371</td>\n",
       "      <td>0.947590</td>\n",
       "      <td>0.945129</td>\n",
       "      <td>0.943710</td>\n",
       "      <td>0.943846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.243346</td>\n",
       "      <td>0.948210</td>\n",
       "      <td>0.945633</td>\n",
       "      <td>0.944454</td>\n",
       "      <td>0.944427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.233345</td>\n",
       "      <td>0.949634</td>\n",
       "      <td>0.947185</td>\n",
       "      <td>0.945609</td>\n",
       "      <td>0.946084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.225327</td>\n",
       "      <td>0.949139</td>\n",
       "      <td>0.947018</td>\n",
       "      <td>0.944953</td>\n",
       "      <td>0.945721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='985' max='1009' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 985/1009 00:29 < 00:00, 33.51 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "    \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c033e6c-1341-4ddd-9c81-10f456146e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_store_path.parent / (model_store_path.stem + \".out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fc1ce-86d3-4168-a4a1-71f521fe37a3",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b699f7bb-e4c9-45ad-9fd1-d7425dd6f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7590-31cc-40ad-b212-d0c47500277f",
   "metadata": {},
   "source": [
    "## Test on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdfafa9a-6265-4792-8513-149ab1b0545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"validation\"])\n",
    "micro_val, macro_val = generate_micro_macro_df(data[\"validation\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c16081ff-6aa6-4620-8cdf-50f3e07b73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.98      0.97      0.98      6666\n",
      "        REFUTES       0.94      0.92      0.93      4888\n",
      "       SUPPORTS       0.92      0.95      0.94      4588\n",
      "\n",
      "       accuracy                           0.95     16142\n",
      "      macro avg       0.95      0.95      0.95     16142\n",
      "   weighted avg       0.95      0.95      0.95     16142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=micro_val[\"actual\"], y_pred=micro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e22242c-df55-4f0b-8a4e-4c69a7d4bffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.98      0.96      0.97      3238\n",
      "        REFUTES       0.93      0.92      0.92      3260\n",
      "       SUPPORTS       0.91      0.95      0.93      3245\n",
      "\n",
      "       accuracy                           0.94      9743\n",
      "      macro avg       0.94      0.94      0.94      9743\n",
      "   weighted avg       0.94      0.94      0.94      9743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=macro_val[\"actual\"], y_pred=macro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d9a077e-42e4-42e5-8456-e49f974d4c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.98      0.99      0.99      3238\n",
      "        REFUTES       0.94      0.91      0.93      3260\n",
      "       SUPPORTS       0.92      0.95      0.94      3245\n",
      "\n",
      "       accuracy                           0.95      9743\n",
      "      macro avg       0.95      0.95      0.95      9743\n",
      "   weighted avg       0.95      0.95      0.95      9743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=macro_val[\"actual\"], y_pred=macro_val[\"proba\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d4cf6-3cfe-458f-9ad7-e58994554cbc",
   "metadata": {},
   "source": [
    "## Test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b9b87e1-0054-4bbe-a6e8-b76867f03b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"test\"])\n",
    "micro_val, macro_val = generate_micro_macro_df(data[\"test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6548fc2f-a81f-4e65-b697-0b9e80ba9402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.98      0.98      0.98      6666\n",
      "        REFUTES       0.93      0.89      0.91      4909\n",
      "       SUPPORTS       0.89      0.94      0.92      4631\n",
      "\n",
      "       accuracy                           0.94     16206\n",
      "      macro avg       0.94      0.94      0.94     16206\n",
      "   weighted avg       0.94      0.94      0.94     16206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=micro_val[\"actual\"], y_pred=micro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6709c77-f012-4965-ab4d-c548aa954bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.98      0.96      0.97      3284\n",
      "        REFUTES       0.93      0.88      0.90      3280\n",
      "       SUPPORTS       0.88      0.95      0.91      3290\n",
      "\n",
      "       accuracy                           0.93      9854\n",
      "      macro avg       0.93      0.93      0.93      9854\n",
      "   weighted avg       0.93      0.93      0.93      9854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=macro_val[\"actual\"], y_pred=macro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec515b39-7ae3-4f19-923b-ba51b688f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.98      0.99      0.99      3284\n",
      "        REFUTES       0.95      0.88      0.91      3280\n",
      "       SUPPORTS       0.89      0.95      0.92      3290\n",
      "\n",
      "       accuracy                           0.94      9854\n",
      "      macro avg       0.94      0.94      0.94      9854\n",
      "   weighted avg       0.94      0.94      0.94      9854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=macro_val[\"actual\"], y_pred=macro_val[\"proba\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae0ef8-63c5-4092-b9a3-1528fe23a2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
