{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402ecfd6-f01c-43ef-aaa4-018f9dc63dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/gen\")\n",
    "sys.path.insert(1, \"../src/rte\")\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from util import read_data, write_jsonl\n",
    "from aggregate import generate_micro_macro_df, generate_doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1929c84-28f4-4949-b880-a85b8f8b8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "LOOKUP = {\n",
    "    \"verifiable\": {\"no\": \"NOT VERIFIABLE\", \"yes\": \"VERIFIABLE\"},\n",
    "    \"label\": {\"nei\": \"NOT ENOUGH INFO\", \"r\": \"REFUTES\", \"s\": \"SUPPORTS\"}\n",
    "}\n",
    "\n",
    "SEED = 123456789\n",
    "\n",
    "LABEL2ID = {\"SUPPORTS\": 0, \"NOT ENOUGH INFO\": 1, \"REFUTES\": 2}\n",
    "ID2LABEL = {0: \"SUPPORTS\", 1: \"NOT ENOUGH INFO\", 2: \"REFUTES\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05b8ab-6775-4eed-98be-10904943ded1",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aacd87-5b38-45b8-8be3-f7046bfa06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    TextClassificationPipeline,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd986a-1e45-4f38-8487-dddf2ee3ef24",
   "metadata": {},
   "source": [
    "# Huggingface Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b42b8e-2eb3-444b-9275-192519540439",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e5281d-7e7d-410e-b1ba-730e334247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2c86dd-cb1b-4b77-836d-1e5619f4bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        num_labels=3, \n",
    "        id2label=ID2LABEL, \n",
    "        label2id=LABEL2ID\n",
    "    )\n",
    "\n",
    "model = model_init()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, do_lower_case=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"evidence\"], examples[\"claim\"], max_length=512, truncation=\"only_first\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4834e0-770d-49dc-8041-f4d9d2729053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"fever\", \"climatefever\", \"fever-climatefever\"]\n",
    "task = [\"hp_tune\", \"output\"]\n",
    "doc_sent = [\"doc\", \"sent\"]\n",
    "\n",
    "di = 0\n",
    "ti = 0\n",
    "ds = 0\n",
    "\n",
    "model_store_path = Path(\"/users/k21190024/study/fact-check-transfer-learning/scratch/thesis/models\").joinpath(model_checkpoint)\n",
    "model_store_path.mkdir(exist_ok=True)\n",
    "model_store_path = model_store_path / f\"{dataset[di]}-{model_checkpoint}-{doc_sent[ds]}-{task[ti]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940210d7-1da5-4e6f-b4f6-95304ceb496b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a618a5-0c63-4a99-b7cb-89deea2416f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/145449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datap = Path(f\"/users/k21190024/study/fact-check-transfer-learning/scratch/dumps/bert-data-{doc_sent[ds]}-evidence\")\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.train.n5.jsonl\")),\n",
    "    \"validation\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.dev.n5.jsonl\")),\n",
    "    \"test\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.test.n5.jsonl\"))\n",
    "}).map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d7f9a-4ddc-4a9b-b6cf-d31233fff94b",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06cfa045-016f-44e2-91c9-567ddd56ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "epoch = 4\n",
    "metric_name = \"f1\"\n",
    "warmup_ratio=0.1\n",
    "save_steps=200\n",
    "eval_steps=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666373d-06fa-476c-9aaf-ffb3daadbf4b",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967aecb0-5bc7-488c-aefa-caf4525216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shard the data if the dataset is large for hyperparameter tuning\n",
    "shard = data[\"train\"].num_rows > 50000\n",
    "hp_tune_train = data[\"train\"].shuffle(seed=SEED).shard(num_shards=5, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e23d248-bb05-43da-a056-4e1fcc2a17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy = \"no\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\",\n",
    "    tf32=True\n",
    ")\n",
    "\n",
    "_ = model.train()\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=hp_tune_train if shard else data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5445d33-24fc-4a6d-8ed2-a89b26aa65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-5, 3e-5, 2e-5]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "    }\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d4edde-28e8-4b30-a700-dc3f4a2a2ce9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 13:43:19,788] A new study created in memory with name: no-name-d00b05c7-1510-47de-b5f6-a7e264da984e\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2730' max='2730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2730/2730 07:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.661620</td>\n",
       "      <td>0.658066</td>\n",
       "      <td>0.658066</td>\n",
       "      <td>0.659200</td>\n",
       "      <td>0.547713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.239154</td>\n",
       "      <td>0.912491</td>\n",
       "      <td>0.912491</td>\n",
       "      <td>0.916174</td>\n",
       "      <td>0.912087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.209630</td>\n",
       "      <td>0.925993</td>\n",
       "      <td>0.925993</td>\n",
       "      <td>0.928032</td>\n",
       "      <td>0.925833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.208217</td>\n",
       "      <td>0.931293</td>\n",
       "      <td>0.931293</td>\n",
       "      <td>0.933916</td>\n",
       "      <td>0.931267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.211251</td>\n",
       "      <td>0.932693</td>\n",
       "      <td>0.932693</td>\n",
       "      <td>0.936083</td>\n",
       "      <td>0.932621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.194829</td>\n",
       "      <td>0.939994</td>\n",
       "      <td>0.939994</td>\n",
       "      <td>0.941238</td>\n",
       "      <td>0.939965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.242561</td>\n",
       "      <td>0.925193</td>\n",
       "      <td>0.925193</td>\n",
       "      <td>0.930704</td>\n",
       "      <td>0.924873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.134500</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.933893</td>\n",
       "      <td>0.933893</td>\n",
       "      <td>0.937704</td>\n",
       "      <td>0.933789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.134500</td>\n",
       "      <td>0.201446</td>\n",
       "      <td>0.936894</td>\n",
       "      <td>0.936894</td>\n",
       "      <td>0.939571</td>\n",
       "      <td>0.936696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.252698</td>\n",
       "      <td>0.936494</td>\n",
       "      <td>0.936494</td>\n",
       "      <td>0.939619</td>\n",
       "      <td>0.936383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.938994</td>\n",
       "      <td>0.938994</td>\n",
       "      <td>0.941169</td>\n",
       "      <td>0.938980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.245618</td>\n",
       "      <td>0.938994</td>\n",
       "      <td>0.938994</td>\n",
       "      <td>0.941354</td>\n",
       "      <td>0.938886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.230146</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>0.942492</td>\n",
       "      <td>0.941066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 13:50:43,919] Trial 0 finished with value: 3.7657467255346386 and parameters: {'learning_rate': 3e-05, 'num_train_epochs': 3}. Best is trial 0 with value: 3.7657467255346386.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3640' max='3640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3640/3640 09:52, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.646420</td>\n",
       "      <td>0.655766</td>\n",
       "      <td>0.655766</td>\n",
       "      <td>0.489200</td>\n",
       "      <td>0.545109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.306013</td>\n",
       "      <td>0.878288</td>\n",
       "      <td>0.878288</td>\n",
       "      <td>0.891253</td>\n",
       "      <td>0.876036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.216311</td>\n",
       "      <td>0.921292</td>\n",
       "      <td>0.921292</td>\n",
       "      <td>0.922859</td>\n",
       "      <td>0.920955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.213145</td>\n",
       "      <td>0.925993</td>\n",
       "      <td>0.925993</td>\n",
       "      <td>0.929980</td>\n",
       "      <td>0.925847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.202700</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.934693</td>\n",
       "      <td>0.934693</td>\n",
       "      <td>0.936586</td>\n",
       "      <td>0.934636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.202700</td>\n",
       "      <td>0.190880</td>\n",
       "      <td>0.939794</td>\n",
       "      <td>0.939794</td>\n",
       "      <td>0.940391</td>\n",
       "      <td>0.939739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.202700</td>\n",
       "      <td>0.211524</td>\n",
       "      <td>0.926593</td>\n",
       "      <td>0.926593</td>\n",
       "      <td>0.931494</td>\n",
       "      <td>0.926282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>0.230081</td>\n",
       "      <td>0.930693</td>\n",
       "      <td>0.930693</td>\n",
       "      <td>0.934777</td>\n",
       "      <td>0.930575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>0.220852</td>\n",
       "      <td>0.930793</td>\n",
       "      <td>0.930793</td>\n",
       "      <td>0.935333</td>\n",
       "      <td>0.930619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.250252</td>\n",
       "      <td>0.933593</td>\n",
       "      <td>0.933593</td>\n",
       "      <td>0.936882</td>\n",
       "      <td>0.933674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.251457</td>\n",
       "      <td>0.936994</td>\n",
       "      <td>0.936994</td>\n",
       "      <td>0.939763</td>\n",
       "      <td>0.936982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.248270</td>\n",
       "      <td>0.935794</td>\n",
       "      <td>0.935794</td>\n",
       "      <td>0.938448</td>\n",
       "      <td>0.935652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.256146</td>\n",
       "      <td>0.936894</td>\n",
       "      <td>0.936894</td>\n",
       "      <td>0.938887</td>\n",
       "      <td>0.936983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.257351</td>\n",
       "      <td>0.932693</td>\n",
       "      <td>0.932693</td>\n",
       "      <td>0.936005</td>\n",
       "      <td>0.932539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.277109</td>\n",
       "      <td>0.934993</td>\n",
       "      <td>0.934993</td>\n",
       "      <td>0.937937</td>\n",
       "      <td>0.934944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.282533</td>\n",
       "      <td>0.934893</td>\n",
       "      <td>0.934893</td>\n",
       "      <td>0.937570</td>\n",
       "      <td>0.934831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.269107</td>\n",
       "      <td>0.936994</td>\n",
       "      <td>0.936994</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.936939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.258951</td>\n",
       "      <td>0.938294</td>\n",
       "      <td>0.938294</td>\n",
       "      <td>0.940262</td>\n",
       "      <td>0.938242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-04 14:00:37,725] Trial 1 finished with value: 3.7550912675037296 and parameters: {'learning_rate': 2e-05, 'num_train_epochs': 4}. Best is trial 0 with value: 3.7657467255346386.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2730' max='2730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2730/2730 07:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.421591</td>\n",
       "      <td>0.811181</td>\n",
       "      <td>0.811181</td>\n",
       "      <td>0.849094</td>\n",
       "      <td>0.799006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.236395</td>\n",
       "      <td>0.916292</td>\n",
       "      <td>0.916292</td>\n",
       "      <td>0.919214</td>\n",
       "      <td>0.916084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>0.201488</td>\n",
       "      <td>0.929793</td>\n",
       "      <td>0.929793</td>\n",
       "      <td>0.930568</td>\n",
       "      <td>0.929640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>0.195903</td>\n",
       "      <td>0.923592</td>\n",
       "      <td>0.923592</td>\n",
       "      <td>0.928485</td>\n",
       "      <td>0.923307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.210948</td>\n",
       "      <td>0.934493</td>\n",
       "      <td>0.934493</td>\n",
       "      <td>0.935555</td>\n",
       "      <td>0.934464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.936294</td>\n",
       "      <td>0.936294</td>\n",
       "      <td>0.937280</td>\n",
       "      <td>0.936243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.253301</td>\n",
       "      <td>0.929393</td>\n",
       "      <td>0.929393</td>\n",
       "      <td>0.932987</td>\n",
       "      <td>0.929063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.134700</td>\n",
       "      <td>0.261010</td>\n",
       "      <td>0.931193</td>\n",
       "      <td>0.931193</td>\n",
       "      <td>0.935101</td>\n",
       "      <td>0.930850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.134700</td>\n",
       "      <td>0.227615</td>\n",
       "      <td>0.931393</td>\n",
       "      <td>0.931393</td>\n",
       "      <td>0.935594</td>\n",
       "      <td>0.931247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.283238</td>\n",
       "      <td>0.935194</td>\n",
       "      <td>0.935194</td>\n",
       "      <td>0.938051</td>\n",
       "      <td>0.934930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.303995</td>\n",
       "      <td>0.934093</td>\n",
       "      <td>0.934093</td>\n",
       "      <td>0.936996</td>\n",
       "      <td>0.934017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.300939</td>\n",
       "      <td>0.932393</td>\n",
       "      <td>0.932393</td>\n",
       "      <td>0.936185</td>\n",
       "      <td>0.932196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.268467</td>\n",
       "      <td>0.937594</td>\n",
       "      <td>0.937594</td>\n",
       "      <td>0.940050</td>\n",
       "      <td>0.937418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:07:58,179] Trial 2 finished with value: 3.7526556601423042 and parameters: {'learning_rate': 5e-05, 'num_train_epochs': 3}. Best is trial 0 with value: 3.7657467255346386.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4550' max='4550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4550/4550 12:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.638685</td>\n",
       "      <td>0.657166</td>\n",
       "      <td>0.657166</td>\n",
       "      <td>0.490467</td>\n",
       "      <td>0.546401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.297662</td>\n",
       "      <td>0.888589</td>\n",
       "      <td>0.888589</td>\n",
       "      <td>0.896408</td>\n",
       "      <td>0.888128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.482400</td>\n",
       "      <td>0.201401</td>\n",
       "      <td>0.925193</td>\n",
       "      <td>0.925193</td>\n",
       "      <td>0.925720</td>\n",
       "      <td>0.925024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.482400</td>\n",
       "      <td>0.221610</td>\n",
       "      <td>0.922592</td>\n",
       "      <td>0.922592</td>\n",
       "      <td>0.927790</td>\n",
       "      <td>0.922260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.202357</td>\n",
       "      <td>0.934493</td>\n",
       "      <td>0.934493</td>\n",
       "      <td>0.936732</td>\n",
       "      <td>0.934358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.207302</td>\n",
       "      <td>0.936294</td>\n",
       "      <td>0.936294</td>\n",
       "      <td>0.937527</td>\n",
       "      <td>0.936291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.254437</td>\n",
       "      <td>0.918192</td>\n",
       "      <td>0.918192</td>\n",
       "      <td>0.926514</td>\n",
       "      <td>0.917614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.224390</td>\n",
       "      <td>0.933593</td>\n",
       "      <td>0.933593</td>\n",
       "      <td>0.937324</td>\n",
       "      <td>0.933337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.200387</td>\n",
       "      <td>0.932293</td>\n",
       "      <td>0.932293</td>\n",
       "      <td>0.936124</td>\n",
       "      <td>0.932034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.248337</td>\n",
       "      <td>0.932393</td>\n",
       "      <td>0.932393</td>\n",
       "      <td>0.936529</td>\n",
       "      <td>0.932306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.268461</td>\n",
       "      <td>0.937194</td>\n",
       "      <td>0.937194</td>\n",
       "      <td>0.939902</td>\n",
       "      <td>0.937125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.234078</td>\n",
       "      <td>0.937994</td>\n",
       "      <td>0.937994</td>\n",
       "      <td>0.939135</td>\n",
       "      <td>0.937894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.277349</td>\n",
       "      <td>0.936394</td>\n",
       "      <td>0.936394</td>\n",
       "      <td>0.938836</td>\n",
       "      <td>0.936129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.282476</td>\n",
       "      <td>0.932693</td>\n",
       "      <td>0.932693</td>\n",
       "      <td>0.936015</td>\n",
       "      <td>0.932660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.330798</td>\n",
       "      <td>0.932393</td>\n",
       "      <td>0.932393</td>\n",
       "      <td>0.936245</td>\n",
       "      <td>0.932225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.318917</td>\n",
       "      <td>0.929793</td>\n",
       "      <td>0.929793</td>\n",
       "      <td>0.934656</td>\n",
       "      <td>0.929490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.284328</td>\n",
       "      <td>0.936594</td>\n",
       "      <td>0.936594</td>\n",
       "      <td>0.939230</td>\n",
       "      <td>0.936587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.294811</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>0.940073</td>\n",
       "      <td>0.938195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.326256</td>\n",
       "      <td>0.935094</td>\n",
       "      <td>0.935094</td>\n",
       "      <td>0.937435</td>\n",
       "      <td>0.935022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.316807</td>\n",
       "      <td>0.938894</td>\n",
       "      <td>0.938894</td>\n",
       "      <td>0.940420</td>\n",
       "      <td>0.938868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.349212</td>\n",
       "      <td>0.934293</td>\n",
       "      <td>0.934293</td>\n",
       "      <td>0.937216</td>\n",
       "      <td>0.934111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.345553</td>\n",
       "      <td>0.936594</td>\n",
       "      <td>0.936594</td>\n",
       "      <td>0.939008</td>\n",
       "      <td>0.936473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-04 14:20:16,192] Trial 3 finished with value: 3.7486683219841455 and parameters: {'learning_rate': 3e-05, 'num_train_epochs': 5}. Best is trial 0 with value: 3.7657467255346386.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1820' max='1820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1820/1820 04:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.660410</td>\n",
       "      <td>0.657766</td>\n",
       "      <td>0.657766</td>\n",
       "      <td>0.659290</td>\n",
       "      <td>0.547541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.265282</td>\n",
       "      <td>0.901390</td>\n",
       "      <td>0.901390</td>\n",
       "      <td>0.910063</td>\n",
       "      <td>0.900281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.212183</td>\n",
       "      <td>0.925593</td>\n",
       "      <td>0.925593</td>\n",
       "      <td>0.928161</td>\n",
       "      <td>0.925487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.207044</td>\n",
       "      <td>0.930793</td>\n",
       "      <td>0.930793</td>\n",
       "      <td>0.933915</td>\n",
       "      <td>0.930638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.192595</td>\n",
       "      <td>0.937394</td>\n",
       "      <td>0.937394</td>\n",
       "      <td>0.938419</td>\n",
       "      <td>0.937468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.188796</td>\n",
       "      <td>0.941194</td>\n",
       "      <td>0.941194</td>\n",
       "      <td>0.941990</td>\n",
       "      <td>0.941255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.214060</td>\n",
       "      <td>0.932293</td>\n",
       "      <td>0.932293</td>\n",
       "      <td>0.936176</td>\n",
       "      <td>0.932091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.225375</td>\n",
       "      <td>0.933393</td>\n",
       "      <td>0.933393</td>\n",
       "      <td>0.936936</td>\n",
       "      <td>0.933297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.210765</td>\n",
       "      <td>0.937094</td>\n",
       "      <td>0.937094</td>\n",
       "      <td>0.939722</td>\n",
       "      <td>0.937012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:25:13,468] Trial 4 finished with value: 3.75092165167713 and parameters: {'learning_rate': 2e-05, 'num_train_epochs': 2}. Best is trial 0 with value: 3.7657467255346386.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='2730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/2730 02:43 < 04:43, 6.09 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.421591</td>\n",
       "      <td>0.811181</td>\n",
       "      <td>0.811181</td>\n",
       "      <td>0.849094</td>\n",
       "      <td>0.799006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.236395</td>\n",
       "      <td>0.916292</td>\n",
       "      <td>0.916292</td>\n",
       "      <td>0.919214</td>\n",
       "      <td>0.916084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>0.201488</td>\n",
       "      <td>0.929793</td>\n",
       "      <td>0.929793</td>\n",
       "      <td>0.930568</td>\n",
       "      <td>0.929640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>0.195903</td>\n",
       "      <td>0.923592</td>\n",
       "      <td>0.923592</td>\n",
       "      <td>0.928485</td>\n",
       "      <td>0.923307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.210948</td>\n",
       "      <td>0.934493</td>\n",
       "      <td>0.934493</td>\n",
       "      <td>0.935555</td>\n",
       "      <td>0.934464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:27:58,545] Trial 5 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='2730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/2730 02:43 < 04:44, 6.09 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.421591</td>\n",
       "      <td>0.811181</td>\n",
       "      <td>0.811181</td>\n",
       "      <td>0.849094</td>\n",
       "      <td>0.799006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.236395</td>\n",
       "      <td>0.916292</td>\n",
       "      <td>0.916292</td>\n",
       "      <td>0.919214</td>\n",
       "      <td>0.916084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>0.201488</td>\n",
       "      <td>0.929793</td>\n",
       "      <td>0.929793</td>\n",
       "      <td>0.930568</td>\n",
       "      <td>0.929640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>0.195903</td>\n",
       "      <td>0.923592</td>\n",
       "      <td>0.923592</td>\n",
       "      <td>0.928485</td>\n",
       "      <td>0.923307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.210948</td>\n",
       "      <td>0.934493</td>\n",
       "      <td>0.934493</td>\n",
       "      <td>0.935555</td>\n",
       "      <td>0.934464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:30:43,660] Trial 6 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='4550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/4550 02:44 < 09:45, 6.07 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.661620</td>\n",
       "      <td>0.658066</td>\n",
       "      <td>0.658066</td>\n",
       "      <td>0.659200</td>\n",
       "      <td>0.547713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.247155</td>\n",
       "      <td>0.910791</td>\n",
       "      <td>0.910791</td>\n",
       "      <td>0.911932</td>\n",
       "      <td>0.911265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.203672</td>\n",
       "      <td>0.926693</td>\n",
       "      <td>0.926693</td>\n",
       "      <td>0.928743</td>\n",
       "      <td>0.926445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.211886</td>\n",
       "      <td>0.924892</td>\n",
       "      <td>0.924892</td>\n",
       "      <td>0.928839</td>\n",
       "      <td>0.924667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.211908</td>\n",
       "      <td>0.931793</td>\n",
       "      <td>0.931793</td>\n",
       "      <td>0.934177</td>\n",
       "      <td>0.931588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:33:29,292] Trial 7 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1820' max='1820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1820/1820 04:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.469518</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>0.831569</td>\n",
       "      <td>0.684342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.230421</td>\n",
       "      <td>0.921692</td>\n",
       "      <td>0.921692</td>\n",
       "      <td>0.922456</td>\n",
       "      <td>0.921592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.400900</td>\n",
       "      <td>0.195518</td>\n",
       "      <td>0.927893</td>\n",
       "      <td>0.927893</td>\n",
       "      <td>0.930097</td>\n",
       "      <td>0.927742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.400900</td>\n",
       "      <td>0.198819</td>\n",
       "      <td>0.935594</td>\n",
       "      <td>0.935594</td>\n",
       "      <td>0.938158</td>\n",
       "      <td>0.935490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.208504</td>\n",
       "      <td>0.935594</td>\n",
       "      <td>0.935594</td>\n",
       "      <td>0.938557</td>\n",
       "      <td>0.935567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.199093</td>\n",
       "      <td>0.940294</td>\n",
       "      <td>0.940294</td>\n",
       "      <td>0.941006</td>\n",
       "      <td>0.940311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.198199</td>\n",
       "      <td>0.940694</td>\n",
       "      <td>0.940694</td>\n",
       "      <td>0.942118</td>\n",
       "      <td>0.940634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.234338</td>\n",
       "      <td>0.934093</td>\n",
       "      <td>0.934093</td>\n",
       "      <td>0.937923</td>\n",
       "      <td>0.933980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.207779</td>\n",
       "      <td>0.939494</td>\n",
       "      <td>0.939494</td>\n",
       "      <td>0.942063</td>\n",
       "      <td>0.939446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:38:27,071] Trial 8 finished with value: 3.7604967047778217 and parameters: {'learning_rate': 3e-05, 'num_train_epochs': 2}. Best is trial 0 with value: 3.7657467255346386.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1820' max='1820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1820/1820 04:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.469518</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>0.831569</td>\n",
       "      <td>0.684342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.230421</td>\n",
       "      <td>0.921692</td>\n",
       "      <td>0.921692</td>\n",
       "      <td>0.922456</td>\n",
       "      <td>0.921592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.400900</td>\n",
       "      <td>0.195518</td>\n",
       "      <td>0.927893</td>\n",
       "      <td>0.927893</td>\n",
       "      <td>0.930097</td>\n",
       "      <td>0.927742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.400900</td>\n",
       "      <td>0.198819</td>\n",
       "      <td>0.935594</td>\n",
       "      <td>0.935594</td>\n",
       "      <td>0.938158</td>\n",
       "      <td>0.935490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.208504</td>\n",
       "      <td>0.935594</td>\n",
       "      <td>0.935594</td>\n",
       "      <td>0.938557</td>\n",
       "      <td>0.935567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.199093</td>\n",
       "      <td>0.940294</td>\n",
       "      <td>0.940294</td>\n",
       "      <td>0.941006</td>\n",
       "      <td>0.940311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.198199</td>\n",
       "      <td>0.940694</td>\n",
       "      <td>0.940694</td>\n",
       "      <td>0.942118</td>\n",
       "      <td>0.940634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.234338</td>\n",
       "      <td>0.934093</td>\n",
       "      <td>0.934093</td>\n",
       "      <td>0.937923</td>\n",
       "      <td>0.933980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.207779</td>\n",
       "      <td>0.939494</td>\n",
       "      <td>0.939494</td>\n",
       "      <td>0.942063</td>\n",
       "      <td>0.939446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 14:43:25,487] Trial 9 finished with value: 3.7604967047778217 and parameters: {'learning_rate': 3e-05, 'num_train_epochs': 2}. Best is trial 0 with value: 3.7657467255346386.\n"
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    n_trials=10, \n",
    "    hp_space=optuna_hp_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a73ebc-2b56-45fd-93e8-c7f8caf940c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='0', objective=3.7657467255346386, hyperparameters={'learning_rate': 3e-05, 'num_train_epochs': 3}, run_summary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94567a32-b8bc-47b6-a670-1235f2784399",
   "metadata": {},
   "source": [
    "## Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b313c5e4-bd9f-41d2-a073-223e41db8f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13638' max='13638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13638/13638 38:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.724722</td>\n",
       "      <td>0.639464</td>\n",
       "      <td>0.639464</td>\n",
       "      <td>0.483425</td>\n",
       "      <td>0.532786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.594086</td>\n",
       "      <td>0.658666</td>\n",
       "      <td>0.658666</td>\n",
       "      <td>0.486251</td>\n",
       "      <td>0.546110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.600800</td>\n",
       "      <td>0.378740</td>\n",
       "      <td>0.851985</td>\n",
       "      <td>0.851985</td>\n",
       "      <td>0.881005</td>\n",
       "      <td>0.847920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.600800</td>\n",
       "      <td>0.220865</td>\n",
       "      <td>0.916392</td>\n",
       "      <td>0.916392</td>\n",
       "      <td>0.918620</td>\n",
       "      <td>0.915787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.237452</td>\n",
       "      <td>0.915392</td>\n",
       "      <td>0.915392</td>\n",
       "      <td>0.922580</td>\n",
       "      <td>0.914607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.215078</td>\n",
       "      <td>0.926193</td>\n",
       "      <td>0.926193</td>\n",
       "      <td>0.928079</td>\n",
       "      <td>0.926313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.197244</td>\n",
       "      <td>0.926193</td>\n",
       "      <td>0.926193</td>\n",
       "      <td>0.929311</td>\n",
       "      <td>0.925924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.227010</td>\n",
       "      <td>0.926693</td>\n",
       "      <td>0.926693</td>\n",
       "      <td>0.930919</td>\n",
       "      <td>0.926455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.186896</td>\n",
       "      <td>0.935994</td>\n",
       "      <td>0.935994</td>\n",
       "      <td>0.936083</td>\n",
       "      <td>0.936036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.185183</td>\n",
       "      <td>0.931593</td>\n",
       "      <td>0.931593</td>\n",
       "      <td>0.934586</td>\n",
       "      <td>0.931380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.195922</td>\n",
       "      <td>0.934893</td>\n",
       "      <td>0.934893</td>\n",
       "      <td>0.938751</td>\n",
       "      <td>0.934802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.249444</td>\n",
       "      <td>0.923692</td>\n",
       "      <td>0.923692</td>\n",
       "      <td>0.931118</td>\n",
       "      <td>0.923214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.215578</td>\n",
       "      <td>0.926493</td>\n",
       "      <td>0.926493</td>\n",
       "      <td>0.932800</td>\n",
       "      <td>0.926379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.191005</td>\n",
       "      <td>0.937194</td>\n",
       "      <td>0.937194</td>\n",
       "      <td>0.939877</td>\n",
       "      <td>0.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>0.942913</td>\n",
       "      <td>0.941055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.171061</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.943025</td>\n",
       "      <td>0.940539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.199635</td>\n",
       "      <td>0.932393</td>\n",
       "      <td>0.932393</td>\n",
       "      <td>0.937687</td>\n",
       "      <td>0.932091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.155100</td>\n",
       "      <td>0.159995</td>\n",
       "      <td>0.942394</td>\n",
       "      <td>0.942394</td>\n",
       "      <td>0.944760</td>\n",
       "      <td>0.942378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.155100</td>\n",
       "      <td>0.176127</td>\n",
       "      <td>0.940794</td>\n",
       "      <td>0.940794</td>\n",
       "      <td>0.942821</td>\n",
       "      <td>0.940729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.199188</td>\n",
       "      <td>0.933093</td>\n",
       "      <td>0.933093</td>\n",
       "      <td>0.938549</td>\n",
       "      <td>0.932830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.169405</td>\n",
       "      <td>0.942794</td>\n",
       "      <td>0.942794</td>\n",
       "      <td>0.945465</td>\n",
       "      <td>0.942694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.188054</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>0.944222</td>\n",
       "      <td>0.942010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.147700</td>\n",
       "      <td>0.168174</td>\n",
       "      <td>0.946395</td>\n",
       "      <td>0.946395</td>\n",
       "      <td>0.947783</td>\n",
       "      <td>0.946332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.147700</td>\n",
       "      <td>0.190484</td>\n",
       "      <td>0.944094</td>\n",
       "      <td>0.944094</td>\n",
       "      <td>0.946427</td>\n",
       "      <td>0.944036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.180514</td>\n",
       "      <td>0.946795</td>\n",
       "      <td>0.946795</td>\n",
       "      <td>0.947888</td>\n",
       "      <td>0.946708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.174917</td>\n",
       "      <td>0.949095</td>\n",
       "      <td>0.949095</td>\n",
       "      <td>0.949561</td>\n",
       "      <td>0.949007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.183878</td>\n",
       "      <td>0.948195</td>\n",
       "      <td>0.948195</td>\n",
       "      <td>0.949303</td>\n",
       "      <td>0.948132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>0.200771</td>\n",
       "      <td>0.937394</td>\n",
       "      <td>0.937394</td>\n",
       "      <td>0.941345</td>\n",
       "      <td>0.937271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>0.183286</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>0.944706</td>\n",
       "      <td>0.941895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.245694</td>\n",
       "      <td>0.935494</td>\n",
       "      <td>0.935494</td>\n",
       "      <td>0.940501</td>\n",
       "      <td>0.935413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.159207</td>\n",
       "      <td>0.949095</td>\n",
       "      <td>0.949095</td>\n",
       "      <td>0.949830</td>\n",
       "      <td>0.949111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.197995</td>\n",
       "      <td>0.945195</td>\n",
       "      <td>0.945195</td>\n",
       "      <td>0.946967</td>\n",
       "      <td>0.945152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.190713</td>\n",
       "      <td>0.947595</td>\n",
       "      <td>0.947595</td>\n",
       "      <td>0.949129</td>\n",
       "      <td>0.947479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.158802</td>\n",
       "      <td>0.948895</td>\n",
       "      <td>0.948895</td>\n",
       "      <td>0.950470</td>\n",
       "      <td>0.948835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.160298</td>\n",
       "      <td>0.947895</td>\n",
       "      <td>0.947895</td>\n",
       "      <td>0.949356</td>\n",
       "      <td>0.947837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.157847</td>\n",
       "      <td>0.949695</td>\n",
       "      <td>0.949695</td>\n",
       "      <td>0.950723</td>\n",
       "      <td>0.949747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.194154</td>\n",
       "      <td>0.946095</td>\n",
       "      <td>0.946095</td>\n",
       "      <td>0.948319</td>\n",
       "      <td>0.946027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.185376</td>\n",
       "      <td>0.947095</td>\n",
       "      <td>0.947095</td>\n",
       "      <td>0.948991</td>\n",
       "      <td>0.946918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.183457</td>\n",
       "      <td>0.946795</td>\n",
       "      <td>0.946795</td>\n",
       "      <td>0.948628</td>\n",
       "      <td>0.946796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.151509</td>\n",
       "      <td>0.948595</td>\n",
       "      <td>0.948595</td>\n",
       "      <td>0.950084</td>\n",
       "      <td>0.948554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.164470</td>\n",
       "      <td>0.950595</td>\n",
       "      <td>0.950595</td>\n",
       "      <td>0.951496</td>\n",
       "      <td>0.950671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.208458</td>\n",
       "      <td>0.945995</td>\n",
       "      <td>0.945995</td>\n",
       "      <td>0.948346</td>\n",
       "      <td>0.945875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.169173</td>\n",
       "      <td>0.948495</td>\n",
       "      <td>0.948495</td>\n",
       "      <td>0.950319</td>\n",
       "      <td>0.948448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.184044</td>\n",
       "      <td>0.949595</td>\n",
       "      <td>0.949595</td>\n",
       "      <td>0.951002</td>\n",
       "      <td>0.949578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.164497</td>\n",
       "      <td>0.949295</td>\n",
       "      <td>0.949295</td>\n",
       "      <td>0.950715</td>\n",
       "      <td>0.949275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.191744</td>\n",
       "      <td>0.950095</td>\n",
       "      <td>0.950095</td>\n",
       "      <td>0.951536</td>\n",
       "      <td>0.950078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.198687</td>\n",
       "      <td>0.946995</td>\n",
       "      <td>0.946995</td>\n",
       "      <td>0.949222</td>\n",
       "      <td>0.946944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.200785</td>\n",
       "      <td>0.951895</td>\n",
       "      <td>0.951895</td>\n",
       "      <td>0.953120</td>\n",
       "      <td>0.951874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.213886</td>\n",
       "      <td>0.947695</td>\n",
       "      <td>0.947695</td>\n",
       "      <td>0.949923</td>\n",
       "      <td>0.947688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.207960</td>\n",
       "      <td>0.952095</td>\n",
       "      <td>0.952095</td>\n",
       "      <td>0.953229</td>\n",
       "      <td>0.952103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.206016</td>\n",
       "      <td>0.947995</td>\n",
       "      <td>0.947995</td>\n",
       "      <td>0.949998</td>\n",
       "      <td>0.947995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.181057</td>\n",
       "      <td>0.951595</td>\n",
       "      <td>0.951595</td>\n",
       "      <td>0.952733</td>\n",
       "      <td>0.951560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.217855</td>\n",
       "      <td>0.947895</td>\n",
       "      <td>0.947895</td>\n",
       "      <td>0.949963</td>\n",
       "      <td>0.947841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.217012</td>\n",
       "      <td>0.947395</td>\n",
       "      <td>0.947395</td>\n",
       "      <td>0.949350</td>\n",
       "      <td>0.947472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.201147</td>\n",
       "      <td>0.952295</td>\n",
       "      <td>0.952295</td>\n",
       "      <td>0.953132</td>\n",
       "      <td>0.952327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.199624</td>\n",
       "      <td>0.950895</td>\n",
       "      <td>0.950895</td>\n",
       "      <td>0.952254</td>\n",
       "      <td>0.950851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.203163</td>\n",
       "      <td>0.951195</td>\n",
       "      <td>0.951195</td>\n",
       "      <td>0.952208</td>\n",
       "      <td>0.951153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.208306</td>\n",
       "      <td>0.950595</td>\n",
       "      <td>0.950595</td>\n",
       "      <td>0.951869</td>\n",
       "      <td>0.950591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.201082</td>\n",
       "      <td>0.952195</td>\n",
       "      <td>0.952195</td>\n",
       "      <td>0.953035</td>\n",
       "      <td>0.952159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.191787</td>\n",
       "      <td>0.950995</td>\n",
       "      <td>0.950995</td>\n",
       "      <td>0.952103</td>\n",
       "      <td>0.950974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.193604</td>\n",
       "      <td>0.951895</td>\n",
       "      <td>0.951895</td>\n",
       "      <td>0.952895</td>\n",
       "      <td>0.951858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.208782</td>\n",
       "      <td>0.949795</td>\n",
       "      <td>0.949795</td>\n",
       "      <td>0.951309</td>\n",
       "      <td>0.949711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.187352</td>\n",
       "      <td>0.953095</td>\n",
       "      <td>0.953095</td>\n",
       "      <td>0.954033</td>\n",
       "      <td>0.953066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.191086</td>\n",
       "      <td>0.950995</td>\n",
       "      <td>0.950995</td>\n",
       "      <td>0.952469</td>\n",
       "      <td>0.950950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.191096</td>\n",
       "      <td>0.951395</td>\n",
       "      <td>0.951395</td>\n",
       "      <td>0.952727</td>\n",
       "      <td>0.951357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.185360</td>\n",
       "      <td>0.952695</td>\n",
       "      <td>0.952695</td>\n",
       "      <td>0.953785</td>\n",
       "      <td>0.952669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.186561</td>\n",
       "      <td>0.951795</td>\n",
       "      <td>0.951795</td>\n",
       "      <td>0.953114</td>\n",
       "      <td>0.951759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.188599</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.953053</td>\n",
       "      <td>0.951663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13638, training_loss=0.13386852515025263, metrics={'train_runtime': 2303.3965, 'train_samples_per_second': 189.436, 'train_steps_per_second': 5.921, 'total_flos': 5.8014335353860776e+16, 'train_loss': 0.13386852515025263, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\",\n",
    "    tf32=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "    \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c033e6c-1341-4ddd-9c81-10f456146e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_store_path.parent / (model_store_path.stem + \".out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fc1ce-86d3-4168-a4a1-71f521fe37a3",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b699f7bb-e4c9-45ad-9fd1-d7425dd6f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7590-31cc-40ad-b212-d0c47500277f",
   "metadata": {},
   "source": [
    "## Test on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdfafa9a-6265-4792-8513-149ab1b0545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"validation\"])\n",
    "val = generate_doc_df(data[\"validation\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c16081ff-6aa6-4620-8cdf-50f3e07b73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       1.00      1.00      1.00      3333\n",
      "        REFUTES       0.96      0.90      0.93      3333\n",
      "       SUPPORTS       0.91      0.96      0.93      3333\n",
      "\n",
      "       accuracy                           0.95      9999\n",
      "      macro avg       0.95      0.95      0.95      9999\n",
      "   weighted avg       0.95      0.95      0.95      9999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=val[\"actual\"], y_pred=val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d4cf6-3cfe-458f-9ad7-e58994554cbc",
   "metadata": {},
   "source": [
    "## Test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b9b87e1-0054-4bbe-a6e8-b76867f03b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"test\"])\n",
    "tes = generate_doc_df(data[\"test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6548fc2f-a81f-4e65-b697-0b9e80ba9402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       1.00      1.00      1.00      3333\n",
      "        REFUTES       0.95      0.87      0.91      3333\n",
      "       SUPPORTS       0.88      0.95      0.92      3333\n",
      "\n",
      "       accuracy                           0.94      9999\n",
      "      macro avg       0.94      0.94      0.94      9999\n",
      "   weighted avg       0.94      0.94      0.94      9999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=tes[\"actual\"], y_pred=tes[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae0ef8-63c5-4092-b9a3-1528fe23a2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
