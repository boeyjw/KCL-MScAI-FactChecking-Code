{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5aa8e9-34d2-4f68-8626-8b7809c69b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from gen.util import read_data, write_jsonl\n",
    "from rte.aggregate import generate_micro_macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505a8cc8-af95-4872-b93d-35d6a551547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = Path(\"../../data\").resolve()\n",
    "root_model = Path(\"../../models\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1929c84-28f4-4949-b880-a85b8f8b8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "LOOKUP = {\n",
    "    \"verifiable\": {\"no\": \"NOT VERIFIABLE\", \"yes\": \"VERIFIABLE\"},\n",
    "    \"label\": {\"nei\": \"NOT ENOUGH INFO\", \"r\": \"REFUTES\", \"s\": \"SUPPORTS\"}\n",
    "}\n",
    "\n",
    "SEED = 123456789\n",
    "\n",
    "LABEL2ID = {\"SUPPORTS\": 0, \"NOT ENOUGH INFO\": 1, \"REFUTES\": 2}\n",
    "ID2LABEL = {0: \"SUPPORTS\", 1: \"NOT ENOUGH INFO\", 2: \"REFUTES\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05b8ab-6775-4eed-98be-10904943ded1",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71aacd87-5b38-45b8-8be3-f7046bfa06be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/mainenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    TextClassificationPipeline,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd986a-1e45-4f38-8487-dddf2ee3ef24",
   "metadata": {},
   "source": [
    "# Huggingface Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b42b8e-2eb3-444b-9275-192519540439",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e5281d-7e7d-410e-b1ba-730e334247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae2c86dd-cb1b-4b77-836d-1e5619f4bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        num_labels=3, \n",
    "        id2label=ID2LABEL, \n",
    "        label2id=LABEL2ID\n",
    "    )\n",
    "\n",
    "model = model_init()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, do_lower_case=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"evidence\"], examples[\"claim\"], max_length=512, truncation=\"only_first\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d4834e0-770d-49dc-8041-f4d9d2729053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"fever\", \"climatefeverpure\", \"fever-climatefeverpure\", \"climatefever\", \"fever-climatefever\"]\n",
    "doc_sent = [\"doc\", \"sent\"]\n",
    "\n",
    "di = 0\n",
    "ds = 1\n",
    "\n",
    "model_store_path = root_model.joinpath(model_checkpoint)\n",
    "model_store_path.mkdir(exist_ok=True)\n",
    "model_store_path = model_store_path / f\"{dataset[di]}-{model_checkpoint}-{doc_sent[ds]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940210d7-1da5-4e6f-b4f6-95304ceb496b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a618a5-0c63-4a99-b7cb-89deea2416f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    }
   ],
   "source": [
    "datap = root_data / f\"{doc_sent[ds]}-dataset\"\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.train.n5.jsonl\")),\n",
    "    \"validation\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.dev.n5.jsonl\")),\n",
    "    \"test\": Dataset.from_list(read_data(datap / f\"{dataset[di]}.test.n5.jsonl\"))\n",
    "}).map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d7f9a-4ddc-4a9b-b6cf-d31233fff94b",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06cfa045-016f-44e2-91c9-567ddd56ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "epoch = 4\n",
    "metric_name = \"f1\"\n",
    "warmup_ratio=0.1\n",
    "save_steps=200\n",
    "eval_steps=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666373d-06fa-476c-9aaf-ffb3daadbf4b",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967aecb0-5bc7-488c-aefa-caf4525216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shard the data if the dataset is large for hyperparameter tuning\n",
    "shard = data[\"train\"].num_rows > 50000\n",
    "hp_tune_train = data[\"train\"].shuffle(seed=SEED).shard(num_shards=5, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e23d248-bb05-43da-a056-4e1fcc2a17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy = \"no\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "_ = model.train()\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=hp_tune_train if shard else data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5445d33-24fc-4a6d-8ed2-a89b26aa65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-5, 3e-5, 2e-5]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "    }\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d4edde-28e8-4b30-a700-dc3f4a2a2ce9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-02 22:45:06,097] A new study created in memory with name: no-name-37d4434d-1dee-4279-87ed-5a4253f73215\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6985' max='6985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6985/6985 26:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.661240</td>\n",
       "      <td>0.673770</td>\n",
       "      <td>0.643054</td>\n",
       "      <td>0.460897</td>\n",
       "      <td>0.527280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.539173</td>\n",
       "      <td>0.770103</td>\n",
       "      <td>0.755476</td>\n",
       "      <td>0.808334</td>\n",
       "      <td>0.726822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.268429</td>\n",
       "      <td>0.904721</td>\n",
       "      <td>0.898588</td>\n",
       "      <td>0.898730</td>\n",
       "      <td>0.896791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.254028</td>\n",
       "      <td>0.913518</td>\n",
       "      <td>0.907115</td>\n",
       "      <td>0.907062</td>\n",
       "      <td>0.906717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.233321</td>\n",
       "      <td>0.920022</td>\n",
       "      <td>0.913982</td>\n",
       "      <td>0.916712</td>\n",
       "      <td>0.913144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.277065</td>\n",
       "      <td>0.909491</td>\n",
       "      <td>0.902535</td>\n",
       "      <td>0.911353</td>\n",
       "      <td>0.900359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.231667</td>\n",
       "      <td>0.924854</td>\n",
       "      <td>0.918203</td>\n",
       "      <td>0.922928</td>\n",
       "      <td>0.917754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.195200</td>\n",
       "      <td>0.267987</td>\n",
       "      <td>0.923182</td>\n",
       "      <td>0.922281</td>\n",
       "      <td>0.917324</td>\n",
       "      <td>0.919027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.195200</td>\n",
       "      <td>0.249193</td>\n",
       "      <td>0.927642</td>\n",
       "      <td>0.923455</td>\n",
       "      <td>0.923442</td>\n",
       "      <td>0.921896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.148300</td>\n",
       "      <td>0.275984</td>\n",
       "      <td>0.920022</td>\n",
       "      <td>0.914826</td>\n",
       "      <td>0.918875</td>\n",
       "      <td>0.912893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.148300</td>\n",
       "      <td>0.251378</td>\n",
       "      <td>0.925598</td>\n",
       "      <td>0.920464</td>\n",
       "      <td>0.922866</td>\n",
       "      <td>0.918843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.148300</td>\n",
       "      <td>0.211757</td>\n",
       "      <td>0.936439</td>\n",
       "      <td>0.932311</td>\n",
       "      <td>0.931482</td>\n",
       "      <td>0.931359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.205706</td>\n",
       "      <td>0.937802</td>\n",
       "      <td>0.932494</td>\n",
       "      <td>0.934213</td>\n",
       "      <td>0.932280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.204469</td>\n",
       "      <td>0.941023</td>\n",
       "      <td>0.936674</td>\n",
       "      <td>0.936867</td>\n",
       "      <td>0.936254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.285891</td>\n",
       "      <td>0.934147</td>\n",
       "      <td>0.930715</td>\n",
       "      <td>0.929642</td>\n",
       "      <td>0.929137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.297172</td>\n",
       "      <td>0.924235</td>\n",
       "      <td>0.920545</td>\n",
       "      <td>0.921071</td>\n",
       "      <td>0.918099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.240521</td>\n",
       "      <td>0.939289</td>\n",
       "      <td>0.934335</td>\n",
       "      <td>0.935482</td>\n",
       "      <td>0.934010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.285508</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>0.929150</td>\n",
       "      <td>0.930540</td>\n",
       "      <td>0.928173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.264104</td>\n",
       "      <td>0.933465</td>\n",
       "      <td>0.929444</td>\n",
       "      <td>0.930199</td>\n",
       "      <td>0.928061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.274188</td>\n",
       "      <td>0.933713</td>\n",
       "      <td>0.930151</td>\n",
       "      <td>0.929086</td>\n",
       "      <td>0.928477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.298271</td>\n",
       "      <td>0.929872</td>\n",
       "      <td>0.926589</td>\n",
       "      <td>0.925704</td>\n",
       "      <td>0.924393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.295440</td>\n",
       "      <td>0.935448</td>\n",
       "      <td>0.931188</td>\n",
       "      <td>0.931307</td>\n",
       "      <td>0.930001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.293060</td>\n",
       "      <td>0.933342</td>\n",
       "      <td>0.929130</td>\n",
       "      <td>0.929697</td>\n",
       "      <td>0.927657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.326783</td>\n",
       "      <td>0.933589</td>\n",
       "      <td>0.928753</td>\n",
       "      <td>0.930427</td>\n",
       "      <td>0.927609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.310562</td>\n",
       "      <td>0.934333</td>\n",
       "      <td>0.930340</td>\n",
       "      <td>0.930617</td>\n",
       "      <td>0.928867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.295054</td>\n",
       "      <td>0.937988</td>\n",
       "      <td>0.933788</td>\n",
       "      <td>0.933843</td>\n",
       "      <td>0.932786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.318311</td>\n",
       "      <td>0.938607</td>\n",
       "      <td>0.933756</td>\n",
       "      <td>0.935271</td>\n",
       "      <td>0.933229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.311558</td>\n",
       "      <td>0.934704</td>\n",
       "      <td>0.930457</td>\n",
       "      <td>0.931222</td>\n",
       "      <td>0.929128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.320580</td>\n",
       "      <td>0.939722</td>\n",
       "      <td>0.935292</td>\n",
       "      <td>0.935829</td>\n",
       "      <td>0.934514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.338040</td>\n",
       "      <td>0.936129</td>\n",
       "      <td>0.932669</td>\n",
       "      <td>0.931557</td>\n",
       "      <td>0.931080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.345758</td>\n",
       "      <td>0.935262</td>\n",
       "      <td>0.930944</td>\n",
       "      <td>0.931770</td>\n",
       "      <td>0.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.340712</td>\n",
       "      <td>0.936377</td>\n",
       "      <td>0.932373</td>\n",
       "      <td>0.932301</td>\n",
       "      <td>0.931112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.352072</td>\n",
       "      <td>0.936129</td>\n",
       "      <td>0.931722</td>\n",
       "      <td>0.932580</td>\n",
       "      <td>0.930629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.356423</td>\n",
       "      <td>0.935200</td>\n",
       "      <td>0.931040</td>\n",
       "      <td>0.931567</td>\n",
       "      <td>0.929730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-02 23:11:37,614] Trial 0 finished with value: 3.7275370030572534 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 5}. Best is trial 0 with value: 3.7275370030572534.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5588' max='5588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5588/5588 15:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.713938</td>\n",
       "      <td>0.656424</td>\n",
       "      <td>0.630072</td>\n",
       "      <td>0.459434</td>\n",
       "      <td>0.517025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.572352</td>\n",
       "      <td>0.710569</td>\n",
       "      <td>0.682192</td>\n",
       "      <td>0.724736</td>\n",
       "      <td>0.613365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.608900</td>\n",
       "      <td>0.369273</td>\n",
       "      <td>0.856895</td>\n",
       "      <td>0.843793</td>\n",
       "      <td>0.868721</td>\n",
       "      <td>0.836504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.608900</td>\n",
       "      <td>0.463622</td>\n",
       "      <td>0.851010</td>\n",
       "      <td>0.842995</td>\n",
       "      <td>0.868047</td>\n",
       "      <td>0.835117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.266537</td>\n",
       "      <td>0.909491</td>\n",
       "      <td>0.906530</td>\n",
       "      <td>0.902463</td>\n",
       "      <td>0.904010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.254050</td>\n",
       "      <td>0.915500</td>\n",
       "      <td>0.910151</td>\n",
       "      <td>0.909995</td>\n",
       "      <td>0.908843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.299564</td>\n",
       "      <td>0.909057</td>\n",
       "      <td>0.902846</td>\n",
       "      <td>0.909395</td>\n",
       "      <td>0.900416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.281428</td>\n",
       "      <td>0.918102</td>\n",
       "      <td>0.915227</td>\n",
       "      <td>0.913254</td>\n",
       "      <td>0.912469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.311625</td>\n",
       "      <td>0.908376</td>\n",
       "      <td>0.907120</td>\n",
       "      <td>0.906291</td>\n",
       "      <td>0.902925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.275113</td>\n",
       "      <td>0.916677</td>\n",
       "      <td>0.910674</td>\n",
       "      <td>0.916798</td>\n",
       "      <td>0.908790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.252425</td>\n",
       "      <td>0.929005</td>\n",
       "      <td>0.925050</td>\n",
       "      <td>0.925249</td>\n",
       "      <td>0.923468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.279766</td>\n",
       "      <td>0.919031</td>\n",
       "      <td>0.914461</td>\n",
       "      <td>0.917664</td>\n",
       "      <td>0.912214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.211146</td>\n",
       "      <td>0.936191</td>\n",
       "      <td>0.932823</td>\n",
       "      <td>0.930958</td>\n",
       "      <td>0.931641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.237470</td>\n",
       "      <td>0.929687</td>\n",
       "      <td>0.924047</td>\n",
       "      <td>0.927499</td>\n",
       "      <td>0.923269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.250597</td>\n",
       "      <td>0.936191</td>\n",
       "      <td>0.931882</td>\n",
       "      <td>0.932600</td>\n",
       "      <td>0.930884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.261250</td>\n",
       "      <td>0.936997</td>\n",
       "      <td>0.932320</td>\n",
       "      <td>0.933401</td>\n",
       "      <td>0.931725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.271992</td>\n",
       "      <td>0.933961</td>\n",
       "      <td>0.929621</td>\n",
       "      <td>0.930801</td>\n",
       "      <td>0.928528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.250511</td>\n",
       "      <td>0.936811</td>\n",
       "      <td>0.932437</td>\n",
       "      <td>0.932936</td>\n",
       "      <td>0.931503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.284700</td>\n",
       "      <td>0.929067</td>\n",
       "      <td>0.924346</td>\n",
       "      <td>0.926635</td>\n",
       "      <td>0.922797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.278845</td>\n",
       "      <td>0.932165</td>\n",
       "      <td>0.928666</td>\n",
       "      <td>0.928751</td>\n",
       "      <td>0.926841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.295585</td>\n",
       "      <td>0.931545</td>\n",
       "      <td>0.928834</td>\n",
       "      <td>0.927699</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.313936</td>\n",
       "      <td>0.928014</td>\n",
       "      <td>0.924344</td>\n",
       "      <td>0.925953</td>\n",
       "      <td>0.922164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.249022</td>\n",
       "      <td>0.937740</td>\n",
       "      <td>0.933402</td>\n",
       "      <td>0.933896</td>\n",
       "      <td>0.932589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.265458</td>\n",
       "      <td>0.935076</td>\n",
       "      <td>0.931129</td>\n",
       "      <td>0.931763</td>\n",
       "      <td>0.929642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.254932</td>\n",
       "      <td>0.938298</td>\n",
       "      <td>0.934614</td>\n",
       "      <td>0.934254</td>\n",
       "      <td>0.933399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.255172</td>\n",
       "      <td>0.938669</td>\n",
       "      <td>0.935127</td>\n",
       "      <td>0.934675</td>\n",
       "      <td>0.933838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.253059</td>\n",
       "      <td>0.938422</td>\n",
       "      <td>0.934236</td>\n",
       "      <td>0.934972</td>\n",
       "      <td>0.933270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-02 23:26:59,684] Trial 1 finished with value: 3.7408988111774795 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}. Best is trial 1 with value: 3.7408988111774795.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6985' max='6985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6985/6985 26:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.698922</td>\n",
       "      <td>0.664602</td>\n",
       "      <td>0.636219</td>\n",
       "      <td>0.459870</td>\n",
       "      <td>0.521839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.566699</td>\n",
       "      <td>0.688638</td>\n",
       "      <td>0.660647</td>\n",
       "      <td>0.738482</td>\n",
       "      <td>0.558522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.336267</td>\n",
       "      <td>0.874799</td>\n",
       "      <td>0.867402</td>\n",
       "      <td>0.866054</td>\n",
       "      <td>0.865417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.266552</td>\n",
       "      <td>0.903853</td>\n",
       "      <td>0.897167</td>\n",
       "      <td>0.899918</td>\n",
       "      <td>0.895026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.232260</td>\n",
       "      <td>0.921323</td>\n",
       "      <td>0.915652</td>\n",
       "      <td>0.916754</td>\n",
       "      <td>0.914432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.257161</td>\n",
       "      <td>0.912960</td>\n",
       "      <td>0.906940</td>\n",
       "      <td>0.913312</td>\n",
       "      <td>0.904523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.225788</td>\n",
       "      <td>0.925226</td>\n",
       "      <td>0.919257</td>\n",
       "      <td>0.922695</td>\n",
       "      <td>0.918457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.242691</td>\n",
       "      <td>0.927580</td>\n",
       "      <td>0.925277</td>\n",
       "      <td>0.921837</td>\n",
       "      <td>0.923007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.214552</td>\n",
       "      <td>0.934952</td>\n",
       "      <td>0.930160</td>\n",
       "      <td>0.930326</td>\n",
       "      <td>0.929989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.231514</td>\n",
       "      <td>0.931483</td>\n",
       "      <td>0.925910</td>\n",
       "      <td>0.928592</td>\n",
       "      <td>0.925475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.274666</td>\n",
       "      <td>0.920394</td>\n",
       "      <td>0.915311</td>\n",
       "      <td>0.919185</td>\n",
       "      <td>0.913187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.205534</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.932220</td>\n",
       "      <td>0.931223</td>\n",
       "      <td>0.931246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>0.225012</td>\n",
       "      <td>0.935572</td>\n",
       "      <td>0.930698</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.930070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>0.229305</td>\n",
       "      <td>0.934952</td>\n",
       "      <td>0.930113</td>\n",
       "      <td>0.931907</td>\n",
       "      <td>0.929255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.272454</td>\n",
       "      <td>0.935696</td>\n",
       "      <td>0.931237</td>\n",
       "      <td>0.931905</td>\n",
       "      <td>0.930257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.288516</td>\n",
       "      <td>0.930244</td>\n",
       "      <td>0.925596</td>\n",
       "      <td>0.927237</td>\n",
       "      <td>0.924009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.264948</td>\n",
       "      <td>0.930987</td>\n",
       "      <td>0.926538</td>\n",
       "      <td>0.927806</td>\n",
       "      <td>0.924996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>0.277796</td>\n",
       "      <td>0.934704</td>\n",
       "      <td>0.930164</td>\n",
       "      <td>0.931188</td>\n",
       "      <td>0.929101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>0.255673</td>\n",
       "      <td>0.934271</td>\n",
       "      <td>0.929731</td>\n",
       "      <td>0.930803</td>\n",
       "      <td>0.928551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.255184</td>\n",
       "      <td>0.938174</td>\n",
       "      <td>0.934514</td>\n",
       "      <td>0.933288</td>\n",
       "      <td>0.933327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.268039</td>\n",
       "      <td>0.938050</td>\n",
       "      <td>0.933488</td>\n",
       "      <td>0.934164</td>\n",
       "      <td>0.932745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.296745</td>\n",
       "      <td>0.934890</td>\n",
       "      <td>0.931146</td>\n",
       "      <td>0.930744</td>\n",
       "      <td>0.929693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.291705</td>\n",
       "      <td>0.936005</td>\n",
       "      <td>0.932014</td>\n",
       "      <td>0.931990</td>\n",
       "      <td>0.930766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.290382</td>\n",
       "      <td>0.936997</td>\n",
       "      <td>0.933248</td>\n",
       "      <td>0.932328</td>\n",
       "      <td>0.932002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.292585</td>\n",
       "      <td>0.939103</td>\n",
       "      <td>0.935184</td>\n",
       "      <td>0.934807</td>\n",
       "      <td>0.934160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.295251</td>\n",
       "      <td>0.937059</td>\n",
       "      <td>0.933738</td>\n",
       "      <td>0.932456</td>\n",
       "      <td>0.932273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.307662</td>\n",
       "      <td>0.936687</td>\n",
       "      <td>0.932118</td>\n",
       "      <td>0.933322</td>\n",
       "      <td>0.931301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.300582</td>\n",
       "      <td>0.937430</td>\n",
       "      <td>0.933682</td>\n",
       "      <td>0.933535</td>\n",
       "      <td>0.932386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.334219</td>\n",
       "      <td>0.935448</td>\n",
       "      <td>0.931725</td>\n",
       "      <td>0.931727</td>\n",
       "      <td>0.930156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.318709</td>\n",
       "      <td>0.936687</td>\n",
       "      <td>0.933187</td>\n",
       "      <td>0.932380</td>\n",
       "      <td>0.931689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.338069</td>\n",
       "      <td>0.934828</td>\n",
       "      <td>0.930621</td>\n",
       "      <td>0.931433</td>\n",
       "      <td>0.929297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.320219</td>\n",
       "      <td>0.938050</td>\n",
       "      <td>0.934019</td>\n",
       "      <td>0.934191</td>\n",
       "      <td>0.932960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.326068</td>\n",
       "      <td>0.937616</td>\n",
       "      <td>0.933900</td>\n",
       "      <td>0.933569</td>\n",
       "      <td>0.932648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.325245</td>\n",
       "      <td>0.937678</td>\n",
       "      <td>0.934028</td>\n",
       "      <td>0.933506</td>\n",
       "      <td>0.932743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-02 23:53:31,273] Trial 2 finished with value: 3.7379546212305836 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 5}. Best is trial 1 with value: 3.7408988111774795.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4191' max='4191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4191/4191 15:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.655917</td>\n",
       "      <td>0.675505</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.461181</td>\n",
       "      <td>0.528326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.544872</td>\n",
       "      <td>0.776732</td>\n",
       "      <td>0.763614</td>\n",
       "      <td>0.820566</td>\n",
       "      <td>0.738623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.556600</td>\n",
       "      <td>0.274210</td>\n",
       "      <td>0.900322</td>\n",
       "      <td>0.893469</td>\n",
       "      <td>0.894359</td>\n",
       "      <td>0.891699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.556600</td>\n",
       "      <td>0.254721</td>\n",
       "      <td>0.910544</td>\n",
       "      <td>0.905301</td>\n",
       "      <td>0.905490</td>\n",
       "      <td>0.903071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.221874</td>\n",
       "      <td>0.923182</td>\n",
       "      <td>0.917806</td>\n",
       "      <td>0.918590</td>\n",
       "      <td>0.916516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.276691</td>\n",
       "      <td>0.902490</td>\n",
       "      <td>0.894537</td>\n",
       "      <td>0.906878</td>\n",
       "      <td>0.891863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.217758</td>\n",
       "      <td>0.926155</td>\n",
       "      <td>0.920352</td>\n",
       "      <td>0.923985</td>\n",
       "      <td>0.919530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>0.237790</td>\n",
       "      <td>0.929129</td>\n",
       "      <td>0.926383</td>\n",
       "      <td>0.924057</td>\n",
       "      <td>0.924209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>0.218685</td>\n",
       "      <td>0.934581</td>\n",
       "      <td>0.931085</td>\n",
       "      <td>0.929502</td>\n",
       "      <td>0.929655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.238658</td>\n",
       "      <td>0.931855</td>\n",
       "      <td>0.926413</td>\n",
       "      <td>0.929039</td>\n",
       "      <td>0.925684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.288441</td>\n",
       "      <td>0.914261</td>\n",
       "      <td>0.908076</td>\n",
       "      <td>0.915626</td>\n",
       "      <td>0.905661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.937306</td>\n",
       "      <td>0.932635</td>\n",
       "      <td>0.933260</td>\n",
       "      <td>0.931956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.209642</td>\n",
       "      <td>0.939289</td>\n",
       "      <td>0.935062</td>\n",
       "      <td>0.935111</td>\n",
       "      <td>0.934286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.215881</td>\n",
       "      <td>0.938979</td>\n",
       "      <td>0.934038</td>\n",
       "      <td>0.935851</td>\n",
       "      <td>0.933678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.258788</td>\n",
       "      <td>0.936873</td>\n",
       "      <td>0.932094</td>\n",
       "      <td>0.933783</td>\n",
       "      <td>0.931230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.259001</td>\n",
       "      <td>0.935076</td>\n",
       "      <td>0.930561</td>\n",
       "      <td>0.932182</td>\n",
       "      <td>0.929352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.250492</td>\n",
       "      <td>0.935944</td>\n",
       "      <td>0.931575</td>\n",
       "      <td>0.932965</td>\n",
       "      <td>0.930356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.242421</td>\n",
       "      <td>0.942386</td>\n",
       "      <td>0.938680</td>\n",
       "      <td>0.938228</td>\n",
       "      <td>0.937796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.240572</td>\n",
       "      <td>0.940590</td>\n",
       "      <td>0.936473</td>\n",
       "      <td>0.937049</td>\n",
       "      <td>0.935646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.082800</td>\n",
       "      <td>0.242683</td>\n",
       "      <td>0.940652</td>\n",
       "      <td>0.936901</td>\n",
       "      <td>0.936744</td>\n",
       "      <td>0.935891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:09:19,991] Trial 3 finished with value: 3.750187554771879 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 3 with value: 3.750187554771879.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5588' max='5588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5588/5588 15:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673817</td>\n",
       "      <td>0.668195</td>\n",
       "      <td>0.639731</td>\n",
       "      <td>0.463461</td>\n",
       "      <td>0.524970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.455397</td>\n",
       "      <td>0.796122</td>\n",
       "      <td>0.777375</td>\n",
       "      <td>0.817276</td>\n",
       "      <td>0.755482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.305834</td>\n",
       "      <td>0.887437</td>\n",
       "      <td>0.877734</td>\n",
       "      <td>0.886879</td>\n",
       "      <td>0.875873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.570284</td>\n",
       "      <td>0.845930</td>\n",
       "      <td>0.840046</td>\n",
       "      <td>0.863845</td>\n",
       "      <td>0.832247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.276413</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>0.901585</td>\n",
       "      <td>0.901045</td>\n",
       "      <td>0.898659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.249472</td>\n",
       "      <td>0.915128</td>\n",
       "      <td>0.907797</td>\n",
       "      <td>0.911679</td>\n",
       "      <td>0.907887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.913456</td>\n",
       "      <td>0.907045</td>\n",
       "      <td>0.912697</td>\n",
       "      <td>0.905462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.286449</td>\n",
       "      <td>0.919960</td>\n",
       "      <td>0.916439</td>\n",
       "      <td>0.916371</td>\n",
       "      <td>0.914175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.276297</td>\n",
       "      <td>0.918226</td>\n",
       "      <td>0.915824</td>\n",
       "      <td>0.914702</td>\n",
       "      <td>0.912837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.271623</td>\n",
       "      <td>0.919775</td>\n",
       "      <td>0.914782</td>\n",
       "      <td>0.917963</td>\n",
       "      <td>0.912968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.263096</td>\n",
       "      <td>0.925908</td>\n",
       "      <td>0.922901</td>\n",
       "      <td>0.920323</td>\n",
       "      <td>0.920881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.281891</td>\n",
       "      <td>0.917544</td>\n",
       "      <td>0.912342</td>\n",
       "      <td>0.917159</td>\n",
       "      <td>0.910140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.197090</td>\n",
       "      <td>0.936873</td>\n",
       "      <td>0.932378</td>\n",
       "      <td>0.932009</td>\n",
       "      <td>0.932135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.251798</td>\n",
       "      <td>0.925660</td>\n",
       "      <td>0.920412</td>\n",
       "      <td>0.924429</td>\n",
       "      <td>0.919027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.262619</td>\n",
       "      <td>0.932474</td>\n",
       "      <td>0.927867</td>\n",
       "      <td>0.929361</td>\n",
       "      <td>0.926665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.301772</td>\n",
       "      <td>0.929439</td>\n",
       "      <td>0.925675</td>\n",
       "      <td>0.926269</td>\n",
       "      <td>0.923938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.311530</td>\n",
       "      <td>0.929439</td>\n",
       "      <td>0.925628</td>\n",
       "      <td>0.926759</td>\n",
       "      <td>0.923839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.270272</td>\n",
       "      <td>0.934209</td>\n",
       "      <td>0.929578</td>\n",
       "      <td>0.930538</td>\n",
       "      <td>0.928576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.273882</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>0.929864</td>\n",
       "      <td>0.930010</td>\n",
       "      <td>0.928574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.281051</td>\n",
       "      <td>0.931483</td>\n",
       "      <td>0.926766</td>\n",
       "      <td>0.929019</td>\n",
       "      <td>0.925728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.310012</td>\n",
       "      <td>0.929253</td>\n",
       "      <td>0.925610</td>\n",
       "      <td>0.926403</td>\n",
       "      <td>0.923708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.324380</td>\n",
       "      <td>0.925226</td>\n",
       "      <td>0.921252</td>\n",
       "      <td>0.923607</td>\n",
       "      <td>0.919007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.261492</td>\n",
       "      <td>0.933218</td>\n",
       "      <td>0.928798</td>\n",
       "      <td>0.929899</td>\n",
       "      <td>0.927529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.280087</td>\n",
       "      <td>0.931979</td>\n",
       "      <td>0.927901</td>\n",
       "      <td>0.929045</td>\n",
       "      <td>0.926323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>0.248641</td>\n",
       "      <td>0.938979</td>\n",
       "      <td>0.934330</td>\n",
       "      <td>0.935399</td>\n",
       "      <td>0.933891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>0.256449</td>\n",
       "      <td>0.939041</td>\n",
       "      <td>0.934902</td>\n",
       "      <td>0.935137</td>\n",
       "      <td>0.934078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>0.253252</td>\n",
       "      <td>0.938422</td>\n",
       "      <td>0.933828</td>\n",
       "      <td>0.935085</td>\n",
       "      <td>0.933262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:24:42,239] Trial 4 finished with value: 3.7405969469052858 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}. Best is trial 3 with value: 3.750187554771879.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='5588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 800/5588 02:14 < 13:28, 5.92 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.663612</td>\n",
       "      <td>0.673089</td>\n",
       "      <td>0.644814</td>\n",
       "      <td>0.468672</td>\n",
       "      <td>0.529360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.434438</td>\n",
       "      <td>0.827283</td>\n",
       "      <td>0.816443</td>\n",
       "      <td>0.842668</td>\n",
       "      <td>0.805802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.527600</td>\n",
       "      <td>0.294412</td>\n",
       "      <td>0.902614</td>\n",
       "      <td>0.896931</td>\n",
       "      <td>0.895339</td>\n",
       "      <td>0.895885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.527600</td>\n",
       "      <td>0.681339</td>\n",
       "      <td>0.822699</td>\n",
       "      <td>0.815111</td>\n",
       "      <td>0.854527</td>\n",
       "      <td>0.803995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:26:58,222] Trial 5 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='4191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/4191 03:50 < 12:17, 4.33 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.614898</td>\n",
       "      <td>0.684735</td>\n",
       "      <td>0.654134</td>\n",
       "      <td>0.701922</td>\n",
       "      <td>0.544411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.449842</td>\n",
       "      <td>0.845372</td>\n",
       "      <td>0.836244</td>\n",
       "      <td>0.866024</td>\n",
       "      <td>0.827624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.248436</td>\n",
       "      <td>0.909862</td>\n",
       "      <td>0.902213</td>\n",
       "      <td>0.906367</td>\n",
       "      <td>0.901514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.248845</td>\n",
       "      <td>0.912898</td>\n",
       "      <td>0.909705</td>\n",
       "      <td>0.907681</td>\n",
       "      <td>0.906767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.231200</td>\n",
       "      <td>0.255283</td>\n",
       "      <td>0.912774</td>\n",
       "      <td>0.905156</td>\n",
       "      <td>0.914047</td>\n",
       "      <td>0.903913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 00:30:50,123] Trial 6 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='6985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/6985 05:22 < 21:28, 4.34 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.621213</td>\n",
       "      <td>0.678974</td>\n",
       "      <td>0.648541</td>\n",
       "      <td>0.799326</td>\n",
       "      <td>0.531972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.551349</td>\n",
       "      <td>0.794635</td>\n",
       "      <td>0.780704</td>\n",
       "      <td>0.835425</td>\n",
       "      <td>0.759192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>0.256910</td>\n",
       "      <td>0.908747</td>\n",
       "      <td>0.901739</td>\n",
       "      <td>0.902338</td>\n",
       "      <td>0.900960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>0.271723</td>\n",
       "      <td>0.908066</td>\n",
       "      <td>0.904481</td>\n",
       "      <td>0.901273</td>\n",
       "      <td>0.902007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.231932</td>\n",
       "      <td>0.921385</td>\n",
       "      <td>0.914403</td>\n",
       "      <td>0.916562</td>\n",
       "      <td>0.914781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.298552</td>\n",
       "      <td>0.908562</td>\n",
       "      <td>0.902318</td>\n",
       "      <td>0.908898</td>\n",
       "      <td>0.899700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.230403</td>\n",
       "      <td>0.924483</td>\n",
       "      <td>0.920150</td>\n",
       "      <td>0.920356</td>\n",
       "      <td>0.918387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 00:36:13,822] Trial 7 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='2794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/2794 09:12 < 01:30, 4.34 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.622583</td>\n",
       "      <td>0.679779</td>\n",
       "      <td>0.649051</td>\n",
       "      <td>0.798456</td>\n",
       "      <td>0.532824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.452273</td>\n",
       "      <td>0.818858</td>\n",
       "      <td>0.805439</td>\n",
       "      <td>0.841408</td>\n",
       "      <td>0.789827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.275460</td>\n",
       "      <td>0.902366</td>\n",
       "      <td>0.897693</td>\n",
       "      <td>0.896143</td>\n",
       "      <td>0.894814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.225697</td>\n",
       "      <td>0.920270</td>\n",
       "      <td>0.914890</td>\n",
       "      <td>0.913836</td>\n",
       "      <td>0.914034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.228013</td>\n",
       "      <td>0.922996</td>\n",
       "      <td>0.916390</td>\n",
       "      <td>0.920638</td>\n",
       "      <td>0.915939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.239188</td>\n",
       "      <td>0.919465</td>\n",
       "      <td>0.913124</td>\n",
       "      <td>0.919309</td>\n",
       "      <td>0.911469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.210516</td>\n",
       "      <td>0.930368</td>\n",
       "      <td>0.925536</td>\n",
       "      <td>0.926960</td>\n",
       "      <td>0.924423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.237313</td>\n",
       "      <td>0.931297</td>\n",
       "      <td>0.928155</td>\n",
       "      <td>0.926392</td>\n",
       "      <td>0.926240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.216780</td>\n",
       "      <td>0.934147</td>\n",
       "      <td>0.929685</td>\n",
       "      <td>0.929635</td>\n",
       "      <td>0.928730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.221484</td>\n",
       "      <td>0.933156</td>\n",
       "      <td>0.928733</td>\n",
       "      <td>0.929291</td>\n",
       "      <td>0.927653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.250972</td>\n",
       "      <td>0.926093</td>\n",
       "      <td>0.920849</td>\n",
       "      <td>0.924173</td>\n",
       "      <td>0.919293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.228234</td>\n",
       "      <td>0.931421</td>\n",
       "      <td>0.926799</td>\n",
       "      <td>0.927994</td>\n",
       "      <td>0.925536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 00:45:27,449] Trial 8 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='13970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/13970 00:33 < 38:39, 5.94 it/s, Epoch 0/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.804362</td>\n",
       "      <td>0.620555</td>\n",
       "      <td>0.600530</td>\n",
       "      <td>0.446236</td>\n",
       "      <td>0.492092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:46:02,169] Trial 9 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1397' max='1397' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1397/1397 05:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596001</td>\n",
       "      <td>0.686222</td>\n",
       "      <td>0.655545</td>\n",
       "      <td>0.692106</td>\n",
       "      <td>0.549174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.407653</td>\n",
       "      <td>0.840726</td>\n",
       "      <td>0.830230</td>\n",
       "      <td>0.856344</td>\n",
       "      <td>0.819889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>0.903048</td>\n",
       "      <td>0.898465</td>\n",
       "      <td>0.897403</td>\n",
       "      <td>0.895534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.230939</td>\n",
       "      <td>0.920270</td>\n",
       "      <td>0.915061</td>\n",
       "      <td>0.915427</td>\n",
       "      <td>0.913449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.232635</td>\n",
       "      <td>0.921633</td>\n",
       "      <td>0.915719</td>\n",
       "      <td>0.919357</td>\n",
       "      <td>0.914355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.229238</td>\n",
       "      <td>0.922376</td>\n",
       "      <td>0.916482</td>\n",
       "      <td>0.920252</td>\n",
       "      <td>0.915026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 00:51:06,126] Trial 10 finished with value: 3.674136889484728 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 1}. Best is trial 3 with value: 3.750187554771879.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='8382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/8382 00:33 < 23:01, 5.92 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.755566</td>\n",
       "      <td>0.642238</td>\n",
       "      <td>0.618349</td>\n",
       "      <td>0.453993</td>\n",
       "      <td>0.507188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:51:40,941] Trial 11 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='11176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/11176 00:33 < 30:55, 5.92 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.784692</td>\n",
       "      <td>0.629352</td>\n",
       "      <td>0.607767</td>\n",
       "      <td>0.449446</td>\n",
       "      <td>0.498267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:52:15,604] Trial 12 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='2794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 600/2794 01:41 < 06:11, 5.91 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.661518</td>\n",
       "      <td>0.672407</td>\n",
       "      <td>0.644151</td>\n",
       "      <td>0.467679</td>\n",
       "      <td>0.528682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.398717</td>\n",
       "      <td>0.831867</td>\n",
       "      <td>0.816388</td>\n",
       "      <td>0.840346</td>\n",
       "      <td>0.806251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.542900</td>\n",
       "      <td>0.338967</td>\n",
       "      <td>0.872197</td>\n",
       "      <td>0.861355</td>\n",
       "      <td>0.878070</td>\n",
       "      <td>0.856416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:53:57,876] Trial 13 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='5588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/5588 00:45 < 20:50, 4.31 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.676214</td>\n",
       "      <td>0.670611</td>\n",
       "      <td>0.640775</td>\n",
       "      <td>0.460771</td>\n",
       "      <td>0.525490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:54:45,118] Trial 14 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='8382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/8382 00:33 < 22:58, 5.93 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.714016</td>\n",
       "      <td>0.656424</td>\n",
       "      <td>0.630072</td>\n",
       "      <td>0.459434</td>\n",
       "      <td>0.517025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 00:55:19,695] Trial 15 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='2794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/2794 09:12 < 01:30, 4.34 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.622583</td>\n",
       "      <td>0.679779</td>\n",
       "      <td>0.649051</td>\n",
       "      <td>0.798456</td>\n",
       "      <td>0.532824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.452273</td>\n",
       "      <td>0.818858</td>\n",
       "      <td>0.805439</td>\n",
       "      <td>0.841408</td>\n",
       "      <td>0.789827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.275460</td>\n",
       "      <td>0.902366</td>\n",
       "      <td>0.897693</td>\n",
       "      <td>0.896143</td>\n",
       "      <td>0.894814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.225697</td>\n",
       "      <td>0.920270</td>\n",
       "      <td>0.914890</td>\n",
       "      <td>0.913836</td>\n",
       "      <td>0.914034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.228013</td>\n",
       "      <td>0.922996</td>\n",
       "      <td>0.916390</td>\n",
       "      <td>0.920638</td>\n",
       "      <td>0.915939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.239188</td>\n",
       "      <td>0.919465</td>\n",
       "      <td>0.913124</td>\n",
       "      <td>0.919309</td>\n",
       "      <td>0.911469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.210516</td>\n",
       "      <td>0.930368</td>\n",
       "      <td>0.925536</td>\n",
       "      <td>0.926960</td>\n",
       "      <td>0.924423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.237313</td>\n",
       "      <td>0.931297</td>\n",
       "      <td>0.928155</td>\n",
       "      <td>0.926392</td>\n",
       "      <td>0.926240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.216780</td>\n",
       "      <td>0.934147</td>\n",
       "      <td>0.929685</td>\n",
       "      <td>0.929635</td>\n",
       "      <td>0.928730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.221484</td>\n",
       "      <td>0.933156</td>\n",
       "      <td>0.928733</td>\n",
       "      <td>0.929291</td>\n",
       "      <td>0.927653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.250972</td>\n",
       "      <td>0.926093</td>\n",
       "      <td>0.920849</td>\n",
       "      <td>0.924173</td>\n",
       "      <td>0.919293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.228234</td>\n",
       "      <td>0.931421</td>\n",
       "      <td>0.926799</td>\n",
       "      <td>0.927994</td>\n",
       "      <td>0.925536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 01:04:33,893] Trial 16 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='11176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  200/11176 00:34 < 31:34, 5.79 it/s, Epoch 0/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.784692</td>\n",
       "      <td>0.629352</td>\n",
       "      <td>0.607767</td>\n",
       "      <td>0.449446</td>\n",
       "      <td>0.498267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 01:05:09,440] Trial 17 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='8382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/8382 00:33 < 22:58, 5.94 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.714016</td>\n",
       "      <td>0.656424</td>\n",
       "      <td>0.630072</td>\n",
       "      <td>0.459434</td>\n",
       "      <td>0.517025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-03 01:05:44,019] Trial 18 pruned. \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/users/k21190024/envs/conda/p-dis-torch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1397' max='1397' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1397/1397 05:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.399952</td>\n",
       "      <td>0.832363</td>\n",
       "      <td>0.819415</td>\n",
       "      <td>0.841437</td>\n",
       "      <td>0.809131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.430494</td>\n",
       "      <td>0.860860</td>\n",
       "      <td>0.851505</td>\n",
       "      <td>0.876884</td>\n",
       "      <td>0.844144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.235065</td>\n",
       "      <td>0.916057</td>\n",
       "      <td>0.911863</td>\n",
       "      <td>0.910538</td>\n",
       "      <td>0.909805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.235221</td>\n",
       "      <td>0.922562</td>\n",
       "      <td>0.917968</td>\n",
       "      <td>0.917290</td>\n",
       "      <td>0.916428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>0.223917</td>\n",
       "      <td>0.927952</td>\n",
       "      <td>0.922378</td>\n",
       "      <td>0.926033</td>\n",
       "      <td>0.921359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>0.225649</td>\n",
       "      <td>0.924669</td>\n",
       "      <td>0.919261</td>\n",
       "      <td>0.923845</td>\n",
       "      <td>0.917701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-03 01:10:47,932] Trial 19 finished with value: 3.68547573227396 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 1}. Best is trial 3 with value: 3.750187554771879.\n"
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    n_trials=20, \n",
    "    hp_space=optuna_hp_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a73ebc-2b56-45fd-93e8-c7f8caf940c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='3', objective=3.750187554771879, hyperparameters={'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}, run_summary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94567a32-b8bc-47b6-a670-1235f2784399",
   "metadata": {},
   "source": [
    "## Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b313c5e4-bd9f-41d2-a073-223e41db8f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23733' max='23733' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23733/23733 1:20:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.377900</td>\n",
       "      <td>0.282555</td>\n",
       "      <td>0.896201</td>\n",
       "      <td>0.890241</td>\n",
       "      <td>0.898962</td>\n",
       "      <td>0.888655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.199390</td>\n",
       "      <td>0.934364</td>\n",
       "      <td>0.932148</td>\n",
       "      <td>0.932955</td>\n",
       "      <td>0.931223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.169217</td>\n",
       "      <td>0.949710</td>\n",
       "      <td>0.946888</td>\n",
       "      <td>0.948141</td>\n",
       "      <td>0.946748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.148025</td>\n",
       "      <td>0.955227</td>\n",
       "      <td>0.953154</td>\n",
       "      <td>0.953243</td>\n",
       "      <td>0.952832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.134400</td>\n",
       "      <td>0.143667</td>\n",
       "      <td>0.957124</td>\n",
       "      <td>0.955334</td>\n",
       "      <td>0.954964</td>\n",
       "      <td>0.954972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.166775</td>\n",
       "      <td>0.952583</td>\n",
       "      <td>0.950127</td>\n",
       "      <td>0.951747</td>\n",
       "      <td>0.949852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.138376</td>\n",
       "      <td>0.958848</td>\n",
       "      <td>0.956620</td>\n",
       "      <td>0.957303</td>\n",
       "      <td>0.956576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.160141</td>\n",
       "      <td>0.959653</td>\n",
       "      <td>0.957636</td>\n",
       "      <td>0.958052</td>\n",
       "      <td>0.957443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.155362</td>\n",
       "      <td>0.961205</td>\n",
       "      <td>0.959438</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.077400</td>\n",
       "      <td>0.146129</td>\n",
       "      <td>0.961492</td>\n",
       "      <td>0.959656</td>\n",
       "      <td>0.959629</td>\n",
       "      <td>0.959471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.137382</td>\n",
       "      <td>0.961894</td>\n",
       "      <td>0.959511</td>\n",
       "      <td>0.960060</td>\n",
       "      <td>0.959711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.074400</td>\n",
       "      <td>0.146790</td>\n",
       "      <td>0.962067</td>\n",
       "      <td>0.959728</td>\n",
       "      <td>0.960539</td>\n",
       "      <td>0.959922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.165589</td>\n",
       "      <td>0.959768</td>\n",
       "      <td>0.957915</td>\n",
       "      <td>0.957869</td>\n",
       "      <td>0.957609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.156090</td>\n",
       "      <td>0.961435</td>\n",
       "      <td>0.959291</td>\n",
       "      <td>0.960061</td>\n",
       "      <td>0.959228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.165298</td>\n",
       "      <td>0.960975</td>\n",
       "      <td>0.959374</td>\n",
       "      <td>0.958966</td>\n",
       "      <td>0.958987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.163956</td>\n",
       "      <td>0.962814</td>\n",
       "      <td>0.960771</td>\n",
       "      <td>0.961226</td>\n",
       "      <td>0.960726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.169322</td>\n",
       "      <td>0.963561</td>\n",
       "      <td>0.961290</td>\n",
       "      <td>0.962152</td>\n",
       "      <td>0.961463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.180367</td>\n",
       "      <td>0.961435</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>0.959894</td>\n",
       "      <td>0.959277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.176615</td>\n",
       "      <td>0.962354</td>\n",
       "      <td>0.960239</td>\n",
       "      <td>0.961080</td>\n",
       "      <td>0.960233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.183290</td>\n",
       "      <td>0.961837</td>\n",
       "      <td>0.959996</td>\n",
       "      <td>0.960464</td>\n",
       "      <td>0.959802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.182729</td>\n",
       "      <td>0.962929</td>\n",
       "      <td>0.961195</td>\n",
       "      <td>0.961400</td>\n",
       "      <td>0.960952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.181830</td>\n",
       "      <td>0.962527</td>\n",
       "      <td>0.960695</td>\n",
       "      <td>0.961106</td>\n",
       "      <td>0.960496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.174052</td>\n",
       "      <td>0.962929</td>\n",
       "      <td>0.961088</td>\n",
       "      <td>0.961383</td>\n",
       "      <td>0.960933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=23733, training_loss=0.10302246744561495, metrics={'train_runtime': 4843.5148, 'train_samples_per_second': 156.791, 'train_steps_per_second': 4.9, 'total_flos': 5.144692423815806e+16, 'train_loss': 0.10302246744561495, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_store_path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "    \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c033e6c-1341-4ddd-9c81-10f456146e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_store_path.parent / (model_store_path.stem + \".out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fc1ce-86d3-4168-a4a1-71f521fe37a3",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b699f7bb-e4c9-45ad-9fd1-d7425dd6f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7590-31cc-40ad-b212-d0c47500277f",
   "metadata": {},
   "source": [
    "## Test on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdfafa9a-6265-4792-8513-149ab1b0545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"validation\"])\n",
    "micro_val, macro_val = generate_micro_macro_df(data[\"validation\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c16081ff-6aa6-4620-8cdf-50f3e07b73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.99      0.99      0.99      6666\n",
      "        REFUTES       0.96      0.92      0.94      5347\n",
      "       SUPPORTS       0.94      0.97      0.95      5386\n",
      "\n",
      "       accuracy                           0.96     17399\n",
      "      macro avg       0.96      0.96      0.96     17399\n",
      "   weighted avg       0.96      0.96      0.96     17399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=micro_val[\"actual\"], y_pred=micro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d4cf6-3cfe-458f-9ad7-e58994554cbc",
   "metadata": {},
   "source": [
    "## Test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b9b87e1-0054-4bbe-a6e8-b76867f03b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(data[\"test\"])\n",
    "micro_val, macro_val = generate_micro_macro_df(data[\"test\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6548fc2f-a81f-4e65-b697-0b9e80ba9402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.98      0.99      0.99      6666\n",
      "        REFUTES       0.95      0.90      0.92      5272\n",
      "       SUPPORTS       0.92      0.96      0.94      5389\n",
      "\n",
      "       accuracy                           0.95     17327\n",
      "      macro avg       0.95      0.95      0.95     17327\n",
      "   weighted avg       0.95      0.95      0.95     17327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=micro_val[\"actual\"], y_pred=micro_val[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae0ef8-63c5-4092-b9a3-1528fe23a2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mainenv]",
   "language": "python",
   "name": "conda-env-mainenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
